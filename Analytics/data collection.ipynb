{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data collection\n",
    "\n",
    "Collect lots of things from lots of sources.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic http tools\n",
    "\n",
    "Copying files from many sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### copy list of links to json\n",
    "\n",
    "https://www.maricopacountyattorney.org/CivicAlerts.aspx?AID=400\n",
    "\n",
    "Crime reports?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/crimes');\n",
    "\n",
    "function scrapeAlert(ID) {\n",
    "    if(fs.existsSync(path.join(project, 'maricopa-alert-' + ID + '.json'))) {\n",
    "        return;\n",
    "    }\n",
    "    return client\n",
    "        .url('https://www.maricopacountyattorney.org/CivicAlerts.aspx?AID=' + ID)\n",
    "        .getAllXPath({\n",
    "            time: '//*[@class = \"single\"]//*[@class = \"date\"]//text()',\n",
    "            title: '//*[contains(@class, \"single\")]//h3//text()',\n",
    "            content: '//*[@class = \"single\"]//*[@class = \"content\"]//text()'\n",
    "        })\n",
    "        .then(r => {\n",
    "            fs.writeFileSync(path.join(project, 'maricopa-alert-' + ID + '.json'), JSON.stringify(r, null, 4));\n",
    "            return r;\n",
    "        })\n",
    "        .catch(e => console.log(e))\n",
    "}\n",
    "module.exports = scrapeAlert;\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    var IDs = Array.from(Array(500).keys());\n",
    "    multiCrawl(IDs, 'crime reports')\n",
    "        .then(r => $$.sendResult(r))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TODO: download amazon history\n",
    "\n",
    "Because Amazon download offer it as a download like Netflix does.\n",
    "\n",
    "https://www.amazon.com/gp/yourstore/iyr/ref=pd_ys_iyr_next?ie=UTF8&collection=watched&iyrGroup=&maxItem=616&minItem=600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search a lot of engines\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### the code \n",
    "\n",
    "meta search all?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "var importer = require('../Core');\n",
    "var multiCrawl = importer.import('multi crawl');\n",
    "\n",
    "// http://www.exploratorium.edu/files/ronh/research/index.html\n",
    "/*\n",
    "\n",
    " Exploratorium\t\n",
    " Search\n",
    " Google\t\n",
    " Search\n",
    " alltheweb\t\n",
    " Search\n",
    " Teoma\t\n",
    " Search\n",
    " AltaVista\t\n",
    " Search\n",
    " Wisenut\t\n",
    " Search\n",
    " HotBot\t\n",
    " Search\n",
    " lii.org\t\n",
    " Search\n",
    " Northern Light\t\n",
    " Search\n",
    " Lycos\t\n",
    " Search\n",
    " Scirus\t\n",
    " \n",
    " Meta:\n",
    " Ask Jeeves\t\n",
    " Search\n",
    " MetaCrawler\t\n",
    " Search\n",
    " Dogpile\t\n",
    " Search\n",
    " SavvySearch\t\n",
    " \n",
    " \n",
    " */\n",
    "var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/searches');\n",
    "\n",
    "function searchAll(query = 'search engine') {\n",
    "    var engines = [\n",
    "        'https://www.google.com/search?q=' + query,\n",
    "        'https://www.bing.com/search?q=' + query,\n",
    "        'https://search.yahoo.com/search?p=' + query,\n",
    "        'https://www.ask.com/web?q=' + query,\n",
    "        'https://search.aol.com/aol/search?q=' + query,\n",
    "        'http://www.baidu.com/s?wd=' + query,\n",
    "        'https://www.wolframalpha.com/input/?i=' + query,\n",
    "        'https://duckduckgo.com/?q=' + query,\n",
    "        'https://www.yandex.com/search/?text=' + query,\n",
    "        'https://archive.org/search.php?query=' + query,\n",
    "    ];\n",
    "    \n",
    "    // TODO: save results\n",
    "    return multiCrawl(engines, 'search results json')\n",
    "        .then(r => {\n",
    "            const time = new Date();\n",
    "            fs.writeFileSync(path.join(project, query.replace(/[^a-z0-9]/ig, '_')\n",
    "                                       + '-' + time.getFullYear()\n",
    "                                       + '-' + (time.getMonth() + 1)\n",
    "                                       + '-' + time.getDate()\n",
    "                                       + '.json'), JSON.stringify(r, null, 4));\n",
    "            return r;\n",
    "        })\n",
    "}\n",
    "module.exports = searchAll;\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async();\n",
    "    searchAll()\n",
    "        .then(r => $$.sendResult(r))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### schedule search all?\n",
    "\n",
    "TODO: convert this to a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var google = require('googleapis');\n",
    "var importer = require('../Core');\n",
    "var {\n",
    "    getOauthClient,\n",
    "    ISODateString,\n",
    "    createNewEvent,\n",
    "} = importer.import([\n",
    "    'convert date iso',\n",
    "    'create new calendar event',\n",
    "    'import google calendar api',\n",
    "]);\n",
    "\n",
    "var options = {\n",
    "    calendarId: 'aws'\n",
    "}\n",
    "\n",
    "function scheduleSearch(search) {\n",
    "    const parameters = {\n",
    "        query: search || 'search engines'\n",
    "    }\n",
    "    const newDate = new Date();\n",
    "    return (typeof options.auth === 'undefined'\n",
    "           ? getOauthClient(options)\n",
    "           : Promise.resolve([]))\n",
    "        .then(() => createNewEvent('meta search all', JSON.stringify(parameters, null, 4), options))\n",
    "}\n",
    "module.exports = scheduleSearch;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### tell joke?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var util = require('bluebird');\n",
    "var request = util.promisify(require('request'));\n",
    "var importer = require('../Core')\n",
    "\n",
    "var jokes;\n",
    "function getJoke() {\n",
    "    // TODO: collect jokes instead\n",
    "    return (typeof jokes === 'undefined'\n",
    "        ? request('http://www.ducksters.com/jokes/silly.php')\n",
    "        .then(res => importer.regexToArray(/^.*?Q:.*$|^.*?A:.*$/igm, res.body))\n",
    "        .then(r => {\n",
    "            r = r.reduce((arr, j, i) => {\n",
    "                if(i % 2 === 1) {\n",
    "                    arr.push([\n",
    "                        r[i-1].replace(/<.*?\\s*\\/?>/ig, '').trim().replace(/^\\s*|\\s*$/igm, ''),\n",
    "                        j.replace(/<.*?\\s*\\/?>/ig, '').trim().replace(/^\\s*|\\s*$/igm, '')\n",
    "                    ]);\n",
    "                }\n",
    "                return arr;\n",
    "            }, []);\n",
    "            console.log(r);\n",
    "            jokes = r;\n",
    "            return r;\n",
    "        })\n",
    "        : Promise.resolve(jokes))\n",
    "        .then(arr => {\n",
    "            const i = Math.round(Math.random() * arr.length);\n",
    "            return arr[i];\n",
    "        })\n",
    "}\n",
    "module.exports = getJoke;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advanced http tools \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### TODO multicrawl with selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n",
    "multi crawl?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var importer = require('../Core');\n",
    "var runSeleniumCell = importer.import('selenium cell');\n",
    "\n",
    "var TIMEOUT = 1000;\n",
    "var CONNECTIONS = 3;\n",
    "\n",
    "// recursively dequeue tasks\n",
    "function deQueue(inputQueue, searchCallback, ctx) {\n",
    "    const results = [];\n",
    "    console.log(ctx.client.requestHandler.sessionID);\n",
    "    const callback = typeof searchCallback === 'function'\n",
    "        ? searchCallback\n",
    "        : importer.import(searchCallback, Object.assign({useCache: false}, ctx));\n",
    "    if(inputQueue.length > 0) {\n",
    "        const item = inputQueue.shift();\n",
    "        return new Promise(resolve => setTimeout(() => resolve(), 100))\n",
    "            .then(() => callback(...[item, ctx]))\n",
    "            .catch(e => {\n",
    "                console.log(e + '');\n",
    "                if((e + '').indexOf('Already') > -1 || (e + '').indexOf('session') > -1) {\n",
    "                    inputQueue.push(item);\n",
    "                    throw new Error('Abandoning session :(', e);\n",
    "                }\n",
    "            })\n",
    "            .then(r => results.push(r))\n",
    "            .then(() => deQueue(inputQueue, searchCallback, ctx))\n",
    "            .then(r => results.concat(r))\n",
    "            .catch(e => console.log(e))\n",
    "    } else {\n",
    "        return results;\n",
    "    }\n",
    "}\n",
    "\n",
    "// create a number of individual selenium sessions and dequeue the tasks with the callback search\n",
    "function multiCrawl(inputList, searchCallback) {\n",
    "    var indexes = Array.from(Array(Math.min(inputList.length, CONNECTIONS)).keys());\n",
    "    var connections = [];\n",
    "    var promises = indexes.map((s, i) => resolve => {\n",
    "        const client = runSeleniumCell(false, false);\n",
    "        return client\n",
    "            // skip this if error\n",
    "            //.then(() => connections[i].onlyOneWindow())\n",
    "            //.then(() => connections[i].resizeWindow())\n",
    "            .then(ctx => {\n",
    "                connections.push(ctx);\n",
    "                resolve(ctx)\n",
    "            })\n",
    "            .catch(e => {\n",
    "                console.log(e);\n",
    "                resolve(null);\n",
    "            })\n",
    "    });\n",
    "    var queue = [].concat(inputList);\n",
    "    var count = 0;\n",
    "    return importer.runAllPromises(promises)\n",
    "        .then(() => {\n",
    "            console.log(connections.map(c => c.client.requestHandler.sessionID));\n",
    "        })\n",
    "        .then(() => {\n",
    "            return connections[0].client\n",
    "                .scanning(true)\n",
    "                .then(() => connections[0].getAllSessionUrls())\n",
    "                .scanning(false)\n",
    "        })\n",
    "        .then(() => console.log('done loading sessions'))\n",
    "        .then(() => Promise.all(connections.map(ctx => deQueue(queue, searchCallback, ctx))))\n",
    "        .then(r => [].concat([], ...r))\n",
    "}\n",
    "module.exports = multiCrawl;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl domain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs');\n",
    "var path = require('path');\n",
    "var importer = require('../Core');\n",
    "var {doBrowserRequest} = importer.import('browser crawler tools')\n",
    "var {\n",
    "    cacheFilename,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "    rmhash\n",
    "} = importer.import('domain cache tools')\n",
    "\n",
    "async function crawlRecursive(url, depth, searches) {\n",
    "    if(!depth) depth = 3 // TODO: minutes depth using time range?\n",
    "    url = (typeof url === 'string' ? [url] : url )\n",
    "    // searches2 keeps track of new pages that should be added if searches is not provided\n",
    "    //   this guaruntees at least one page will be requested when this is called\n",
    "    const searches2 = []\n",
    "    for(var i = 0; i < url.length; i++) {\n",
    "        var l = url[i]\n",
    "        try {\n",
    "            await doBrowserRequest(l, readCache.bind(null, searches || searches2),\n",
    "                                   storeCache.bind(null, searches || searches2))\n",
    "        } catch (e) {\n",
    "            console.log(e)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // TODO: fix this\n",
    "    \n",
    "    // push old cache on to bottom of current searches,\n",
    "    //   so we are always getting at least the page requested\n",
    "    if(!searches) searches = searches2.concat(existingCache(url[0]))\n",
    "    \n",
    "    var existing = searches.map(s => rmhash(s.url))\n",
    "    var links = searches2\n",
    "    // TODO: pattern defensive programming\n",
    "        .map(s => {\n",
    "            var styles = s.styles || []\n",
    "            var links = s.links || []\n",
    "            return styles.concat(links)\n",
    "        })\n",
    "        .flat()\n",
    "        // do not include hash in actual link to the page\n",
    "        .map(s => rmhash(s))\n",
    "        // filter out first occurence\n",
    "        .filter((l, i, arr) => arr.indexOf(l) === i\n",
    "        // filter out all existing urls\n",
    "                && !existing.includes(rmhash(l))\n",
    "        // filter out data uris\n",
    "                && !l.includes('data:') && !l.includes('mailto:')\n",
    "                && !l.includes('javascript:') && !l.includes('ios-app:'))\n",
    "    \n",
    "    if(depth > 1) {\n",
    "        return await crawlRecursive(links, depth - 1, searches)\n",
    "    }\n",
    "    \n",
    "    // close the browser\n",
    "    await doBrowserRequest(false)\n",
    "    \n",
    "    // save the database\n",
    "    var filePath = cacheFilename(searches[0] ? searches[0].url : url[0])\n",
    "    fs.writeFileSync(filePath, JSON.stringify(searches, null, 2))\n",
    "}\n",
    "\n",
    "async function crawlAll(url, depth, searches) {\n",
    "    try {\n",
    "        await crawlRecursive(url, depth, searches)\n",
    "    } catch (e) {\n",
    "        console.log(e)\n",
    "        await doBrowserRequest(false)\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = crawlAll\n",
    "\n",
    "//var importer = require('../Core')\n",
    "//var crawlAll = importer.import('crawl domain')\n",
    "\n",
    "if(typeof $$ !== 'undefined') {\n",
    "    $$.async()\n",
    "    crawlAll('https://google.com', 2)\n",
    "        .then(r => $$.sendResult('done'))\n",
    "        .catch(e => $$.sendError(e))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### domain cache tools?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs')\n",
    "var path = require('path')\n",
    "var importer = require('../Core')\n",
    "var {glob} = importer.import('glob files')\n",
    "var {getResponseContent} = importer.import('browser crawler tools')\n",
    "\n",
    "//var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var PROFILE_PATH = '/Volumes/External/Personal'\n",
    "var project = path.join(PROFILE_PATH, 'Collections/crawls');\n",
    "\n",
    "function cacheFilename(url) {\n",
    "    if(typeof url === 'string') {\n",
    "        url = new URL(url.includes('://') ? url : ('http://' + url.replace(/^\\/\\//, '')))\n",
    "    }\n",
    "    const time = new Date()\n",
    "    var file = safeurl(url.hostname)\n",
    "       + '-' + time.getFullYear()\n",
    "       + '-' + (time.getMonth() + 1)\n",
    "       + '-' + time.getDate() + '.json'\n",
    "    return path.join(project, file)\n",
    "}\n",
    "\n",
    "function findCache(url) {\n",
    "    if(typeof url === 'string') {\n",
    "        url = new URL(url.includes('://') ? url : ('http://' + url.replace(/^\\/\\//, '')))\n",
    "    }\n",
    "    const host = safeurl(url.hostname)\n",
    "    const crawl = glob(\n",
    "        '**/*' + host + '*',\n",
    "        project\n",
    "    )\n",
    "    crawl.sort((a, b) => {\n",
    "        return fs.statSync(b).mtime.getTime() - fs.statSync(a).mtime.getTime()\n",
    "    })\n",
    "    return crawl\n",
    "}\n",
    "\n",
    "function existingCache(url, restrain) {\n",
    "    var cache = findCache(url)\n",
    "    var filePath = cacheFilename(url)\n",
    "    // save pages from the same day in the same database using the url as the keys\n",
    "    if(cache[0]) {\n",
    "        var segments = cache[0].replace(/\\..*$/ig, '').split('-')\n",
    "        var date = Date.parse(segments.slice(segments.length - 3).join('-'))\n",
    "        var inAWeek = date + 1000*60*60*24*7\n",
    "        if(restrain === false\n",
    "          || (restrain === 'week' && inAWeek > Date.now())\n",
    "          || (restrain === 'day' && cache[0] === filePath)) {\n",
    "            return JSON.parse(fs.readFileSync(cache[0])) || []\n",
    "        }\n",
    "    }\n",
    "    return []\n",
    "}\n",
    "\n",
    "async function storeCache(cache, response) {\n",
    "    var headers = await response.headers()\n",
    "    var result = await getResponseContent(response, headers)\n",
    "    if(typeof result.url === 'undefined') {\n",
    "        return\n",
    "    }\n",
    "    var urls = cache.map(s => s.url.toLowerCase())\n",
    "    var index = urls.indexOf(result.url.toLowerCase())\n",
    "    if(index > -1) {\n",
    "        console.log(`Received existing ${result.url}`)\n",
    "        cache[index] = result\n",
    "    } else {\n",
    "        console.log(`Received ${result.url}`)\n",
    "        cache.push(result)\n",
    "    }\n",
    "}\n",
    "\n",
    "// source: https://stackoverflow.com/questions/53807574/how-to-block-ads-with-puppeteer-headless-chrome\n",
    "// TODO: create a notebook for this and add easylist\n",
    "var hosts;\n",
    "function adBlocker() {\n",
    "    if(hosts) return hosts\n",
    "    hosts = {}\n",
    "    //now we read the host file\n",
    "    var hostFile = fs.readFileSync(path.join(__dirname,\n",
    "        '../Resources/Projects/adblocker/hosts.txt'), 'utf8').split('\\n');\n",
    "    for (var i = 0; i < hostFile.length; i++) {\n",
    "        var frags = hostFile[i].split(' ');\n",
    "        if (frags.length > 1 && frags[0] === '0.0.0.0') {\n",
    "            hosts[frags[1].trim()] = true;\n",
    "        }\n",
    "    }\n",
    "    return hosts\n",
    "}\n",
    "\n",
    "function readCache(cache, request) {\n",
    "    if(request.url().substr(0, 5) === 'data:') {\n",
    "        return request.continue()\n",
    "    }\n",
    "    if(adBlocker()[new URL(request.url()).host]) {\n",
    "        return request.abort()\n",
    "    }\n",
    "    var urls = cache.map(s => s.url.toLowerCase())\n",
    "    var index = urls.indexOf(request.url().toLowerCase())\n",
    "    var response = cache[index]\n",
    "    if (response && response.status == 200) {\n",
    "    // TODO: remove this restriction since we literally just downloaded it\n",
    "    //    && response.expires\n",
    "    //    && response.expires > Date.now()\n",
    "    //    && response.content\n",
    "    //if(response.type.includes('\\n')) {\n",
    "    //    debugger\n",
    "    //}\n",
    "        console.log(`Requesting cache ${response.status} ${response.type} ${request.url()}`)\n",
    "        var headers = {\n",
    "            'Access-Control-Allow-Origin': '*'\n",
    "        }\n",
    "        if(response.location) headers['Location'] = response.location\n",
    "        // TODO: save in this format\n",
    "        try {\n",
    "            request.respond({\n",
    "                contentType: response.type,\n",
    "                headers: headers,\n",
    "                status: response.status || 200,\n",
    "                body: response.content\n",
    "                    && response.content.substr(0, 5) === 'data:'\n",
    "                    ? Buffer.from(response.content.split(',')[1], 'base64')\n",
    "                    : Buffer.from(response.content || [], 'utf8'),\n",
    "            })\n",
    "        } catch (e) {\n",
    "            console.log(e)\n",
    "            request.continue()\n",
    "        }\n",
    "        return\n",
    "    }\n",
    "    console.log(`Requesting ${request.url()}`)\n",
    "    request.continue()\n",
    "}\n",
    "\n",
    "// TODO: move this to URL tools in Languages/html.ipynb with getAllLinks\n",
    "function rmhash(url) {\n",
    "    return url.replace(/#.*$/ig, '')\n",
    "}\n",
    "\n",
    "// TODO: replace other occurrences with this function\n",
    "function safeurl(url) {\n",
    "    return url.replace(/[^a-z0-9_-]/ig, '_').substr(0, 100)\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "    cacheFilename,\n",
    "    findCache,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "    rmhash,\n",
    "    safeurl,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### browser crawler tools?\n",
    "\n",
    "Partly derrived from https://help.apify.com/en/articles/2424032-cache-responses-in-puppeteer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var puppeteer = require('puppeteer')\n",
    "var importer = require('../Core')\n",
    "var {selectDom} = importer.import('select tree')\n",
    "\n",
    "// TODO: make a utility for this because it keeps coming up here and in convert spreadsheet\n",
    "function getStyleUrls(content, url) {\n",
    "    return importer.regexToArray(/url\\s*\\(['\"]*([^\\)]*?)['\"]*\\)/ig, content, 1)\n",
    "}\n",
    "\n",
    "// TODO: make a utility for this because it keeps coming up here and in convert spreadsheet\n",
    "function getAllLinks(url, doc) {\n",
    "    // TODO: only return elements because of \n",
    "    //   https://github.com/google/wicked-good-xpath/issues/40\n",
    "    //   Uncaught TypeError: Cannot read property 'createRange' of undefined\n",
    "    // MUST SEPERATE ATTRIBUTES AND ELEMENTS BECAUSE OF ABOVE ISSUE\n",
    "    var r = selectDom({\n",
    "        links: ['//a', './@href'],\n",
    "        sources: ['(//img|//iframe|//audio)[@src]', './@src'],\n",
    "        styles: ['//link', './@href'],\n",
    "        styleInners: ['//style', './text()'],\n",
    "        styleTags: ['//*[@style]', './@style'],\n",
    "        html: ['/*']\n",
    "    }, doc)\n",
    "    var html = r.html.map(h => h.outerHTML).join('')\n",
    "    return {\n",
    "        links: r.links\n",
    "            .concat(r.sources)\n",
    "            .concat(['/favicon.ico']) // always implied 100% of the time\n",
    "            .map(s => {\n",
    "                try {\n",
    "                    return new URL(s, url).href\n",
    "                } catch (e) {\n",
    "                    return null\n",
    "                }\n",
    "            }),\n",
    "        styles: r.styles\n",
    "            .concat(...r.styleInners.map(s => getStyleUrls(s, url)))\n",
    "            .concat(...r.styleTags.map(s => getStyleUrls(s, url)))\n",
    "            .map(s => {\n",
    "                try {\n",
    "                    return new URL(s, url).href\n",
    "                } catch (e) {\n",
    "                    return null\n",
    "                }\n",
    "            }),\n",
    "        html: html\n",
    "    }\n",
    "}\n",
    "\n",
    "async function getExpires(headers) {\n",
    "    const cacheControl = headers['cache-control'] || '';\n",
    "    const maxAgeMatch = cacheControl.match(/max-age=(\\d+)/);\n",
    "    const maxAge = maxAgeMatch && maxAgeMatch.length > 1 ? parseInt(maxAgeMatch[1], 10) : 0;\n",
    "    return Date.now() + (maxAge * 1000)\n",
    "}\n",
    "\n",
    "async function getResponseContent(response, headers) {\n",
    "    // TODO: save in the same format as the request expects\n",
    "    var result = {\n",
    "        url: await response.url(),\n",
    "        type: (headers['content-type'] || '').split(/\\n|;/gi)[0],\n",
    "        length: parseInt(headers['content-length']),\n",
    "        expires: await getExpires(headers),\n",
    "        status: await response.status(),\n",
    "        location: headers['location'],\n",
    "    }\n",
    "    if(result.url.substr(0, 5) === 'data:') {\n",
    "        return {}\n",
    "    }\n",
    "    if(result.status < 200 || result.status > 300) {\n",
    "        return result\n",
    "    }\n",
    "    // check headers and make sure we don't download too much\n",
    "    if(!result.length || isNaN(result.length) || result.length < 100*1024*1024) {\n",
    "        try {\n",
    "            var buffer = await response.buffer()\n",
    "            if(result.type.includes('text/')) {\n",
    "                // TODO replace with encoding from header\n",
    "                result.content = buffer.toString('utf8') \n",
    "            } else {\n",
    "                result.content = `data:${result.type};base64,${buffer.toString('base64')}`\n",
    "            }\n",
    "            if(result.type.includes('/html')) {\n",
    "                Object.assign(result, getAllLinks(result.url, result.content))\n",
    "            }\n",
    "        } catch (e) {\n",
    "            if(e.message.includes('Target closed')\n",
    "              || e.message.includes('Response body is unavailable')\n",
    "              || e.message.includes('Protocol error')\n",
    "              || e.message.includes('Could not load body for this request')\n",
    "              || e.message.includes('Network.getResponseBody timed out')\n",
    "            ) {\n",
    "                console.log(`Probable tracker ${result.url}`)\n",
    "                return {}\n",
    "            } else {\n",
    "                throw e\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return result\n",
    "}\n",
    "\n",
    "var browser\n",
    "var RETRY_COUNT = 1\n",
    "async function doBrowserRequest(url, readCache, storeCache, callback, retry = 0) {\n",
    "    if(url === false) {\n",
    "        console.log('Closing browser')\n",
    "        if(browser) {\n",
    "            await browser.close()\n",
    "            browser = null\n",
    "        }\n",
    "        return\n",
    "    }\n",
    "    try {\n",
    "        var response\n",
    "        //const ext = '/Users/briancullinan/Downloads/1.18.4.crx'\n",
    "        if(!browser) browser = await puppeteer.launch({\n",
    "            args: [\n",
    "            //    `--disable-extensions-except=\"${ext}\"`,\n",
    "            //    `--load-extension=\"${ext}\"`,\n",
    "                '--enable-remote-extensions',\n",
    "                '--no-first-run',\n",
    "                '--disable-notifications',\n",
    "                '--disable-geolocation',\n",
    "                '--disable-infobars',\n",
    "                '--disable-session-crashed-bubble',\n",
    "                '--no-sandbox',\n",
    "                '--silent-debugger-extension-api',\n",
    "                '--extensions-on-chrome-urls',\n",
    "            //    '--single-process',\n",
    "            //    '--no-zygote',\n",
    "            //    '--disable-setuid-sandbox',\n",
    "                // extensions.ui.developer_mode\n",
    "                // --flag-switches-begin --extensions.ui.developer_mode --flag-switches-end.\n",
    "            ],\n",
    "            headless: true,\n",
    "            dumpio: true,\n",
    "            ignoreHTTPSErrors: true,\n",
    "        }).catch(e => console.log(e))\n",
    "        \n",
    "        console.log(`Initiating ${url}`)\n",
    "        const page = await browser.newPage()\n",
    "        await page.setRequestInterception(true)\n",
    "        page.on('request', readCache)\n",
    "        page.on('response', storeCache)\n",
    "        if(url)\n",
    "            response = await page.goto(url, {waitUntil: \"networkidle2\", timeout: 5*1000})\n",
    "\n",
    "        // close page\n",
    "        console.log(`Finishing ${url}`)\n",
    "        await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "        //await page._client.send('Page.stopLoading')\n",
    "        //await page._client.send('BrowserWindow.addDevToolsExtension', ext)\n",
    "        page.off('request', readCache)\n",
    "        page.off('response', storeCache)\n",
    "        await page.setRequestInterception(false)\n",
    "        if(response && callback) await callback(response, page)\n",
    "        console.log('Closing page')\n",
    "        await page.close()\n",
    "        return response\n",
    "    } catch (e) {\n",
    "        if(retry < RETRY_COUNT) {\n",
    "            await doBrowserRequest(false)\n",
    "            return await doBrowserRequest(url, readCache, storeCache, callback, retry+1)\n",
    "        } else if(e.message.includes('ERR_NAME_NOT_RESOLVED')\n",
    "                 || e.message.includes('Navigation timeout')\n",
    "                 || e.message.includes('ERR_CONNECTION_REFUSED')\n",
    "                 || e.message.includes('ERR_ABORTED')) {\n",
    "            console.log(`Could not fetch ${url}`)\n",
    "        } else {\n",
    "            throw e\n",
    "        }\n",
    "    }\n",
    "}\n",
    "    \n",
    "module.exports = {\n",
    "    getAllLinks,\n",
    "    getResponseContent,\n",
    "    doBrowserRequest\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyze cache file?\n",
    "\n",
    "Show some interesting information about cache files, and remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var {URL} = require('url')\n",
    "var fs = require('fs')\n",
    "var {findCache} = importer.import('domain crawler tools')\n",
    "\n",
    "function analyzeCache(url) {\n",
    "    var cache = findCache(url)\n",
    "    if(cache.length === 0) {\n",
    "        return {\n",
    "            error: `No cache file found ${url}`\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    var json = JSON.parse(fs.readFileSync(cache[0]))\n",
    "    var domains = json.map(s => new URL(s.url).hostname)\n",
    "        .filter((h, i, arr) => arr.indexOf(h) === i)\n",
    "    var largeness = json.sort((a, b) => b.content.length - a.content.length)\n",
    "        .slice(0, 10)\n",
    "    var urls = json.map(s => s.url)\n",
    "    var repeats = json.filter((s, i, arr) => i > 0 && urls.indexOf(s.url) === i)\n",
    "    fs.writeFileSync(cache[0], JSON.stringify(repeats, null, 2))\n",
    "    return {\n",
    "        countPages: json.length,\n",
    "        countCaches: cache.length,\n",
    "        target: json[0].url,\n",
    "        countDomains: domains.length,\n",
    "        domains: domains,\n",
    "        countLargest: largeness.reduce((cur, l) => cur + l.content.length, 0),\n",
    "        largest10: largeness.map(l => l.url),\n",
    "        repeats: json.length - repeats.length,\n",
    "    }\n",
    "}\n",
    "\n",
    "module.exports = analyzeCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### schedule crawl domain?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var google = require('googleapis');\n",
    "var importer = require('../Core');\n",
    "var {\n",
    "    getOauthClient,\n",
    "    ISODateString,\n",
    "    createNewEvent,\n",
    "} = importer.import([\n",
    "    'convert date iso',\n",
    "    'create new calendar event',\n",
    "    'import google calendar api',\n",
    "]);\n",
    "\n",
    "var options = {\n",
    "    calendarId: 'aws'\n",
    "}\n",
    "\n",
    "function scheduleSearch(search) {\n",
    "    const parameters = {\n",
    "        query: search\n",
    "    }\n",
    "    const newDate = new Date();\n",
    "    return (typeof options.auth === 'undefined'\n",
    "           ? getOauthClient(options)\n",
    "           : Promise.resolve([]))\n",
    "        .then(() => createNewEvent('crawl domain', JSON.stringify(parameters, null, 4), options))\n",
    "}\n",
    "module.exports = scheduleSearch;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect bookmarks pages\n",
    "\n",
    "Convert all bookmarks to PDF and store in a JSON database for wiring up to express or my editor page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### introduction\n",
    "\n",
    "One of the most irritating and terrifying things about the modern web is losing information systems depend on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've experienced this, pretty much anytime some links to a Microsoft help doc without a title. They don't copy the original point of the article, or even summerize, this makes it impossible to learn. Humans are a lossy species, we lose information all the time, and perhaps it is part of evolution. I on the other hand, have gotten increasingly good at keeping information. \n",
    "\n",
    "What if some instruction to fix my computer suddenly disappears, should my computer become useless because of it.\n",
    "What if I learn something that becomes poigently relevant sometime in the future but the link is gone.\n",
    "\n",
    "We go one link deep, download all dependencies, and use Phantom browser to do it. It also makes a PDF so the content is preserved in different forms.\n",
    "\n",
    "Additionally, the Bookmarks Manager in Chrome can't search page content for relevance, this is something we can fix using \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "collect all bookmarks?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var path = require('path')\n",
    "var importer = require('../Core')\n",
    "var getBookmarksFromTakeout = importer.import('parse bookmarks file')\n",
    "var ISODateString = importer.import('convert date iso')\n",
    "var crawlAll = importer.import('crawl domain')\n",
    "var {doBrowserRequest} = importer.import('browser crawler tools')\n",
    "var {\n",
    "    safeurl,\n",
    "    existingCache,\n",
    "    storeCache,\n",
    "    readCache,\n",
    "} = importer.import('domain cache tools')\n",
    "\n",
    "//var PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE || '';\n",
    "var PROFILE_PATH = '/Volumes/External/Personal';\n",
    "var project = path.join(PROFILE_PATH, 'Collections/pdfs');\n",
    "var project2 = path.join(PROFILE_PATH, 'Collections/screenshots');\n",
    "\n",
    "async function savePdf(filename, response, page) {\n",
    "    try {\n",
    "        var type = (await response.headers())['content-type']\n",
    "        if(type !== 'text/html') return\n",
    "        console.log(`Printing PDF ${filename}`)\n",
    "        await page.addStyleTag({content: '*,*:before,*:after{max-height: 100000px!important;}*:before,*:after{vertical-align:unset!important;}'})\n",
    "        await page.emulateMediaType('screen')\n",
    "        await page.pdf({ path: filename })\n",
    "    } catch (e) {\n",
    "        console.log(e)\n",
    "    }\n",
    "    console.log('Done printing PDF')\n",
    "}\n",
    "\n",
    "async function saveScreenshot(filename, response, page) {\n",
    "    try {\n",
    "        var type = (await response.headers())['content-type']\n",
    "        if(type !== 'text/html') return\n",
    "        console.log(`Printing screenshot ${filename}`)\n",
    "        await page.addStyleTag({content: '*,*:before,*:after{max-height: 100000px!important;}*:before,*:after{vertical-align:unset!important;}'})\n",
    "        await page.emulateMediaType('screen').catch(e => console.log(e))\n",
    "        await page.screenshot({ path: filename, fullPage: true })\n",
    "    } catch (e) {\n",
    "        console.log(e)\n",
    "    }\n",
    "    console.log('Done printing screen')\n",
    "}\n",
    "\n",
    "async function collectAllBookmarks() {\n",
    "    var folders = getBookmarksFromTakeout()\n",
    "    var links = folders.reduce(function flattenFolders(arr, cur) {\n",
    "        if(cur.folder === 'Sad Examples') return arr\n",
    "        arr.push.apply(arr, cur.links.concat(cur.children.reduce(flattenFolders, [])))\n",
    "        return arr\n",
    "    }, [])\n",
    "\n",
    "    var urls = links.map(l => l.url.toLowerCase())\n",
    "    var existing = []\n",
    "    var notexisting = []\n",
    "    console.log(urls.length)\n",
    "    //links = [{url: 'http://lifehacker.com/386811/shutdown-windows-with-a-text-message-thunderbird-edition'}]\n",
    "    //var i = urls.indexOf('http://lifehacker.com/386811/shutdown-windows-with-a-text-message-thunderbird-edition')\n",
    "    for(var i = 0; i < links.length; i++) {\n",
    "        const filename = path.join(project, safeurl(links[i].url) + '.pdf')\n",
    "        const filename2 = path.join(project2, safeurl(links[i].url) + '.png')\n",
    "        \n",
    "        // check if there is a recent pdf and skip\n",
    "        if(fs.existsSync(filename)) {\n",
    "            existing.push(filename)\n",
    "            continue\n",
    "        }\n",
    "        notexisting.push(links[i])\n",
    "        try {\n",
    "            const cache = existingCache(links[i].url, false)\n",
    "            await crawlAll(links[i].url, 1, cache)\n",
    "\n",
    "            // save a pdf\n",
    "            // TODO: add page scrolling because AMP doesn't load images until you scroll to it\n",
    "            await doBrowserRequest(links[i].url, \n",
    "                                   readCache.bind(null, cache),\n",
    "                                   storeCache.bind(null, cache),\n",
    "                                   savePdf.bind(null, filename))\n",
    "            await doBrowserRequest(links[i].url, \n",
    "                                   readCache.bind(null, cache),\n",
    "                                   storeCache.bind(null, cache),\n",
    "                                   saveScreenshot.bind(null, filename2))\n",
    "        } catch (e) {\n",
    "            console.log(e)\n",
    "            await doBrowserRequest(false)\n",
    "        }\n",
    "    }\n",
    "    await doBrowserRequest(false)\n",
    "    console.log(existing)\n",
    "    console.log(notexisting)\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "    collectAllBookmarks,\n",
    "    saveScreenshot,\n",
    "    savePdf\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## html tools\n",
    "\n",
    "TODO: move this to Languages/html.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### find search boxes on a page\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### the code\n",
    "\n",
    "search results as json?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "function searchResultsToJson(url) {\n",
    "    console.log(url + ' - ' + client.requestHandler.sessionID);\n",
    "    return client\n",
    "        .url(url)\n",
    "        .pause(2000)\n",
    "        .getAllXPath({\n",
    "            query: '//input[contains(@aria-label, \"Search\")]/@value'\n",
    "            +\n",
    "            '|//input[contains(@aria-label, \"search\")]/@value'\n",
    "            +\n",
    "            // yahoo\n",
    "            '|//label[contains(., \"Search\")]/following::*//input[@type=\"text\"]/@value' \n",
    "            +\n",
    "            '|//input[contains(@class, \"Search\")]/@value'\n",
    "            +\n",
    "            // wolfram\n",
    "            '|//input[contains(@name, \"query\")]/@value'\n",
    "            +\n",
    "            // duckduckgo\n",
    "            '|//input[contains(@id, \"search\")]/@value'\n",
    "            +\n",
    "            // yandex\n",
    "            '|//input[contains(@aria-label, \"Request\")]/@value',\n",
    "            results: [\n",
    "                '//h3|//h2|div[contains(@class, \"title\")]'\n",
    "                +\n",
    "                // ask\n",
    "                '|//*[contains(@class, \"item-title\")]',\n",
    "                {\n",
    "                    name: './/text()',\n",
    "                    summary: './/following-sibling::div//text()'\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        .then(r => {\n",
    "            return {\n",
    "                url: url,\n",
    "                query: typeof r.query === 'string'\n",
    "                    ? r.query\n",
    "                    : r.query[0],\n",
    "                results: r.results.map(s => ({\n",
    "                    name: typeof s.name === 'string'\n",
    "                        ? s.name : s.name.join('\\n'),\n",
    "                    summary: typeof s.summary === 'string'\n",
    "                        ? s.summary : s.summary.join('\\n')\n",
    "                }))\n",
    "            };\n",
    "        })\n",
    "        .catch(e => {\n",
    "            console.log(e)\n",
    "            return {\n",
    "                url: url,\n",
    "                query: null,\n",
    "                results: []\n",
    "            }\n",
    "        })\n",
    "}\n",
    "module.exports = searchResultsToJson;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
