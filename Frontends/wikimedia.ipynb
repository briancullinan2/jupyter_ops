{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0509b9c1",
   "metadata": {},
   "source": [
    "# Wikimedia\n",
    "\n",
    "A wikimedia database implementaion from the simple download source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba829b",
   "metadata": {},
   "source": [
    "\n",
    "## database tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658c6fd",
   "metadata": {},
   "source": [
    "### search title index\n",
    "\n",
    "list wikimedia articles?\n",
    "\n",
    "ROUTE = /wikimedia-search.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84087e47",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "const readIndex = importer.import('load wikimedia index')\n",
    "const zlib = require('zlib');\n",
    "const readline = require('readline');\n",
    "const { search: fuzzySearch } = require(\"fast-fuzzy\");\n",
    "\n",
    "\n",
    "let allTitles = []\n",
    "//let allTokens = {}\n",
    "let compressed = []\n",
    "\n",
    "const PROFILE_HOME = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "const INDEX_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream-index.txt')\n",
    "const BROTLI_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream-index.br.json')\n",
    "//const TOKEN_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream-tokens.json')\n",
    "//const XML_FILE = path.join(process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE, 'Downloads', 'enwiki-20250420-pages-articles-multistream.xml.bz2')\n",
    "\n",
    "\n",
    "async function readWikimedia(search) {\n",
    "\n",
    "  if (search) {\n",
    "    search = search.toLocaleLowerCase().split(/[ \\s,:-_]/g).filter(s => s.length > 3)\n",
    "  }\n",
    "\n",
    "  if (!fs.existsSync(INDEX_FILE)) {\n",
    "    throw new Error('Index file not found: https://dumps.wikimedia.org/')\n",
    "  }\n",
    "\n",
    "  // use memory cache\n",
    "  if (allTitles.length) {\n",
    "  } else {\n",
    "    if (!fs.existsSync(BROTLI_FILE)) {\n",
    "      let lastPercentRounded = 0\n",
    "      await new Promise(resolve => readIndex(search, ({ offset, length }, titles, finished) => {\n",
    "        let percentRounded = Math.round(finished)\n",
    "        if (percentRounded != lastPercentRounded) {\n",
    "          console.log('reading index:', finished, titles[0])\n",
    "          lastPercentRounded = percentRounded\n",
    "        }\n",
    "        // last time we have to run this to form a brotli file\n",
    "        for (let i = 0; i < titles.length; i++) {\n",
    "          allTitles.push({ offset, length, title: titles[i] })\n",
    "        }\n",
    "\n",
    "        // Compress with Brotli\n",
    "        const data = zlib.brotliCompressSync(Buffer.from(titles.join('\\0')));\n",
    "        compressed.push([offset, length, data.toString('base64')])\n",
    "\n",
    "        if (finished === true) {\n",
    "          resolve()\n",
    "        }\n",
    "      }))\n",
    "\n",
    "      fs.writeFileSync(BROTLI_FILE, '')\n",
    "      let lastPercentRounded2 = 0\n",
    "      compressed.forEach((c, i) => {\n",
    "        let percentRounded = Math.round(i / compressed.length * 100)\n",
    "        if (percentRounded != lastPercentRounded2) {\n",
    "          console.log('writing index:', percentRounded)\n",
    "          lastPercentRounded2 = percentRounded\n",
    "        }\n",
    "        fs.appendFileSync(BROTLI_FILE, JSON.stringify(c) + '\\n')\n",
    "      })\n",
    "    } else {\n",
    "      const length = fs.statSync(BROTLI_FILE).size\n",
    "      const fileStream = fs.createReadStream(BROTLI_FILE);\n",
    "      const rl = readline.createInterface({\n",
    "        input: fileStream,\n",
    "        crlfDelay: Infinity, // Recognize all instances of CR LF as a single line break\n",
    "      });\n",
    "      let processed = 0\n",
    "      let lastPercentRounded = 0\n",
    "      for await (const line of rl) {\n",
    "        // Each line in the file will be available here\n",
    "        let c = JSON.parse(line)\n",
    "        let decompressed = zlib.brotliDecompressSync(Buffer.from(c[2], 'base64'))\n",
    "        let titles = decompressed.toString('utf-8').split('\\0')\n",
    "        let percentRounded = Math.round(processed / length * 100)\n",
    "        if (percentRounded != lastPercentRounded) {\n",
    "          console.log('reading index:', percentRounded, titles[0])\n",
    "          lastPercentRounded = percentRounded\n",
    "        }\n",
    "        titles.forEach(t => {\n",
    "          allTitles.push({ offset: c[0], length: c[1], title: t })\n",
    "        })\n",
    "        processed += line.length + 1\n",
    "      }\n",
    "\n",
    "    }\n",
    "  }\n",
    "\n",
    "  fuzzySearch(\n",
    "    search.join(' '),\n",
    "    allTitles,\n",
    "    { keySelector: (obj) => obj.title },\n",
    "  );\n",
    "  console.log(matches)\n",
    "  /*\n",
    "  let matches = []\n",
    "  for (let i = 0; i < allTitles.length; i++) {\n",
    "    //if(search[i] === 'constructor') search[i] = '$constructor'\n",
    "    //if(typeof allTokens[search[i]] !== 'undefined') {\n",
    "    //  matches = matches.concat(allTokens[search[i]])\n",
    "    //}\n",
    "    if (search) {\n",
    "      let match = search.filter(s => allTitles[i].title.toLocaleLowerCase().includes(s)).length\n",
    "      if (match >= search.length - 1 && match > 0) {\n",
    "        console.log('match found in set:', search, allTitles[i].title)\n",
    "        matches.push(allTitles[i])\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  */\n",
    "\n",
    "  return matches.map(m => m.item)\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = readWikimedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3636d0a",
   "metadata": {},
   "source": [
    "\n",
    "### load index\n",
    "\n",
    "load wikimedia index?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251cbb0",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "//const zlib = require('zlib');\n",
    "const PROFILE_HOME = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "const INDEX_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream-index.txt')\n",
    "const XML_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream.xml.bz2')\n",
    "\n",
    "function readIndex(search, callback) {\n",
    "  const length = fs.statSync(INDEX_FILE).size\n",
    "  const stream = fs.createReadStream(INDEX_FILE, {\n",
    "    highWaterMark: 64 * 1024,\n",
    "  });\n",
    "  let lastOffset\n",
    "  let leftover = ''\n",
    "  let lastTitles = []\n",
    "  let offsetCount = 0\n",
    "  stream.on('data', chunk => {\n",
    "    offsetCount += chunk.length\n",
    "    const lines = (leftover + chunk.toString()).split('\\n');\n",
    "    leftover = lines.pop();\n",
    "    for (const line of lines) {\n",
    "      const [offsetStr, pageId, title] = line.split(':');\n",
    "      const currentOffset = parseInt(offsetStr)\n",
    "      if (currentOffset && currentOffset !== lastOffset) {\n",
    "        if (lastOffset) {\n",
    "          callback({ length: currentOffset - lastOffset, offset: lastOffset }, lastTitles, offsetCount / length * 100.0);\n",
    "        }\n",
    "        lastOffset = currentOffset\n",
    "        lastTitles = []\n",
    "      }\n",
    "      lastTitles.push(title.toLocaleLowerCase())\n",
    "    }\n",
    "  });\n",
    "\n",
    "  stream.on('end', () => {\n",
    "    callback({ length: fs.statSync(XML_FILE).size - lastOffset, offset: lastOffset }, lastTitles, true);\n",
    "  });\n",
    "}\n",
    "\n",
    "module.exports = readIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e475b",
   "metadata": {},
   "source": [
    "\n",
    "### extract xml\n",
    "\n",
    "extract wikimedia chunk?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f9c46",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "//const zlib = require('zlib');\n",
    "const { PassThrough, Transform } = require('stream');\n",
    "const XmlStream = require('xml-stream');\n",
    "const bz2 = require('unbzip2-stream');\n",
    "\n",
    "const XML_FILE = path.join(process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE, 'Downloads', 'enwiki-20250420-pages-articles-multistream.xml.bz2')\n",
    "\n",
    "// Extract and parse one chunk\n",
    "function extractChunk(startOffset, endOffset, callback) {\n",
    "  console.log('reading:', startOffset, endOffset)\n",
    "  const fileStream = fs.createReadStream(XML_FILE, {\n",
    "    start: startOffset,\n",
    "    end: endOffset - 1 // end is inclusive\n",
    "  });\n",
    "\n",
    "  const decompress = bz2();\n",
    "\n",
    "  // 2. Create a Transform stream to replace `\\n`\n",
    "  const normalizeNewlines = new Transform({\n",
    "    decodeStrings: false,\n",
    "    transform(chunk, encoding, callback) {\n",
    "      // Replace or remove newlines here:\n",
    "      const text = chunk.toString().replaceAll('&', '!amp;').replaceAll(/\\n/g, '&lt;br /&gt;'); // or replace(/\\n/g, ' ')\n",
    "      this.push(text);\n",
    "      callback();\n",
    "    }\n",
    "  });\n",
    "  \n",
    "  // 3. Compose stream: wrapperStart + transformed decompressed + wrapperEnd\n",
    "  const wrapperStart = new PassThrough();\n",
    "  wrapperStart.end('<mediawiki>');\n",
    "  \n",
    "  const wrapperEnd = new PassThrough();\n",
    "  wrapperEnd.end('</mediawiki>');\n",
    "  \n",
    "  const decompressedStream = fileStream.pipe(decompress).pipe(normalizeNewlines);\n",
    "  \n",
    "  const fullStream = new PassThrough();\n",
    "  wrapperStart.pipe(fullStream, { end: false });\n",
    "  decompressedStream.pipe(fullStream, { end: false });\n",
    "  decompressedStream.on('end', () => {\n",
    "    wrapperEnd.pipe(fullStream);\n",
    "  });\n",
    "  \n",
    "  // 4. Pipe into XmlStream\n",
    "  const xml = new XmlStream(fullStream);\n",
    "\n",
    "  xml.preserve('text'); // This is a key part\n",
    "  xml.collect('text');\n",
    "  \n",
    "  xml.on('endElement: page', page => {\n",
    "    //console.log(`Title: ${page.title}`);\n",
    "    callback(page)\n",
    "  });\n",
    "\n",
    "  xml.on('end', () => {\n",
    "    console.log(`Chunk finished\\n`);\n",
    "    callback(false);\n",
    "  });\n",
    "\n",
    "  xml.on('error', err => {\n",
    "    console.error('XML error:', err);\n",
    "  });\n",
    "\n",
    "  decompress.on('error', err => {\n",
    "    console.error('BZ2 decompression error:', err);\n",
    "  });\n",
    "}\n",
    "\n",
    "module.exports = extractChunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ebbaf",
   "metadata": {},
   "source": [
    "\n",
    "### convert to html\n",
    "\n",
    "wikimedia-page.html?\n",
    "\n",
    "ROUTE = /wikimedia-page.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19ee42",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const {decode} = require('html-entities')\n",
    "const extractChunk = importer.import('extract wikimedia chunk')\n",
    "const preprocessDoc = importer.import('mediawiki text preprocessor')\n",
    "//const wtf = require('wtf_wikipedia')\n",
    "const Mustache = require('mustache');\n",
    "//const parsoid = require('parsoid');\n",
    "\n",
    "async function extractWikimedia(req, res, next) {\n",
    "  console.log(req.method, req.originalUrl)\n",
    "  let offset = req.body['offset'] || req.params['offset'] || req.query['offset'] || req.cookies['offset']\n",
    "  let length = req.body['length'] || req.params['length'] || req.query['length'] || req.cookies['length']\n",
    "  let search = req.body['search'] || req.params['search'] || req.query['search'] || req.cookies['search']\n",
    "  if (!search) {\n",
    "    throw new Error('Page not found.')\n",
    "  }\n",
    "  let markdown = ''\n",
    "  let xmlPage\n",
    "  offset = parseInt(offset)\n",
    "  length = parseInt(length)\n",
    "  try {\n",
    "    await new Promise(resolve => extractChunk(offset, offset + length, page => {\n",
    "      if (page === false) {\n",
    "        return resolve()\n",
    "      }\n",
    "\n",
    "      if (page.title.toLocaleLowerCase().includes(search.toLocaleLowerCase())) {\n",
    "        //console.log(page.revision.text[0]['$children'].slice(100))\n",
    "        xmlPage = page\n",
    "        markdown = decode(page.revision.text[0]['$children'].join('')\n",
    "          .replaceAll('!amp;', '&')\n",
    "          .replaceAll('<br />', '\\n')); // for parsing\n",
    "      }\n",
    "    }))\n",
    "\n",
    "    //let doc = wtf()\n",
    "    let template = importer.interpret('wikimedia mustache template').code\n",
    "    //console.log(markdown)\n",
    "    //fs.writeFileSync(__dirname + '/../test.md', markdown)\n",
    "    const preprocessed = preprocessDoc(markdown);\n",
    "    const content = Mustache.render(template, Object.assign({\n",
    "      title: xmlPage.title,\n",
    "      source: markdown.replaceAll('<', '&lt;').replaceAll('>', '&gt;'),\n",
    "    }, preprocessed)); //await wikitextToHTML(markdown)\n",
    "    const html = Mustache.render(importer.interpret('wikiemedia clone index').code, {\n",
    "      content: content,\n",
    "      TIMESTAMP: Date.now()\n",
    "    });\n",
    "    return res.send(html)\n",
    "  } catch (e) {\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "module.exports = extractWikimedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635dc24",
   "metadata": {},
   "source": [
    "\n",
    "### fix wikimedia titles?\n",
    "\n",
    "somehow the titles are only partial and don't match the XML title, lets fix this and recompress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9809f8",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "const readIndex = importer.import('load wikimedia index')\n",
    "const zlib = require('zlib');\n",
    "const extractChunk = importer.import('extract wikimedia chunk')\n",
    "\n",
    "const PROFILE_HOME = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "const BROTLI_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream-index.br.json')\n",
    "\n",
    "async function fixTitles() {\n",
    "  let indexes = []\n",
    "  let lastPercentRounded = 0\n",
    "  await new Promise(resolve => readIndex('', ({ offset, length }, titles, finished) => {\n",
    "    let percentRounded = Math.round(finished)\n",
    "    if (percentRounded !== lastPercentRounded) {\n",
    "      console.log('reading index:', finished, titles[0])\n",
    "      lastPercentRounded = percentRounded\n",
    "    }\n",
    "\n",
    "    indexes.push([offset, length])\n",
    "\n",
    "    if (finished === true) {\n",
    "      resolve()\n",
    "    }\n",
    "  }))\n",
    "\n",
    "  let lastPercentRounded3 = 0\n",
    "  for (let i = 0; i < indexes.length; i++) {\n",
    "    let percentRounded = Math.round(i / indexes.length * 100)\n",
    "    if (percentRounded !== lastPercentRounded3) {\n",
    "      console.log('reindexing titles:', percentRounded)\n",
    "      lastPercentRounded3 = percentRounded\n",
    "    }\n",
    "\n",
    "    // all we need is the start address and length\n",
    "    let titles = []\n",
    "    await new Promise(resolve => extractChunk(indexes[i][0], indexes[i][0] + indexes[i][1], page => {\n",
    "      if (page === false) {\n",
    "        return resolve()\n",
    "      }\n",
    "\n",
    "      titles.push(page.title)\n",
    "    }))\n",
    "\n",
    "    const data = zlib.brotliCompressSync(Buffer.from(titles.join('\\0')));\n",
    "    indexes[i][2] = data.toString('base64')\n",
    "  }\n",
    "\n",
    "  fs.writeFileSync(BROTLI_FILE, '')\n",
    "  let lastPercentRounded2 = 0\n",
    "  indexes.forEach((c, i) => {\n",
    "    let percentRounded = Math.round(i / indexes.length * 100)\n",
    "    if (percentRounded !== lastPercentRounded2) {\n",
    "      console.log('writing index:', percentRounded)\n",
    "      lastPercentRounded2 = percentRounded\n",
    "    }\n",
    "    fs.appendFileSync(BROTLI_FILE, JSON.stringify(c) + '\\n')\n",
    "  })\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = fixTitles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815ae20",
   "metadata": {},
   "source": [
    "\n",
    "extract all wikimedia xml?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6b40a",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "//const zlib = require('zlib');\n",
    "const bz2 = require('unbzip2-stream');\n",
    "const readIndex = importer.import('load wikimedia index')\n",
    "\n",
    "const PROFILE_HOME = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "const XML_FILE = path.join(PROFILE_HOME, 'Downloads', 'enwiki-20250420-pages-articles-multistream.xml.bz2')\n",
    "\n",
    "async function fixTitles() {\n",
    "  let indexes = []\n",
    "  let lastPercentRounded = 0\n",
    "  await new Promise(resolve => readIndex('', ({ offset, length }, titles, finished) => {\n",
    "    let percentRounded = Math.round(finished)\n",
    "    if (percentRounded !== lastPercentRounded) {\n",
    "      console.log('reading index:', finished, titles[0])\n",
    "      lastPercentRounded = percentRounded\n",
    "    }\n",
    "\n",
    "    indexes.push([offset, length])\n",
    "\n",
    "    if (finished === true) {\n",
    "      resolve()\n",
    "    }\n",
    "  }))\n",
    "\n",
    "\n",
    "  let lastPercentRounded3 = 0\n",
    "  for (let i = 0; i < indexes.length; i++) {\n",
    "    let percentRounded = Math.round(i / indexes.length * 100)\n",
    "    if (percentRounded !== lastPercentRounded3) {\n",
    "      console.log('extracting titles:', percentRounded)\n",
    "      lastPercentRounded3 = percentRounded\n",
    "    }\n",
    "\n",
    "    let outputPath = path.join(PROFILE_HOME, 'Downloads', 'wikimedia', 'enwiki-20250420-pages-articles-multistream-' + indexes[i][0] + '.xml')\n",
    "    if(fs.existsSync(outputPath)) {\n",
    "      continue\n",
    "    }\n",
    "\n",
    "    const fileStream = fs.createReadStream(XML_FILE, {\n",
    "      start: indexes[i][0],\n",
    "      end: indexes[i][0] + indexes[i][1] - 1 // end is inclusive\n",
    "    });\n",
    "\n",
    "    const decompressed = fileStream.pipe(bz2());\n",
    "\n",
    "    await new Promise((resolve, reject) => {\n",
    "      const writeStream = fs.createWriteStream(outputPath);\n",
    "      decompressed.pipe(writeStream);\n",
    "      writeStream.on('finish', resolve);\n",
    "      writeStream.on('error', reject);\n",
    "    });\n",
    "\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = fixTitles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52cb43",
   "metadata": {},
   "source": [
    "\n",
    "## mediatext processor\n",
    "\n",
    "this was interesting, it's like a less cool version of markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a57f25",
   "metadata": {},
   "source": [
    "\n",
    "### process document\n",
    "\n",
    "mediawiki text preprocessor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73220c32",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const balanced = importer.import('balanced')\n",
    "const Mustache = require('mustache');\n",
    "const { Remarkable } = require('remarkable');\n",
    "const md = new Remarkable({ html: true, xhtmlOut: true, breaks: true });\n",
    "const {\n",
    "  matchTable,\n",
    "  matchWikiLinks,\n",
    "  matchSection,\n",
    "} = importer.import('match wikitext elements')\n",
    "const coalesceByPattern = importer.import('coalesce regex tool')\n",
    "\n",
    "function parseTemplates(text) {\n",
    "  const templates = {}\n",
    "  const citations = [];\n",
    "  let result = '';\n",
    "  let count = 0\n",
    "  while (true) {\n",
    "    const match = balanced('{{', '}}', text);\n",
    "    if (!match) break;\n",
    "\n",
    "    const before = text.slice(0, match.start);\n",
    "    const after = text.slice(match.end + 2);\n",
    "    const body = match.body;\n",
    "    const name = body.split('|')[0].trim();\n",
    "    const params = {};\n",
    "\n",
    "    for (const [, key, value] of body.matchAll(/\\|([^=|]+)=([^|]*)/g)) {\n",
    "      params[key.trim()] = value.trim();\n",
    "    }\n",
    "\n",
    "    const citeData = Object.assign({ name, count, id: name + count }, params);\n",
    "    if (name.startsWith('cite ')) {\n",
    "      citations.push(citeData);\n",
    "      templates[name + count] = Mustache.render(`\n",
    "      <span class=\"author\">\n",
    "      {{#first1}}{{first1}} {{/first1}}{{#last1}}{{last1}}{{/last1}}\n",
    "    </span>\n",
    "    {{#title}}<span class=\"title\">\"<a href=\"{{url}}\" target=\"_blank\">{{title}}</a>\"</span>{{/title}}\n",
    "    {{#work}}<span class=\"work\"><em>{{work}}</em></span>{{/work}},\n",
    "    {{#date}}<span class=\"date\">{{date}}</span>{{/date}}\n",
    "    {{#archive-date}}<span class=\"date\">{{archive-date}}</span>{{/archive-date}}\n",
    "    {{#year}}<span class=\"date\">{{year}}</span>{{/year}}.\n",
    "    {{#access-date}}<span class=\"access-date\">(accessed {{access-date}})</span>{{/access-date}}`, citeData)\n",
    "    } else {\n",
    "      templates[name + count] = `[[[${name}${count}]]]`\n",
    "    }\n",
    "    result += before + (name.toLocaleLowerCase() === 'reflist' ? '{{{reflist}}}' : `{{{${name}${count}}}}`);\n",
    "    text = after;\n",
    "    count++\n",
    "  }\n",
    "\n",
    "  return { text: result + text, citations, templates };\n",
    "}\n",
    "\n",
    "function coalesceTags(text, citations, tags, placeholderPrefix = 'tag') {\n",
    "  const regex = new RegExp(`<(${tags.join(\"|\")})\\\\b([^>]*)\\\\s*(\\\\/>)|<(${tags.join(\"|\")})\\\\b([^>]*)>([\\\\s\\\\S]*?)<\\\\/\\\\4>`, 'gi');\n",
    "  const replacements = {};\n",
    "  let count = 0;\n",
    "  let ids = citations.map(c => c.id)\n",
    "  let count2 = 0\n",
    "  const content = text.replaceAll(regex, (match, tag, attr, _2, tag2, attr2) => {\n",
    "    const key = `${placeholderPrefix}${count++}`;\n",
    "    if ((tag || tag2) === 'ref') {\n",
    "      // mark any corresponding citations for inclusion in reflist\n",
    "      //console.log(match, Array.from(match.matchAll(/name=\"([^\"]*)\"/g)))\n",
    "      Array.from(match.matchAll(/{{{(.+)}}}/g)).concat(Array.from(match.matchAll(/name=\"({^\"}*)\"/g))).forEach(m => {\n",
    "        let i = ids.indexOf(m[1])\n",
    "        if (i > -1) {\n",
    "          citations[i].included = true\n",
    "          replacements[key] = `<sup><a href=\"#ref_${count2 + 1}\">[${count2 + 1}]</a></sup>`;\n",
    "          count2++\n",
    "        }\n",
    "      })\n",
    "    } else {\n",
    "      replacements[key] = match.replaceAll('<', '&lt;').replaceAll('>', '&gt;');\n",
    "    }\n",
    "    return `{{{${key}}}}`;\n",
    "  });\n",
    "\n",
    "  return { content, replacements };\n",
    "}\n",
    "\n",
    "function coalesceLinks(text, startToken, endToken, matchFn, placeholderPrefix = 'link') {\n",
    "  const links = [];\n",
    "  let result = '';\n",
    "  let count = 0;\n",
    "\n",
    "  while (true) {\n",
    "    const match = balanced(startToken, endToken, text);\n",
    "    if (!match) break;\n",
    "\n",
    "    const before = text.slice(0, match.start);\n",
    "    const after = text.slice(match.end + endToken.length);\n",
    "    const parsed = matchFn(match.body);\n",
    "\n",
    "    links.push(parsed);\n",
    "    result += before + `{{{${placeholderPrefix}${count}}}}`;\n",
    "    count++;\n",
    "    text = after;\n",
    "  }\n",
    "\n",
    "  return { content: result + text, links };\n",
    "}\n",
    "\n",
    "function renderSections(wikitext, linkMap, citations) {\n",
    "  const sectionRegex = /(\\={1,6})\\s*(.*?)\\s*\\1\\s*/gm;\n",
    "  const sections = coalesceByPattern(wikitext, sectionRegex, matchSection);\n",
    "  const reflistTemplate = importer.interpret('wikimedia reflist mustache').code\n",
    "\n",
    "  return sections.map(s => {\n",
    "    const renderedContent = s.content.substr(s.raw.length)\n",
    "      .replaceAll(/;[ \\t]*(.+?)[ \\t]*\\n?:[ \\t]*(.+?)(?=\\n|$)/g, (_, term, def) => `<dl><dt>${term}</dt><dd>${def}</dd></dl>`)\n",
    "      .replaceAll(/(^|\\n)[ \\t]*>[ \\t]*(.*)/g, '$1<blockquote>$2</blockquote>');\n",
    "\n",
    "    return Object.assign({\n",
    "      depth: s.depth,\n",
    "      title: s.title,\n",
    "      content: Mustache.render(md.render(formatWikitextSimple(renderedContent)), Object.assign({}, {\n",
    "        reflist: Mustache.render(reflistTemplate, {\n",
    "          reflist: citations.filter(c => c.included).map((c, i) => Object.assign(c, { index: i + 1 }))\n",
    "        }),\n",
    "      }, linkMap))\n",
    "    }, linkMap);\n",
    "  });\n",
    "}\n",
    "\n",
    "function formatWikitextSimple(text) {\n",
    "  return text\n",
    "    .replaceAll(/-{4,}/g, '<hr />')\n",
    "    .replaceAll(/'''''(.*?)'''''/g, '<b><i>$1</i></b>')\n",
    "    .replaceAll(/'''(.*?)'''/g, '<b>$1</b>')\n",
    "    .replaceAll(/''(.*?)''/g, '<i>$1</i>')\n",
    "    .replaceAll(/(^|\\n|[ \\t])[ \\t]*\\*+[ \\t]*([^\\*\\n]+)/g, (_, _2, item) => `<ul><li>${item}</li></ul>`)\n",
    "    .replaceAll(/(^|\\n|[ \\t])[ \\t]*\\#+[ \\t]*([^\\#\\n]+)/g, (_, _2, item) => `<ol><li>${item}</li></ol>`)\n",
    "    .replaceAll(/<\\/ul>\\s*<ul>/g, '')\n",
    "    .replaceAll(/<\\/ol>\\s*<ol>/g, '');\n",
    "}\n",
    "\n",
    "function renderTable({ caption, rows }) {\n",
    "  const captionHtml = caption ? `<caption>${caption}</caption>` : '';\n",
    "\n",
    "  const rowsHtml = rows.map(cells => {\n",
    "    const cellsHtml = cells.map(cell =>\n",
    "      `<${cell.type}>${cell.text}</${cell.type}>`\n",
    "    ).join('');\n",
    "    return `<tr>${cellsHtml}</tr>`;\n",
    "  }).join('\\n');\n",
    "\n",
    "  return `<table class=\"wikitable\">\\n${captionHtml}\\n${rowsHtml}\\n</table>`;\n",
    "}\n",
    "\n",
    "function preprocessDoc(wikiText) {\n",
    "  const { text: withTemplates, templates, citations } = parseTemplates(wikiText);\n",
    "  const { content: withTags, replacements: tagMap } = coalesceTags(withTemplates, citations, [\"ref\", \"references\", \"syntaxhighlight\", \"code\", \"pre\", \"math\", \"gallery\", \"poem\"]);\n",
    "  const tablePattern = /\\{\\|[\\s\\S]*?\\|\\}/g;\n",
    "  const tables = coalesceByPattern(withTags, tablePattern, matchTable)\n",
    "  const renderTables = {}\n",
    "  const withTables = tables.map((t, i) => {\n",
    "    //console.log(tables[i].rows)\n",
    "    if (t.rows) {\n",
    "      return renderTable(t) + t.content.substr(t.raw.length)\n",
    "    } else {\n",
    "      return t.content.substr(t.raw.length)\n",
    "    }\n",
    "  }).join('')\n",
    "\n",
    "  const wikiLinkMatcher = match => matchWikiLinks((/(?:(\\w{2,12}):)?([^\\|\\]]+)(?:\\|([^\\]]+))?/gi).exec(match));\n",
    "  const { content: withLinks, links } = coalesceLinks(withTables, '[[', ']]', wikiLinkMatcher);\n",
    "  const linkMap = links.reduce((map, link, i) => {\n",
    "    if (link.type === 'file') return map;\n",
    "    map[`link${i}`] = `<a href=\"/wiki/${link.target}\">${link.label}</a>`;\n",
    "    return map;\n",
    "  }, tagMap);\n",
    "\n",
    "  const linkRegex = /\\[(https?):\\/\\/([^\\]]+)*\\]/g;\n",
    "  const pageLinks = coalesceByPattern(withLinks, linkRegex, matchWikiLinks)\n",
    "  //console.log(pageLinks)\n",
    "  const pageHtmlLinks = pageLinks.map((t, i) => {\n",
    "    if (t.raw.length === 0) {\n",
    "      return t.content\n",
    "    }\n",
    "    linkMap['link' + (links.length + i)] = `<a href=\"${t.lang}://${t.target}\">${t.lang === 'https' ? 'https://' : ''}${t.label}</a>`\n",
    "    return '{{{link' + (links.length + i) + '}}}' + t.content.substr(t.raw.length)\n",
    "  }).join('')\n",
    "\n",
    "  return { sections: renderSections(pageHtmlLinks, Object.assign(linkMap, templates, renderTables), citations) };\n",
    "}\n",
    "\n",
    "module.exports = preprocessDoc;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38753763",
   "metadata": {},
   "source": [
    "\n",
    "### match wikitext elements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437e44d",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "function matchTable(match) {\n",
    "  const tableText = match[0];\n",
    "  const lines = tableText.split(/\\r?\\n/).map(line => line.trim());\n",
    "  const table = {\n",
    "    caption: null,\n",
    "    rows: [],\n",
    "  };\n",
    "\n",
    "  let currentRow = [];\n",
    "\n",
    "  for (let line of lines) {\n",
    "    if (line.startsWith('{|')) continue;\n",
    "    if (line.startsWith('|}')) {\n",
    "      if (currentRow.length) table.rows.push(currentRow);\n",
    "      break;\n",
    "    }\n",
    "\n",
    "    if (line.startsWith('|+')) {\n",
    "      table.caption = line.substring(2).trim();\n",
    "    } else if (line.startsWith('|-')) {\n",
    "      if (currentRow.length) table.rows.push(currentRow);\n",
    "      currentRow = [];\n",
    "    } else if (line.startsWith('!')) {\n",
    "      // Header cells\n",
    "      const headers = line.substring(1).split(/!!/g).map(h => h.trim());\n",
    "      currentRow.push(...headers.map(text => ({ type: 'th', text })));\n",
    "    } else if (line.startsWith('|')) {\n",
    "      // Regular cells\n",
    "      const cells = line.substring(1).split(/\\|\\|/g).map(c => c.trim());\n",
    "      currentRow.push(...cells.map(text => ({ type: 'td', text })));\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return table\n",
    "}\n",
    "\n",
    "function matchWikiLinks(match) {\n",
    "  const langOrNs = match[1];\n",
    "  const target = match[2];\n",
    "  const label = match[3] || match[2];\n",
    "\n",
    "  let type = \"internal\";\n",
    "  if (langOrNs === \"Category\") type = \"category\";\n",
    "  if (langOrNs === \"File\") type = \"file\";\n",
    "  if (langOrNs === 'http' || langOrNs === 'https') type = \"url\";\n",
    "  else if (langOrNs && langOrNs.length === 2) type = \"interwiki\";\n",
    "\n",
    "  return {\n",
    "    type,\n",
    "    lang: langOrNs,\n",
    "    target,\n",
    "    label,\n",
    "  };\n",
    "}\n",
    "\n",
    "function matchSection(match) {\n",
    "  const depth = match[1].length;\n",
    "  const title = match[2].trim();\n",
    "  return { depth, title };\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  matchTable,\n",
    "  matchWikiLinks,\n",
    "  matchSection,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a7b089",
   "metadata": {},
   "source": [
    "\n",
    "### coalesce regex tool?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04099e85",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "function coalesceByPattern(text, regex, extractLabel = (match) => match[0]) {\n",
    "  const matches = [...text.matchAll(regex)];\n",
    "  const sections = [];\n",
    "\n",
    "  if (matches.length === 0) {\n",
    "    return [{ offset: 0, match: null, content: text, raw: '' }];\n",
    "  } else {\n",
    "    sections.push({\n",
    "      offset: 0,\n",
    "      match: null,\n",
    "      content: text.slice(0, matches[0].index),\n",
    "      raw: ''\n",
    "    })\n",
    "  }\n",
    "\n",
    "  for (let i = 0; i < matches.length; i++) {\n",
    "    const current = matches[i];\n",
    "    const next = matches[i + 1];\n",
    "\n",
    "    const startOffset = current.index;\n",
    "    const endOffset = next ? next.index : text.length;\n",
    "\n",
    "    sections.push(Object.assign({}, extractLabel(current), {\n",
    "      offset: startOffset,\n",
    "      match: current,\n",
    "      content: text.slice(startOffset, endOffset),\n",
    "      raw: current[0]\n",
    "    }));\n",
    "  }\n",
    "\n",
    "  return sections;\n",
    "}\n",
    "\n",
    "module.exports = coalesceByPattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccfe99",
   "metadata": {},
   "source": [
    "\n",
    "## frontend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2e70f",
   "metadata": {},
   "source": [
    "\n",
    "### mustache templates\n",
    "\n",
    "wikimedia mustache template?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024582f7",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<h1>{{title}}</h1>\n",
    "<p><strong>Page ID:</strong> {{pageID}}</p>\n",
    "\n",
    "{{#categories.length}}\n",
    "<p><strong>Categories:</strong> {{#categories}}<span>{{.}}</span> {{/categories}}</p>\n",
    "{{/categories.length}}\n",
    "\n",
    "{{#sections}}\n",
    "<h{{depth}}>{{title}}</h{{depth}}>\n",
    "\n",
    "{{{content}}}\n",
    "\n",
    "{{/sections}}\n",
    "\n",
    "<pre><code>{{{source}}}</code></pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885cd7e",
   "metadata": {},
   "source": [
    "\n",
    "wikimedia reflist mustache?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02cda0",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<ol>\n",
    "  {{#reflist}}\n",
    "  <li id=\"ref_{{index}}\" class=\"citation\">\n",
    "    <span class=\"author\">\n",
    "      {{#first1}}{{first1}} {{/first1}}{{#last1}}{{last1}}{{/last1}}\n",
    "    </span>\n",
    "    {{#title}}<span class=\"title\">\"<a href=\"{{url}}\" target=\"_blank\">{{title}}</a>\"</span>{{/title}}\n",
    "    {{#work}}<span class=\"work\"><em>{{work}}</em></span>{{/work}},\n",
    "    {{#date}}<span class=\"date\">{{date}}</span>{{/date}}.\n",
    "    {{#access-date}}<span class=\"access-date\">(accessed {{access-date}})</span>{{/access-date}}\n",
    "  </li>\n",
    "  {{/reflist}}\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61187143",
   "metadata": {},
   "source": [
    "\n",
    "### index\n",
    "\n",
    "wikimedia service?\n",
    "\n",
    "ROUTE[] = /wiki/:article\n",
    "\n",
    "ROUTE[] = /wiki\n",
    "\n",
    "ROOT = true\n",
    "\n",
    "DEFAULT = true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ba501",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const readWikimedia = importer.import('list wikimedia articles')\n",
    "const levSearch = importer.import('search levenshtein distance')\n",
    "const levDist = importer.import('find the levenshtein distance')\n",
    "//const extractWikimedia = importer.import('wikimedia-page.html')\n",
    "const Mustache = require('mustache');\n",
    "\n",
    "async function searchWikimedia(req, res, next) {\n",
    "  let article = req.body['article'] || req.params['article'] || req.query['article'] || req.cookies['article']\n",
    "  let searchResults = await readWikimedia(article)\n",
    "\n",
    "  if (searchResults.length === 0) {\n",
    "    throw new Error('Page not found: ' + article)\n",
    "  }\n",
    "\n",
    "  console.log('sorting:', searchResults.length)\n",
    "  let sorted = levSearch(searchResults, { keys: ['title'] }, article).slice(0, 100)\n",
    "\n",
    "  console.log(sorted[0].title, levDist(sorted[0].title, article), sorted[1].title, levDist(sorted[1].title, article))\n",
    "  // check for multiple good matches and display a list\n",
    "  if ((sorted[0] && levDist(sorted[0].title, article) / article.length < 0.1\n",
    "    && sorted[1] && levDist(sorted[1].title, article) / article.length < 0.1\n",
    "    // because sometimes there is a matching redirect page\n",
    "    && sorted[2] && levDist(sorted[2].title, article) / article.length < 0.1)\n",
    "    // no great matches\n",
    "    || (levDist(sorted[0].title, article) / article.length > 0.5)\n",
    "  ) {\n",
    "    let template = importer.interpret('wikimedia mustache template').code\n",
    "    const content = Mustache.render(`\n",
    "<h1>Search results</h1>\n",
    "\n",
    "{{#sections}}\n",
    "<h2><a href=\"/wikimedia-page.html?offset={{offset}}&length={{length}}&search={{title}}\">{{title}}</a></h2>\n",
    "{{/sections}}\n",
    "`, {\n",
    "      sections: sorted\n",
    "    });\n",
    "    const html = Mustache.render(importer.interpret('wikiemedia clone index').code, {\n",
    "      content: content,\n",
    "      TIMESTAMP: Date.now()\n",
    "    });\n",
    "    return res.send(html)\n",
    "  }\n",
    "\n",
    "  //let content = extractWikimedia(sorted[0].offset, sorted[0].length, sorted[0].title)\n",
    "  return res.redirect('/wikimedia-page.html?offset=' + sorted[0].offset + '&length=' + sorted[0].length + '&search=' + sorted[0].title)\n",
    "}\n",
    "\n",
    "module.exports = searchWikimedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127906c",
   "metadata": {},
   "source": [
    "\n",
    "### wrapper html\n",
    "\n",
    "wikiemedia clone index?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f5368",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "  <title>Shleppedia</title>\n",
    "  <link rel=\"stylesheet\" href=\"/wikimedia-style.css?t={{TIMESTAMP}}\">\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "\n",
    "  <header>\n",
    "    <h1>Shleppedia</h1>\n",
    "    <form id=\"search-box\" method=\"get\" action=\"/wiki\">\n",
    "      <input name=\"article\" type=\"text\" placeholder=\"Search articles...\">\n",
    "    </form>\n",
    "    <p>The slightly off-brand, slightly cooler encyclopedia clone</p>\n",
    "  </header>\n",
    "\n",
    "\n",
    "  <div id=\"main-content\">\n",
    "    <!-- Mustache-rendered content goes here -->\n",
    "    {{{ content }}}\n",
    "\n",
    "    <div class=\"infobox\">\n",
    "      <details open>\n",
    "        <summary>ðŸ§  Infobox Decoder</summary>\n",
    "        <div id=\"infobox-area\">\n",
    "          <!-- Dynamically rendered infobox properties here -->\n",
    "          <p>No infoboxes loaded yet.</p>\n",
    "        </div>\n",
    "      </details>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eade925",
   "metadata": {},
   "source": [
    "\n",
    "### stylesheet\n",
    "\n",
    "wikimedia-style?\n",
    "\n",
    "ROUTE = /wikimedia-style.css\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20e3bc",
   "metadata": {
    "vscode": {
     "languageId": "css"
    }
   },
   "outputs": [],
   "source": [
    "body {\n",
    "  margin: 0;\n",
    "  padding: 0;\n",
    "  font-family: system-ui, sans-serif;\n",
    "  background: #f9f9f9;\n",
    "  color: #333;\n",
    "}\n",
    "\n",
    "header {\n",
    "  background: #4b8bbe;\n",
    "  color: white;\n",
    "  padding: 1rem 2rem;\n",
    "  display: flex;\n",
    "  align-items: center;\n",
    "  justify-content: space-between;\n",
    "  flex-wrap: wrap;\n",
    "  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "\n",
    "header h1 {\n",
    "  font-size: 1.8rem;\n",
    "  margin: 0;\n",
    "  flex: 0;\n",
    "}\n",
    "\n",
    "#search-box {\n",
    "  flex: 2;\n",
    "  max-width: 400px;\n",
    "  display: flex;\n",
    "  justify-content: flex-end;\n",
    "}\n",
    "\n",
    "#search-box input {\n",
    "  width: 100%;\n",
    "  padding: 0.5rem 0.75rem;\n",
    "  font-size: 1rem;\n",
    "  border: 1px solid #ccc;\n",
    "  border-radius: 4px;\n",
    "  outline: none;\n",
    "  transition: border-color 0.3s;\n",
    "  background: white;\n",
    "}\n",
    "\n",
    "#search-box input:focus {\n",
    "  border-color: #2e6fa3;\n",
    "}\n",
    "\n",
    "#main-content {\n",
    "  width: 100%;\n",
    "  max-width: 1200px;\n",
    "  margin: 2rem auto;\n",
    "  padding: 2rem;\n",
    "  background: white;\n",
    "  border-radius: 8px;\n",
    "  box-shadow: 0 0 8px rgba(0, 0, 0, 0.05);\n",
    "}\n",
    "\n",
    ".infobox {\n",
    "  margin-top: 2rem;\n",
    "  padding: 1rem;\n",
    "  border-left: 4px solid #4b8bbe;\n",
    "  background: #eef6ff;\n",
    "}\n",
    "\n",
    "details summary {\n",
    "  cursor: pointer;\n",
    "  font-weight: bold;\n",
    "}\n",
    "\n",
    "a {\n",
    "  color: #2970b8;\n",
    "  text-decoration: none;\n",
    "}\n",
    "\n",
    "a:hover {\n",
    "  text-decoration: underline;\n",
    "}\n",
    "\n",
    ".citation {\n",
    "  font-size: 0.95rem;\n",
    "  line-height: 1.4;\n",
    "  margin-bottom: 0.5rem;\n",
    "  color: #444;\n",
    "}\n",
    "\n",
    ".citation .author {\n",
    "  font-weight: bold;\n",
    "}\n",
    "\n",
    ".citation .title a {\n",
    "  text-decoration: none;\n",
    "  color: #1a0dab;\n",
    "}\n",
    "\n",
    ".citation .work {\n",
    "  font-style: italic;\n",
    "}\n",
    "\n",
    ".citation .date,\n",
    ".citation .access-date {\n",
    "  color: #777;\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
