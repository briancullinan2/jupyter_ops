{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658c6fd",
   "metadata": {},
   "source": [
    "\n",
    "list wikimedia articles?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84087e47",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const path = require('path')\n",
    "//const zlib = require('zlib');\n",
    "const { PassThrough, Readable } = require('stream');\n",
    "const XmlStream = require('xml-stream');\n",
    "const bz2 = require('unbzip2-stream');\n",
    "\n",
    "const INDEX_FILE = path.join(process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE, 'Downloads', 'enwiki-20250420-pages-articles-multistream-index.txt')\n",
    "const XML_FILE = path.join(process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE, 'Downloads', 'enwiki-20250420-pages-articles-multistream.xml.bz2')\n",
    "\n",
    "async function readWikimedia(search) {\n",
    "\n",
    "  // === Step 3: Tie it all together ===\n",
    "  await new Promise(resolve => readIndex(search, ({ offset, length, titles }, finished) => {\n",
    "\n",
    "    let isMatch = false\n",
    "    if (search) {\n",
    "      for (let i = 0; i < titles.length; i++) {\n",
    "        if (titles[i].includes(search.toLocaleLowerCase())) {\n",
    "          console.log('match found in set:', titles[i])\n",
    "          isMatch = true\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    } else {\n",
    "      isMatch = true\n",
    "    }\n",
    "\n",
    "    if (isMatch) {\n",
    "      extractChunk(offset, offset + length, () => {\n",
    "\n",
    "\n",
    "\n",
    "      });\n",
    "    }\n",
    "\n",
    "    if (finished) {\n",
    "      resolve()\n",
    "    }\n",
    "  }));\n",
    "\n",
    "}\n",
    "\n",
    "function readIndex(search, callback) {\n",
    "  const stream = fs.createReadStream(INDEX_FILE, {\n",
    "    highWaterMark: 64 * 1024,\n",
    "  });\n",
    "  let lastOffset\n",
    "  let leftover = ''\n",
    "  let lastTitles = []\n",
    "  stream.on('data', chunk => {\n",
    "    const lines = (leftover + chunk.toString()).split('\\n');\n",
    "    leftover = lines.pop();\n",
    "    for (const line of lines) {\n",
    "      const [offsetStr, pageId, title] = line.split(':');\n",
    "      const currentOffset = parseInt(offsetStr)\n",
    "      if (currentOffset && currentOffset !== lastOffset) {\n",
    "        if (lastOffset) {\n",
    "          callback({ length: currentOffset - lastOffset, offset: lastOffset, titles: lastTitles });\n",
    "        }\n",
    "        lastOffset = currentOffset\n",
    "        lastTitles = []\n",
    "      }\n",
    "      lastTitles.push(title.toLocaleLowerCase())\n",
    "    }\n",
    "  });\n",
    "\n",
    "  stream.on('end', () => {\n",
    "    callback({ length: fs.statSync(INDEX_FILE).size - lastOffset, offset: lastOffset, titles: lastTitles }, true);\n",
    "  });\n",
    "}\n",
    "\n",
    "// Extract and parse one chunk\n",
    "function extractChunk(startOffset, endOffset, callback) {\n",
    "  console.log('reading:', startOffset, endOffset)\n",
    "  const fileStream = fs.createReadStream(XML_FILE, {\n",
    "    start: startOffset,\n",
    "    end: endOffset - 1 // end is inclusive\n",
    "  });\n",
    "\n",
    "  const decompress = bz2();\n",
    "\n",
    "  const wrapperStart = Readable.from(['<root>']);\n",
    "  const wrapperEnd = Readable.from(['</root>']);\n",
    "\n",
    "  const decompressedStream = fileStream.pipe(decompress);\n",
    "\n",
    "  // Combine: <root> + decompressed + </root>\n",
    "  const fullStream = PassThrough();\n",
    "  wrapperStart.pipe(fullStream, { end: false });\n",
    "  decompressedStream.pipe(fullStream, { end: false });\n",
    "  decompressedStream.on('end', () => {\n",
    "    wrapperEnd.pipe(fullStream);\n",
    "  });\n",
    "\n",
    "  const xml = new XmlStream(fullStream);\n",
    "\n",
    "  xml.on('endElement: page', page => {\n",
    "    //console.log(`Title: ${page.title}`);\n",
    "  });\n",
    "\n",
    "  xml.on('end', () => {\n",
    "    console.log(`Chunk finished\\n`);\n",
    "    callback();\n",
    "  });\n",
    "\n",
    "  xml.on('error', err => {\n",
    "    console.error('XML error:', err);\n",
    "  });\n",
    "\n",
    "  decompress.on('error', err => {\n",
    "    console.error('BZ2 decompression error:', err);\n",
    "  });\n",
    "}\n",
    "\n",
    "module.exports = readWikimedia\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
