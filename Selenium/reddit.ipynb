{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Reddit\n",
    "\n",
    "Reddit automations, because I hate reddit, so I should interact with it in my own way instead of their way.\n",
    "\n",
    "Because whether i like it or not it is an information authority.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Auth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Login\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "reddit login?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const getClient = importer.import('selenium client')\n",
    "const { Builder, Browser, By, Key, until } = require('selenium-webdriver')\n",
    "\n",
    "//const PROFILE_PATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE;\n",
    "//const PASSWORDS_FILE = path.join(PROFILE_PATH, '.credentials', 'brian-reddit.json');\n",
    "\n",
    "async function enterReddit(driver) {\n",
    "  console.log('Reddit: Sign in required');\n",
    "\n",
    "  //var credentials = require(PASSWORDS_FILE)\n",
    "\n",
    "  //let body = await driver.findElement(By.css('body'))\n",
    "  await driver.wait(until.elementLocated(By.css('#post-submit-form')), 30000);\n",
    "  \n",
    "  let loginStill, loginStill2, loginStill3\n",
    "  try {\n",
    "    loginStill = await driver.findElement(By.css('#captcha-internal'))\n",
    "  } catch (e) {}\n",
    "  try {\n",
    "    loginStill2 = await driver.findElement(By.css('input[name*=\"session_password\"]'))\n",
    "  } catch (e) {}\n",
    "  try {\n",
    "    loginStill3 = await driver.findElement(By.xpath('//*[contains(text(),\"Check your notifications\")]'))\n",
    "  } catch (e) {}\n",
    "\n",
    "  if(loginStill3) {\n",
    "    await new Promise(resolve => setTimeout(resolve, 20000))\n",
    "    try {\n",
    "      loginStill = false\n",
    "      loginStill2 = false\n",
    "      loginStill3 = await driver.findElement(By.xpath('//*[contains(text(),\"Check your notifications\")]'))\n",
    "    } catch (e) {}\n",
    "  }\n",
    "\n",
    "  if(loginStill || loginStill2 || loginStill3) {\n",
    "    throw new Error('captcha')\n",
    "  }\n",
    "}\n",
    "\n",
    "async function loginReddit(driver) {\n",
    "  if(!driver) {\n",
    "    driver = await getClient();\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    let url = await driver.getCurrentUrl()\n",
    "    let loggedIn = url.indexOf('reddit') > -1 && url.indexOf('login') == -1\n",
    "\n",
    "    debugger\n",
    "    if(loggedIn) {\n",
    "      //if(await driver.findElement(By.xpath('iframe.authentication-iframe'))) {\n",
    "      //  await driver.frame((await driver.element('iframe.authentication-iframe')).value)\n",
    "        await enterReddit(driver)\n",
    "        //await frame()\n",
    "      //}\n",
    "    } else {\n",
    "      await driver.get('https://www.reddit.com/submit')\n",
    "\n",
    "      let loginLink, loginLink2\n",
    "      try {\n",
    "        loginLink = await driver.findElement(By.xpath('//auth-flow-link[text()[contains(.,\"Forgot password?\")]]'))\n",
    "      } catch (e) {}\n",
    "      try {\n",
    "        loginLink2 = await driver.findElement(By.xpath('//auth-flow-link[text()[contains(.,\"Sign up\")]]'))\n",
    "      } catch (e) {}\n",
    "\n",
    "      if(loginLink || loginLink2) {\n",
    "        await enterReddit(driver)\n",
    "      }\n",
    "    }\n",
    "  } catch (e) {\n",
    "    driver.quit() // avoid leaving sessions hanging around\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "\n",
    "  return driver\n",
    "}\n",
    "\n",
    "module.exports = loginReddit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Post\n",
    "\n",
    "create a post on reddit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "reddit post?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const loginReddit = importer.import('reddit login')\n",
    "const getClient = importer.import('selenium client')\n",
    "const getAllQuery = importer.import('selenium query')\n",
    "const { By } = require('selenium-webdriver')\n",
    "\n",
    "\n",
    "async function redditPost(driver, content, startPage) {\n",
    "  let {llmPrompt} = await importer.import('create llm session')\n",
    "\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  if(!startPage.includes('/submit')) {\n",
    "    startPage += '/submit?type=TEXT'\n",
    "  }\n",
    "\n",
    "  if(!driver) {\n",
    "    driver = await getClient()\n",
    "  }\n",
    "\n",
    "  await driver.get(startPage)\n",
    "\n",
    "  await loginReddit(driver)\n",
    "\n",
    "  await new Promise(resolve => setTimeout(resolve, 1500))\n",
    "\n",
    "  let titleStr = (/\\*\\*(.*?)\\*\\*\\n*/gi).exec(content)\n",
    "  if(titleStr) {\n",
    "    content = content.replace(titleStr[0], '')\n",
    "    titleStr = titleStr[1].replaceAll(/[#*\"']/gi, '')\n",
    "  } else {\n",
    "    titleStr = await llmPrompt(\n",
    "      'Generate a short title for this summary:\\n' + \n",
    "      content)\n",
    "    titleStr = titleStr.replaceAll(/[#*\"']/gi, '')\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    let title = await driver.findElement(By.css('faceplate-textarea-input[name*=\"title\"]'))\n",
    "    await title.click()\n",
    "    //await driver.executeScript('arguments[0].click();', title)\n",
    "    await driver.actions().sendKeys(titleStr).perform()\n",
    "\n",
    "    let switchButton = await getAllQuery(driver, [\n",
    "      'shreddit-composer',\n",
    "      'button[aria-label*=\"Switch to Markdown\"]'\n",
    "    ])\n",
    "    await driver.executeScript('arguments[0].click();', switchButton[0])\n",
    "    await new Promise(resolve => setTimeout(resolve, 500))\n",
    "\n",
    "    let body = await getAllQuery(driver, [\n",
    "      'shreddit-composer',\n",
    "      'shreddit-markdown-composer',\n",
    "      'textarea[placeholder*=\"Body\"]'\n",
    "    ])\n",
    "    await driver.executeScript('arguments[0].click();', body[0])\n",
    "    await driver.actions().sendKeys(content).perform()\n",
    "\n",
    "    let submit = await driver.findElement(By.css('#submit-post-button'))\n",
    "    submit.click()\n",
    "    await new Promise(resolve => setTimeout(resolve, 1500))\n",
    "\n",
    "  } catch (e) {\n",
    "    //driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = redditPost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### test reddit post?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const redditPost = importer.import('reddit post')\n",
    "\n",
    "\n",
    "async function testPost() {\n",
    "  await redditPost(void 0, 'this is a test', 'CollapseGently')\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testPost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weekly Summerizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "posts articles from a reddit list to another reddit owned by me but with a bunch of llm conversion in between.\n",
    "\n",
    "reddit weekly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "const redditList = importer.import('reddit month of links')\n",
    "const redditPost = importer.import('reddit post')\n",
    "const summerizeArticle = importer.import('summarize llm article')\n",
    "const {alternativeSummary, CONVERSION_PROMPTS} = importer.import('convert summaries')\n",
    "const { persistSummaries } = importer.import('default link collector')\n",
    "const extractArticle = importer.import('extract llm article')\n",
    "\n",
    "// TODO: send an email or post updates on reddit.com/r/collapseGently?\n",
    "\n",
    "async function redditWeekly(\n",
    "  startPage = 'CollapseSupport+climatechange+collapse+economicCollapse',\n",
    "  postPage = 'CollapseGently'\n",
    ") {\n",
    "\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  let driver = await getClient()\n",
    "\n",
    "  // TODO: get top\n",
    "  let top = await redditLinks(driver, startPage + '/top/')\n",
    "  let topLinks = top.map(post => post.link)\n",
    "\n",
    "  // TODO: sort by most comments\n",
    "  let posts = await redditList(driver, startPage)\n",
    "  let topCommented = posts.sort((a, b) => {\n",
    "    let aInt = parseInt(a.comment.replace(/comments*/gi, '').trim())\n",
    "    let bInt = parseInt(b.comment.replace(/comments*/gi, '').trim())\n",
    "    return bInt - aInt\n",
    "  }).filter(post => !topLinks.includes(post.link))\n",
    "\n",
    "  let freshPosts = topCommented.slice(0, 25).concat(top.slice(0, 25))\n",
    "  let summaries = persistSummaries()\n",
    "\n",
    "  // TODO: loop through top 20 (10 of each) and repost\n",
    "  for(let i = 0; i < freshPosts.length; i++) {\n",
    "    let summary = summaries[freshPosts[i].link]\n",
    "    if(!summary) {\n",
    "      let article = await extractArticle(driver, freshPosts[i].link)\n",
    "      summary = await summerizeArticle(article)\n",
    "    }\n",
    "\n",
    "    // TODO: extract funny summary instead\n",
    "    let rand = Math.round(Math.random() * (CONVERSION_PROMPTS.length - 1)) + 1\n",
    "    let alternative = await alternativeSummary(summary, CONVERSION_PROMPTS[rand])\n",
    "\n",
    "    await redditPost(driver, \n",
    "      'TLDR: ' + alternative[1] \n",
    "      + '\\n\\n' + alternative[0] \n",
    "      + '\\n\\n' + '[' + freshPosts[i].link + '](' + freshPosts[i].link + ')\\n', \n",
    "      postPage)\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = redditWeekly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week/Month Extractor\n",
    "\n",
    "Extract and entire week/month of links, go to the next page until the articles are older than 1 week/month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit month of links?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function redditList(driver, startPage, timeSpan = 'week') {\n",
    "  if(!startPage) {\n",
    "    startPage = 'https://www.reddit.com/r/CollapseSupport+climatechange+collapse+economicCollapse/'\n",
    "  }\n",
    "\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  if(!driver)\n",
    "    driver = await getClient()\n",
    "\n",
    "  try {\n",
    "\n",
    "    let safety = 20\n",
    "\n",
    "    let weekAgo = new Date(Date.now() - 1000 * 60 * 60 * 24 * 7)\n",
    "    let monthAgo = new Date(Date.now() - 1000 * 60 * 60 * 24 * 7 * 4.2)\n",
    "\n",
    "    let finalResult = []\n",
    "\n",
    "    let result\n",
    "    let beforeTimeSpan = 0\n",
    "    do {\n",
    "\n",
    "      console.log(startPage)\n",
    "\n",
    "      result = await redditLinks(driver, startPage)\n",
    "\n",
    "      finalResult = finalResult.concat(result)\n",
    "\n",
    "      startPage = result.next\n",
    "\n",
    "      // not yet includes any articles over a month ago\n",
    "      beforeTimeSpan = result.filter(r => timeSpan == 'month' \n",
    "        ? r.time < monthAgo : r.time < weekAgo).length\n",
    "\n",
    "      safety--\n",
    "\n",
    "      if(result.next && beforeTimeSpan == 0 && safety > 0) {\n",
    "        await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "      }\n",
    "\n",
    "    } while(result.next && beforeTimeSpan == 0 && safety > 0)\n",
    "\n",
    "    return finalResult;\n",
    "\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = redditList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scrape Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit scraper?\n",
    "\n",
    "TODO: go to reddit and download links and articles from every page in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const selectDom = importer.import('selenium select')\n",
    "const getClient = importer.import('selenium client')\n",
    "const {URL} = require('url')\n",
    "\n",
    "async function redditLinks(driver, startPage) {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  let startUrl = new URL(startPage)\n",
    "\n",
    "  if(!driver) {\n",
    "    driver = await getClient()\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    await driver.get(startPage)\n",
    "\n",
    "    await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "\n",
    "    let links = await selectDom(driver, [\n",
    "      '//div[contains(@role, \"main\")]//div[contains(@class, \"link\") and not(contains(@class, \"linklisting\")) and not(contains(@class, \"promoted\"))]'])\n",
    "\n",
    "    // TODO: get some special links, get comment count, titles, finally next page\n",
    "    let results = []\n",
    "    for(let i = 0; i < links.length; i++) {\n",
    "      let result = await selectDom(driver, {\n",
    "        title: './/a[contains(@class, \"title\")]/text()',\n",
    "        link: './/a[contains(@class, \"title\")]/@href',\n",
    "        time: './/time/@datetime',\n",
    "        comment: './/a[contains(@class, \"comments\")]/text()',\n",
    "      }, links[i])\n",
    "      results.push(result)\n",
    "    }\n",
    "\n",
    "    let next = await selectDom(driver, '//a[contains(@rel, \"next\")]/@href')\n",
    "\n",
    "    let objectArray = results.map(r => ({\n",
    "      title: r.title,\n",
    "      time: new Date(r.time),\n",
    "      link: r.link.includes('://') \n",
    "        ? r.link : ((!r.link.startsWith('/') \n",
    "        ? (startUrl.origin + '/' + startUrl.pathname + './') : startUrl.origin) + r.link),\n",
    "      comment: r.comment\n",
    "    }))\n",
    "    objectArray.next = next\n",
    "    return objectArray\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = redditLinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scraper\n",
    "\n",
    "test reddit scraper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function testScraper(startPage = 'https://www.reddit.com/r/CollapseSupport+climatechange+collapse+economicCollapse/') {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  let result = await redditLinks(driver, startPage)\n",
    "\n",
    "  driver.quit()\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testScraper\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
