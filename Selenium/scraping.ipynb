{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "# Scraping Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reddit\n",
    "\n",
    "Because whether i like it or not it is an information authority.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scrape Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit scraper?\n",
    "\n",
    "TODO: go to reddit and download links and articles from every page in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const selectDom = importer.import('selenium select')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function redditLinks(driver, startPage) {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  if(!driver) {\n",
    "    driver = await getClient()\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    await driver.get(startPage)\n",
    "\n",
    "    await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "\n",
    "    let links = await selectDom(driver, [\n",
    "      '//div[contains(@role, \"main\")]//div[contains(@class, \"link\") and not(contains(@class, \"linklisting\")) and not(contains(@class, \"promoted\"))]'])\n",
    "\n",
    "    // TODO: get some special links, get comment count, titles, finally next page\n",
    "    let results = []\n",
    "    for(let i = 0; i < links.length; i++) {\n",
    "      let result = await selectDom(driver, {\n",
    "        title: './/a[contains(@class, \"title\")]/text()',\n",
    "        link: './/a[contains(@class, \"title\")]/@href',\n",
    "        time: './/time/@datetime',\n",
    "        comment: './/a[contains(@class, \"comments\")]/text()',\n",
    "      }, links[i])\n",
    "      results.push(result)\n",
    "    }\n",
    "\n",
    "    let next = await selectDom(driver, '//a[contains(@rel, \"next\")]/@href')\n",
    "\n",
    "    let objectArray = results.map(r => ({\n",
    "      title: r.title,\n",
    "      time: new Date(r.time),\n",
    "      link: r.link.includes('://') ? r.link : (startPage + (!r.link.startsWith('/') ? '/' : '') + r.link),\n",
    "      comment: r.comment\n",
    "    }))\n",
    "    objectArray.next = next\n",
    "    return objectArray\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = redditLinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scraper\n",
    "\n",
    "test reddit scraper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function testScraper(startPage = 'https://www.reddit.com/r/CollapseSupport+climatechange+collapse+economicCollapse/') {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  let result = await redditLinks(driver, startPage)\n",
    "\n",
    "  driver.quit()\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testScraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week/Month Extractor\n",
    "\n",
    "Extract and entire week/month of links, go to the next page until the articles are older than 1 week/month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit month of links?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function redditList(driver, startPage, timeSpan = 'week') {\n",
    "  if(!startPage) {\n",
    "    startPage = 'https://www.reddit.com/r/CollapseSupport+climatechange+collapse+economicCollapse/'\n",
    "  }\n",
    "\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  if(!driver)\n",
    "    driver = await getClient()\n",
    "\n",
    "  try {\n",
    "\n",
    "    let safety = 20\n",
    "\n",
    "    let weekAgo = new Date(Date.now() - 1000 * 60 * 60 * 24 * 7)\n",
    "    let monthAgo = new Date(Date.now() - 1000 * 60 * 60 * 24 * 7 * 4.2)\n",
    "\n",
    "    let finalResult = []\n",
    "\n",
    "    let result\n",
    "    let beforeTimeSpan = 0\n",
    "    do {\n",
    "\n",
    "      console.log(startPage)\n",
    "\n",
    "      result = await redditLinks(driver, startPage)\n",
    "\n",
    "      finalResult = finalResult.concat(result)\n",
    "\n",
    "      startPage = result.next\n",
    "\n",
    "      // not yet includes any articles over a month ago\n",
    "      beforeTimeSpan = result.filter(r => timeSpan == 'month' \n",
    "        ? r.time < monthAgo : r.time < weekAgo).length\n",
    "\n",
    "      safety--\n",
    "\n",
    "      if(result.next && beforeTimeSpan == 0 && safety > 0) {\n",
    "        await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "      }\n",
    "\n",
    "    } while(result.next && beforeTimeSpan == 0 && safety > 0)\n",
    "\n",
    "    return finalResult;\n",
    "\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = redditList\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weekly Summarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "reddit weekly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// TODO: connect reddit article extractor with weekly summarizer below.\n",
    "\n",
    "\n",
    "// TODO: email the results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLM Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Article Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "extract llm article?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const getClient = importer.import('selenium client')\n",
    "const llama = importer.import('llm code')\n",
    "const selectDom = importer.import('selenium select')\n",
    "\n",
    "// TODO: help me extract this article from html, only return the article in plain text and remove the html:\n",
    "\n",
    "\n",
    "async function extractArticle(driver, startPage) {\n",
    "  let llmCode = await llama\n",
    "\n",
    "  if(!driver)\n",
    "    driver = await getClient()\n",
    "\n",
    "  if(!startPage) {\n",
    "    return\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    await driver.get(startPage)\n",
    "\n",
    "    let bodyElements = await selectDom(driver, ['//*[text() and not(self::script|self::style|self::form) and not(ancestor::aside|ancestor::nav|ancestor::form)]', './text()'])\n",
    "    return bodyElements\n",
    "      .map(t => Array.isArray(t) ? t.join('\\n').trim() : t.trim())\n",
    "      .filter(t => t.length)\n",
    "      .join('\\n')\n",
    "/*\n",
    "\n",
    "    let q1 = 'Extract the text content from this HTML page without forming any new HTML code:\\n'\n",
    "      + bodyElements.join('\\n') + '\\nReturn plain text extracted from the page and nothing else.'\n",
    "\n",
    "    console.log('User: ' + q1)\n",
    "    let a1 = await llmCode(q1)\n",
    "    console.log('AI: ' + a1)\n",
    "\n",
    "    let code = a1.matchAll(/```(markdown)*\\n[\\s\\S]*?\\n```/gi)\n",
    "\n",
    "    // extract code blocks from response\n",
    "    let codeBlocks = ''\n",
    "    for(let match of code) {\n",
    "      codeBlocks += match[0].replace(/^```(markdown)*\\n|\\n```$/gi, '') + '\\n'\n",
    "    }\n",
    "    if(!codeBlocks) {\n",
    "      console.log('Error, couldn\\'t find code in:' + a1)\n",
    "      return\n",
    "    }\n",
    "\n",
    "    return codeBlocks.join('\\n')\n",
    "*/\n",
    "\n",
    "    } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = extractArticle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "test article extract?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "const extractArticle = importer.import('extract llm article')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function testExtractor(startPage) {\n",
    "  if(!startPage) {\n",
    "    startPage = 'https://tsakraklides.com/2025/02/05/in-the-age-of-infinite-consumer-choice-the-only-choice-is-collapse/'\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  let result = await extractArticle(driver, startPage)\n",
    "\n",
    "  driver.quit()\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testExtractor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "summarize llm article?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const llama = importer.import('create llm session')\n",
    "\n",
    "// TODO: prompt llm, in one sentence summarize this article. in a whole paragraph summarize this article.\n",
    "\n",
    "async function summerizeArticle(article) {\n",
    "  let {llmPrompt} = await llama\n",
    "\n",
    "  let q1 = 'Summerize this article in a short paragraph:\\n' + article + '\\nOnly return the summary and nothing else, no explanations.'\n",
    "  console.log('User: ' + q1)\n",
    "  let a1 = await llmPrompt(q1)\n",
    "  console.log('AI: ' + a1)\n",
    "\n",
    "\n",
    "  let q2 = 'Summerize this article in a single sentence:\\n' + article + '\\nOnly return the summary and nothing else, no explanations.'\n",
    "  console.log('User: ' + q2)\n",
    "  let a2 = await llmPrompt(q2)\n",
    "  console.log('AI: ' + a2)\n",
    "\n",
    "  return [a1, a2]\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = summerizeArticle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "test article summarizer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const extractArticle = importer.import('extract llm article')\n",
    "const getClient = importer.import('selenium client')\n",
    "const summerizeArticle = importer.import('summarize llm article')\n",
    "\n",
    "async function testExtractor(startPage) {\n",
    "  if(!startPage) {\n",
    "    startPage = 'https://tsakraklides.com/2025/02/05/in-the-age-of-infinite-consumer-choice-the-only-choice-is-collapse/'\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  let result = await extractArticle(driver, startPage)\n",
    "\n",
    "  driver.quit()\n",
    "\n",
    "  let summary = await summerizeArticle(result)\n",
    "\n",
    "  return summary\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summarize All\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "summarize all articles?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const extractArticle = importer.import('extract llm article')\n",
    "const summerizeArticle = importer.import('summarize llm article')\n",
    "\n",
    "async function defaultCollector(startPage, selector = '//a[@href]/@href') {\n",
    "  const selectDom = importer.import('selenium select')\n",
    "  const getClient = importer.import('selenium client')\n",
    "  const driver = getClient()\n",
    "\n",
    "  try {\n",
    "    await driver.get(startPage)\n",
    "    await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "    let links = await selectDom(driver, selector)\n",
    "    return links.map(l => ({link: l})) // to match reddit post lister\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "async function summerizeAll(links, selector, startPage) {\n",
    "\n",
    "  if(!startPage && !links) {\n",
    "    console.error('No start page or links to summerize.')\n",
    "    return\n",
    "  }\n",
    "\n",
    "  if(!selector && startPage.includes('reddit.com')) {\n",
    "    selector = importer.import('reddit scraper')\n",
    "  } else if(!selector) {\n",
    "    selector = defaultCollector\n",
    "  } else if(typeof selector == 'string') {\n",
    "    selector = defaultCollector.bind(null, startPage, selector)\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  try {\n",
    "    if(!links && startPage) {\n",
    "      links = await selector(startPage)\n",
    "    }\n",
    "    let summaries = []\n",
    "    for (let i = 0; i < links.length; i++) {\n",
    "      let article = await extractArticle(driver, links[i])\n",
    "\n",
    "      let summary = await summerizeArticle(article)\n",
    "      \n",
    "      summaries.push(summary)\n",
    "    }\n",
    "\n",
    "    return summaries\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = summerizeAll\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
