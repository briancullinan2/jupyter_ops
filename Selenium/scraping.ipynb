{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "# Scraping Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Reddit\n",
    "\n",
    "Because whether i like it or not it is an information authority.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scrape Links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit scraper?\n",
    "\n",
    "TODO: go to reddit and download links and articles from every page in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const selectDom = importer.import('selenium select')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function redditLinks(driver, startPage) {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  if(!driver) {\n",
    "    driver = await getClient()\n",
    "  }\n",
    "\n",
    "  try {\n",
    "    await driver.get(startPage)\n",
    "\n",
    "    await new Promise(resolve => setTimeout(resolve, 1000))\n",
    "\n",
    "    // TODO: get some special links, get comment count, titles, finally next page\n",
    "    let results = await selectDom(driver, {\n",
    "      titles: ['//div[contains(@role, \"main\")]//a[contains(@class, \"title\") and not(ancestor::div[contains(@class, \"promoted\")])]', './text()'],\n",
    "      links: ['//div[contains(@role, \"main\")]//a[contains(@class, \"title\") and not(ancestor::div[contains(@class, \"promoted\")])]', './@href'],\n",
    "      comments: ['//div[contains(@role, \"main\")]//a[contains(@class, \"comments\") and not(ancestor::div[contains(@class, \"promoted\")])]', './text()'],\n",
    "      next: ['//a[contains(@rel, \"next\")]', './@href']\n",
    "    })\n",
    "\n",
    "    let objectArray = results.titles.map((t, i) => ({\n",
    "      title: t,\n",
    "      link: results.links[i],\n",
    "      comments: results.comments[i]\n",
    "    }))\n",
    "    objectArray.next = results.next[0]\n",
    "    return objectArray\n",
    "  } catch (e) {\n",
    "    driver.quit()\n",
    "\n",
    "    throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = redditLinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scraper\n",
    "\n",
    "test reddit scraper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const redditLinks = importer.import('reddit scraper')\n",
    "const getClient = importer.import('selenium client')\n",
    "\n",
    "async function testScraper(startPage = 'https://www.reddit.com/r/CollapseSupport+climatechange+collapse+economicCollapse/') {\n",
    "  if(!startPage.includes('://')) {\n",
    "    startPage = 'https://www.reddit.com/r/' + startPage\n",
    "  }\n",
    "\n",
    "  driver = await getClient()\n",
    "\n",
    "  let result = await redditLinks(driver, startPage)\n",
    "\n",
    "  //driver.quit()\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = testScraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week/Month Extractor\n",
    "\n",
    "Extract and entire week/month of links, go to the next page until the articles are older than 1 week/month.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit month of links?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test month extractor\n",
    "\n",
    "test reddit month extractor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Type of Article\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "reddit article type?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Weekly Summarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code\n",
    "\n",
    "reddit weekly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// TODO: connect reddit article extractor with weekly summarizer below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLM Tools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Article Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "llm article?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "// TODO: help me extract this article from html, only return the article in plain text and remove the html:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "test article extract?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "// TODO: prompt llm, in one sentence summarize this article. in a whole paragraph summarize this article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "test article summarizer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summarize All\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### the code\n",
    "\n",
    "summarize all articles?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
