{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "get python params?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# import inspect\n",
    "\n",
    "\n",
    "def get_parameter_names(source):\n",
    "    \"\"\"\n",
    "    Extracts parameter names from a function using the ast module.\n",
    "\n",
    "    Args:\n",
    "        func: The function to inspect.\n",
    "\n",
    "    Returns:\n",
    "        A list of parameter names as strings.\n",
    "    \"\"\"\n",
    "    #source = inspect.getsource(func)\n",
    "    tree = ast.parse(source)\n",
    "    function_def = next((node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)), None)\n",
    "\n",
    "    if function_def:\n",
    "        return [arg.arg for arg in function_def.args.args]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "__all__ = {\n",
    "  \"get_parameter_names\": get_parameter_names,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### import notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import ctypes\n",
    "import os\n",
    "import re\n",
    "\n",
    "def safeurl(name):\n",
    "    return re.sub(r'\\W+', '_', name)\n",
    "\n",
    "modules = {}\n",
    "\n",
    "def import_notebook(query_str, ctx):\n",
    "    from Core import search_whoosh, build_dylib\n",
    "    \"\"\"Searches the Whoosh index and returns a module of the retrieved code.\"\"\"\n",
    "    results = search_whoosh(query_str)\n",
    "    \n",
    "    if not results:\n",
    "        raise ImportError(f\"No matching notebook found for query: {query_str}\")\n",
    "    \n",
    "    if(results[0][\"filename\"] in modules):\n",
    "        return vars(modules[results[0][\"filename\"]])[\"__all__\"]\n",
    "\n",
    "\n",
    "    module = types.ModuleType(results[0][\"filename\"])\n",
    "    module.__file__ = results[0][\"filename\"]\n",
    "    module.import_notebook = import_notebook\n",
    "\n",
    "    if(results[0][\"language\"] == \"c\" or results[0][\"language\"] == \"cpp\" or results[0][\"language\"] == \"objective-c\"):\n",
    "        build_dylib(results[0][\"code\"], results[0][\"filename\"], {})\n",
    "        lib_name = safeurl(results[0][\"questions\"][0])\n",
    "        BUILD_DIRECTORY = os.path.join(os.path.dirname(__file__), \"../.build\")\n",
    "        result = ctypes.CDLL(os.path.join(BUILD_DIRECTORY, lib_name + '.dylib'))\n",
    "        import_notebook(\"list c functions with python\", ctx)\n",
    "        functions = get_functions(results[0][\"code\"])\n",
    "        module.__all__ = {}\n",
    "        for f in functions:\n",
    "            func_name = f.split('(')[0]\n",
    "            vars(module)['__all__'][func_name] = getattr(result, func_name)\n",
    "        \n",
    "    elif(results[0][\"language\"] == \"python\"):\n",
    "        exec(results[0][\"code\"], module.__dict__)  # Execute the first matching cell in the module namespace\n",
    "    \n",
    "    for name in vars(module)['__all__']:\n",
    "        # setattr(vars(module)['__all__'], name, vars(module)['__all__'][name])\n",
    "        ctx[name] = vars(module)['__all__'][name]\n",
    "        module.__dict__[name] = vars(module)['__all__'][name]\n",
    "        globals()[name] = vars(module)['__all__'][name]\n",
    "\n",
    "    modules[results[0][\"filename\"]] = module\n",
    "\n",
    "    return vars(module)['__all__']\n",
    "\n",
    "__all__ = {\n",
    "  \"import_notebook\": import_notebook,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### build_dylib()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "BUILD_DIRECTORY = os.path.join(os.path.dirname(__file__), \"../.build\")\n",
    "os.makedirs(BUILD_DIRECTORY, exist_ok=True)\n",
    "\n",
    "def safeurl(name):\n",
    "    return re.sub(r'\\W+', '_', name)\n",
    "\n",
    "def build_dylib(code, path_to_code, ctx):\n",
    "    from Core import search_whoosh\n",
    "    code_cell = search_whoosh(path_to_code)[0]\n",
    "    lib_name = safeurl(code_cell['questions'][0])\n",
    "    lib_ext = \".cpp\" if code_cell['language'] == \"cpp\" else \".c\"\n",
    "    lib_path = os.path.join(BUILD_DIRECTORY, f\"{lib_name}{lib_ext}\")\n",
    "    \n",
    "    if not os.path.exists(lib_path) or os.path.getmtime(code_cell['filename']) > os.path.getmtime(lib_path):\n",
    "        with open(lib_path, \"w\") as f:\n",
    "            f.write(\"\".join(code_cell['code']))\n",
    "    \n",
    "    env = {}\n",
    "    \n",
    "    for match in re.finditer(r'([A-Z_])\\s*[:=-]+\\s*(.*?)\\s*(\\n|$)', \"\".join(code_cell['markdown'])):\n",
    "        env[match[1]] = match[2]\n",
    "    \n",
    "    if code_cell['language'] == \"cpp\":\n",
    "        env[\"CXX\"] = os.getenv(\"CXX\", \"clang++\").split()\n",
    "        env[\"STD\"] = os.getenv(\"STD\", \"-std=c++17 -stdlib=libc++\").split()\n",
    "    else:\n",
    "        env[\"CXX\"] = os.getenv(\"CXX\", \"clang\").split()\n",
    "    \n",
    "    obj_path = os.path.join(BUILD_DIRECTORY, f\"{lib_name}.o\")\n",
    "    if not os.path.exists(obj_path) or os.path.getmtime(lib_path) > os.path.getmtime(obj_path):\n",
    "        mods = [\"-x\", \"objective-c\", \"-fno-objc-arc\"] if code_cell['language'] == \"objective-c\" else []\n",
    "        if \"@import\" in \"\".join(code_cell['code']):\n",
    "            mods = [\"-fmodules\"] + mods\n",
    "        \n",
    "        cflags = []\n",
    "        if \"PKG_CONFIG\" in env:\n",
    "            result = subprocess.run([\"pkg-config\", \"--cflags\"] + env[\"PKG_CONFIG\"].split(), capture_output=True, text=True)\n",
    "            cflags = result.stdout.split()\n",
    "        \n",
    "        args = [\"-c\", lib_path, \"-o\", obj_path]\n",
    "        subprocess.run(env[\"CXX\"] + mods + cflags + args, check=True, env=os.environ)\n",
    "    \n",
    "    mod_path = os.path.join(BUILD_DIRECTORY, f\"{lib_name}.dylib\")\n",
    "    if not os.path.exists(mod_path) or os.path.getmtime(obj_path) > os.path.getmtime(mod_path):\n",
    "        libs = []\n",
    "        if \"PKG_CONFIG\" in env:\n",
    "            result = subprocess.run([\"pkg-config\", \"--libs\"] + env[\"PKG_CONFIG\"].split(), capture_output=True, text=True)\n",
    "            libs = result.stdout.split()\n",
    "        \n",
    "        mods = [\"-dynamiclib\", \"-rdynamic\"] if \"clang\" in env[\"CXX\"][0] else []\n",
    "        args = [\"-o\", mod_path]\n",
    "        subprocess.run(env[\"CXX\"] + [obj_path] + mods + libs + args, check=True, env=os.environ)\n",
    "\n",
    "__all__ = {\n",
    "    \"build_dylib\": build_dylib\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### accumulate markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "def get_questions(source, markdown):\n",
    "    \"\"\"Extracts questions from markdown and source code.\"\"\"\n",
    "    match_questions = re.compile(r'^.*\\?.*$', re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    questions = [re.sub(r'how to|\\?|#+', '', q, flags=re.IGNORECASE).strip()\n",
    "                 for q in match_questions.findall(markdown)]\n",
    "    \n",
    "    questions += [re.sub(r'how to|\\?|#+', '', q, flags=re.IGNORECASE).strip()\n",
    "                  for q in match_questions.findall(source) if 'how to' in q.lower()]\n",
    "    \n",
    "    questions.sort(key=len)\n",
    "    return questions + [questions[0]] if questions else ['']\n",
    "\n",
    "def accumulate_markdown(cells):\n",
    "    \"\"\"Accumulates markdown leading up to code cells.\"\"\"\n",
    "    codes = [c for c in cells if c[\"cell_type\"] == \"code\"]\n",
    "    result = []\n",
    "    \n",
    "    for i, code_cell in enumerate(codes):\n",
    "        from_idx = cells.index(codes[i-1]) + 1 if i > 0 else 0\n",
    "        to_idx = cells.index(code_cell)\n",
    "        markdown = \"\\n\".join(\"\".join(m[\"source\"]) for m in cells[from_idx:to_idx])\n",
    "        code = \"\".join(code_cell[\"source\"])\n",
    "        result.append({\n",
    "            \"id\": code_cell[\"id\"], \n",
    "            \"language\": code_cell[\"language\"], \n",
    "            \"from\": from_idx, \n",
    "            \"to\": to_idx, \n",
    "            \"markdown\": markdown, \n",
    "            \"code\": code\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "def cache_cells(filename):\n",
    "    from Core import get_cells\n",
    "    \"\"\"Parses a Jupyter notebook, extracts relevant cells, and generates cache entries.\"\"\"\n",
    "    filename = os.path.abspath(filename)\n",
    "    mtime = os.path.getmtime(filename)\n",
    "    \n",
    "    cells = get_cells(filename)\n",
    "    new_cache = accumulate_markdown(cells)\n",
    "    \n",
    "    return [{\n",
    "        \"id\": f\"{os.path.basename(filename)}[{i}]\",\n",
    "        \"filename\": filename,\n",
    "        \"mtime\": datetime.datetime.fromtimestamp(mtime),\n",
    "        \"questions\": get_questions(c[\"code\"], c[\"markdown\"]),\n",
    "        \"notebook\": os.path.basename(filename),\n",
    "        **c\n",
    "    } for i, c in enumerate(new_cache)]\n",
    "\n",
    "\n",
    "__all__ = {\n",
    "  \"cache_cells\": cache_cells,\n",
    "  \"accumulate_markdown\": accumulate_markdown,\n",
    "  \"get_questions\": get_questions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### initialize database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID, DATETIME, KEYWORD\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "from whoosh.writing import AsyncWriter\n",
    "\n",
    "# Define schema for Whoosh index\n",
    "schema = Schema(markdown=TEXT(stored=True), language=TEXT(stored=True), mtime=DATETIME(stored=True), id=ID(stored=True), questions=TEXT(stored=True), filename=TEXT(stored=True), code=TEXT(stored=True))\n",
    "\n",
    "# Ensure index directory exists\n",
    "if not os.path.exists(\".cache\"):\n",
    "    os.mkdir(\".cache\")\n",
    "    index = create_in(\".cache\", schema)\n",
    "else:\n",
    "    from whoosh.index import open_dir\n",
    "    index = open_dir(\".cache\")\n",
    "\n",
    "def store_in_whoosh(cells):\n",
    "    \"\"\"Stores extracted cells in Whoosh index.\"\"\"\n",
    "    writer = AsyncWriter(index)\n",
    "    for cell in cells:\n",
    "        if 'code' in cell: # and cell[\"language\"] == \"python\":\n",
    "            # print(cell[\"questions\"], cell[\"filename\"])\n",
    "            writer.add_document(markdown=cell[\"markdown\"], language=cell[\"language\"], mtime=cell[\"mtime\"], id=cell[\"id\"], questions=\"\\n\".join(cell[\"questions\"]), filename=cell[\"filename\"], code=cell[\"code\"])\n",
    "            # writer.update_document(id=cell[\"id\"], questions=\" \".join(cell[\"questions\"]), filename=cell[\"filename\"], code=cell[\"code\"])\n",
    "    writer.commit()\n",
    "\n",
    "globals()['first'] = True\n",
    "def search_whoosh(question):\n",
    "#  with index.searcher() as searcher:\n",
    "#    query = QueryParser(\"questions\", index.schema).parse(question)  # Fuzzy search\n",
    "#    results = searcher.search(query)\n",
    "#    return results\n",
    "    if globals()['first']:\n",
    "        scan_directory(os.path.join(os.path.dirname(__file__), '..'), 3)\n",
    "        globals()['first'] = False\n",
    "\n",
    "    with index.searcher() as searcher:\n",
    "        parser = MultifieldParser([\"filename\", \"questions\"], schema=schema)\n",
    "        query = parser.parse(question)\n",
    "        #results = searcher.search(query, limit=10)  # Adjust limit as needed\n",
    "        results = searcher.search(query)  # Adjust limit as needed\n",
    "        return [{\"filename\": r[\"filename\"], \"code\": r[\"code\"], \"language\": r[\"language\"], \"markdown\": r[\"markdown\"], \"questions\": r[\"questions\"].split('\\n')} for r in results]\n",
    "\n",
    "def scan_directory(directory, limit):\n",
    "    from Core import cache_cells\n",
    "    \"\"\"Recursively scans a directory for notebooks and stores extracted cells in Whoosh index.\"\"\"\n",
    "    all_cells = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "\n",
    "            if os.path.basename(root).startswith(\".\"):\n",
    "                continue\n",
    "\n",
    "            if file.endswith(\".ipynb\"):\n",
    "\n",
    "                # TODO: compare date time and delete notebooks only if outdated\n",
    "                #parser = QueryParser(\"date\", schema=ix.schema)\n",
    "                #query = DateRange(\"date\", datetime(1, 1, 1), datetime())\n",
    "                #results = searcher.search(query)\n",
    "\n",
    "                notebook_path = os.path.join(root, file)\n",
    "                parser = QueryParser(\"filename\", schema=schema)\n",
    "                query = parser.parse(os.path.abspath(notebook_path))\n",
    "                results = index.searcher().search(query)\n",
    "                if(not results or len(results) == 0\n",
    "                    or results[0]['mtime'] < datetime.datetime.fromtimestamp(os.path.getmtime(os.path.abspath(notebook_path)))):\n",
    "                    print(\"replacing: \", notebook_path)\n",
    "                    writer = AsyncWriter(index)\n",
    "                    writer.delete_by_query(query, index.searcher())\n",
    "                    writer.commit()\n",
    "                    all_cells.extend(cache_cells(notebook_path))\n",
    "            \n",
    "\n",
    "    store_in_whoosh(all_cells)\n",
    "    print(f\"Stored {len(all_cells)} cells in Whoosh index.\")\n",
    "\n",
    "\n",
    "__all__ = {\n",
    "  \"scan_directory\": scan_directory,\n",
    "  \"search_whoosh\": search_whoosh,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### get_cells(notebook_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def get_cells(notebook_path, types=['*', 'code']):\n",
    "    \"\"\"Extract cells from a Jupyter Notebook with additional metadata.\"\"\"\n",
    "    notebook_path = os.path.abspath(notebook_path)\n",
    "\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    kernel = notebook.get('metadata', {}).get('kernelspec', {})\n",
    "    \n",
    "    cells = [\n",
    "        {\n",
    "            **cell,\n",
    "            \"language\": (cell.get(\"metadata\", {}).get(\"vscode\", {}).get(\"languageId\") or\n",
    "                         kernel.get(\"language\") or\n",
    "                         notebook.get(\"metadata\", {}).get(\"language_info\", {}).get(\"name\", \"\")),\n",
    "            \"filename\": notebook_path,\n",
    "            \"id\": f\"{os.path.basename(notebook_path)}[{i}]\"\n",
    "        }\n",
    "        for i, cell in enumerate(notebook.get(\"cells\", []))\n",
    "        if '*' in types or cell.get(\"cell_type\") in types\n",
    "    ]\n",
    "\n",
    "    return cells\n",
    "\n",
    "__all__ = {\n",
    "  \"get_cells\": get_cells\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### run()\n",
    "\n",
    "run python cells?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "\n",
    "def run():\n",
    "    from Core import search_whoosh, import_notebook, get_parameter_names\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python script.py <notebook_path> <function_args>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    notebook_path = sys.argv[1]\n",
    "    inputs = sys.argv[2:]\n",
    "\n",
    "    # Import the notebook as a module\n",
    "    results = search_whoosh(notebook_path)\n",
    "    #print(results[0]['code'])\n",
    "\n",
    "    module = import_notebook(notebook_path, globals())\n",
    "    print(module)\n",
    "\n",
    "    # Find the first function in the module\n",
    "    func = None\n",
    "    for name in list(module.keys()):\n",
    "        # TODO: support classes ? attr = getattr(module, name)\n",
    "        attr = module[name]\n",
    "        if callable(attr):\n",
    "            func = attr\n",
    "            break\n",
    "\n",
    "    if not func:\n",
    "        print(\"No function found in the notebook.\")\n",
    "        sys.exit(1)\n",
    "    print(func)\n",
    "\n",
    "    # Extract parameters and map inputs\n",
    "    params = []\n",
    "    if(results[0][\"language\"] == 'python'):\n",
    "        params = get_parameter_names(results[0]['code'])\n",
    "        print(params)\n",
    "    mapped_inputs = []\n",
    "\n",
    "    for param in params:\n",
    "        for i, arg in enumerate(inputs):\n",
    "            if arg.startswith(f\"--{param}=\"):\n",
    "                mapped_inputs.append(arg.split(\"=\")[1])\n",
    "                break\n",
    "        else:\n",
    "            mapped_inputs.append(inputs.pop(0) if inputs else None)\n",
    "\n",
    "    # Convert types based on function annotations (if available)\n",
    "    if hasattr(func, '__annotations__'):\n",
    "        func_annotations = func.__annotations__\n",
    "        print(func_annotations)\n",
    "        for i, param in enumerate(params):\n",
    "            if param in func_annotations:\n",
    "                mapped_inputs[i] = func_annotations[param](mapped_inputs[i])\n",
    "\n",
    "    # Execute the function\n",
    "    result = func(*mapped_inputs)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__run__\":\n",
    "    run()\n",
    "\n",
    "__all__ = {\n",
    "  \"run\": run\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
