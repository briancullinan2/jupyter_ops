{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### import notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_notebook(query_str):\n",
    "    from Core import search_whoosh\n",
    "    \"\"\"Searches the Whoosh index and returns a module of the retrieved code.\"\"\"\n",
    "    results = search_whoosh(query_str)\n",
    "    \n",
    "    if not results:\n",
    "        raise ImportError(f\"No matching notebook found for query: {query_str}\")\n",
    "    \n",
    "    module = types.ModuleType(\"imported_notebook\")\n",
    "    exec(results[0][\"code\"], module.__dict__)  # Execute the first matching cell in the module namespace\n",
    "    \n",
    "    return module\n",
    "\n",
    "__all__ = {\n",
    "  \"import_notebook\": import_notebook,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### accumulate markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_questions(source, markdown):\n",
    "    \"\"\"Extracts questions from markdown and source code.\"\"\"\n",
    "    match_questions = re.compile(r'^.*\\?.*$', re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    questions = [re.sub(r'how to|\\?|#+', '', q, flags=re.IGNORECASE).strip()\n",
    "                 for q in match_questions.findall(markdown)]\n",
    "    \n",
    "    questions += [re.sub(r'how to|\\?|#+', '', q, flags=re.IGNORECASE).strip()\n",
    "                  for q in match_questions.findall(source) if 'how to' in q.lower()]\n",
    "    \n",
    "    questions.sort(key=len)\n",
    "    return questions + [questions[0]] if questions else ['']\n",
    "\n",
    "def accumulate_markdown(cells):\n",
    "    \"\"\"Accumulates markdown leading up to code cells.\"\"\"\n",
    "    codes = [c for c in cells if c[\"cell_type\"] == \"code\"]\n",
    "    result = []\n",
    "    \n",
    "    for i, code_cell in enumerate(codes):\n",
    "        from_idx = cells.index(codes[i-1]) + 1 if i > 0 else 0\n",
    "        to_idx = cells.index(code_cell)\n",
    "        markdown = \"\\n\".join(\"\".join(m[\"source\"]) for m in cells[from_idx:to_idx])\n",
    "        code = \"\".join(code_cell[\"source\"])\n",
    "        result.append({\"from\": from_idx, \"to\": to_idx, \"markdown\": markdown, \"code\": code})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def cache_cells(filename):\n",
    "    from Core import get_cells\n",
    "    \"\"\"Parses a Jupyter notebook, extracts relevant cells, and generates cache entries.\"\"\"\n",
    "    filename = os.path.abspath(filename)\n",
    "    mtime = os.path.getmtime(filename)\n",
    "    \n",
    "    cells = get_cells(filename)\n",
    "    new_cache = accumulate_markdown(cells)\n",
    "    \n",
    "    return [{\n",
    "        \"id\": f\"{os.path.basename(filename)}[{i}]\",\n",
    "        \"filename\": filename,\n",
    "        \"mtime\": mtime,\n",
    "        \"questions\": get_questions(c[\"code\"], c[\"markdown\"]),\n",
    "        \"notebook\": os.path.basename(filename),\n",
    "        **c\n",
    "    } for i, c in enumerate(new_cache)]\n",
    "\n",
    "\n",
    "__all__ = {\n",
    "  \"cache_cells\": cache_cells,\n",
    "  \"accumulate_markdown\": accumulate_markdown,\n",
    "  \"get_questions\": get_questions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### initialize database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.qparser import QueryParser, MultifieldParser\n",
    "\n",
    "# Define schema for Whoosh index\n",
    "schema = Schema(questions=TEXT(stored=True), filename=ID(stored=True), code=TEXT(stored=True))\n",
    "\n",
    "# Ensure index directory exists\n",
    "if not os.path.exists(\"indexdir\"):\n",
    "    os.mkdir(\"indexdir\")\n",
    "    index = create_in(\"indexdir\", schema)\n",
    "else:\n",
    "    from whoosh.index import open_dir\n",
    "    index = open_dir(\"indexdir\")\n",
    "\n",
    "def store_in_whoosh(cells):\n",
    "    \"\"\"Stores extracted cells in Whoosh index.\"\"\"\n",
    "    writer = index.writer()\n",
    "    for cell in cells:\n",
    "        if 'code' in cell:\n",
    "            # print(cell[\"questions\"], cell[\"code\"])\n",
    "            writer.add_document(questions=cell[\"questions\"], filename=cell[\"filename\"], code=cell[\"code\"])\n",
    "    writer.commit()\n",
    "\n",
    "def search_whoosh(question):\n",
    "#  with index.searcher() as searcher:\n",
    "#    query = QueryParser(\"questions\", index.schema).parse(question)  # Fuzzy search\n",
    "#    results = searcher.search(query)\n",
    "#    return results\n",
    "    scan_directory(os.path.join(os.path.dirname(__file__), '../Core'))\n",
    "    with index.searcher() as searcher:\n",
    "        parser = MultifieldParser([\"filename\", \"questions\"], schema=index.schema)\n",
    "        query = parser.parse(question)\n",
    "        results = searcher.search(query, limit=10)  # Adjust limit as needed\n",
    "        return [{\"filename\": r[\"filename\"], \"code\": r[\"code\"]} for r in results]\n",
    "\n",
    "def scan_directory(directory):\n",
    "    from Core import cache_cells\n",
    "    \"\"\"Recursively scans a directory for notebooks and stores extracted cells in Whoosh index.\"\"\"\n",
    "    all_cells = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "            if file.endswith(\".ipynb\"):\n",
    "                notebook_path = os.path.join(root, file)\n",
    "                all_cells.extend(cache_cells(notebook_path))\n",
    "\n",
    "    store_in_whoosh(all_cells)\n",
    "    print(f\"Stored {len(all_cells)} cells in Whoosh index.\")\n",
    "\n",
    "\n",
    "__all__ = {\n",
    "  \"scan_directory\": scan_directory,\n",
    "  \"search_whoosh\": search_whoosh,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### get_cells(notebook_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def get_cells(notebook_path, types=['*', 'code']):\n",
    "    \"\"\"Extract cells from a Jupyter Notebook with additional metadata.\"\"\"\n",
    "    notebook_path = os.path.abspath(notebook_path)\n",
    "\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    kernel = notebook.get('metadata', {}).get('kernelspec', {})\n",
    "    \n",
    "    cells = [\n",
    "        {\n",
    "            **cell,\n",
    "            \"language\": (cell.get(\"metadata\", {}).get(\"vscode\", {}).get(\"languageId\") or\n",
    "                         kernel.get(\"language\") or\n",
    "                         notebook.get(\"metadata\", {}).get(\"language_info\", {}).get(\"name\", \"\")),\n",
    "            \"filename\": notebook_path,\n",
    "            \"id\": f\"{os.path.basename(notebook_path)}[{i}]\"\n",
    "        }\n",
    "        for i, cell in enumerate(notebook.get(\"cells\", []))\n",
    "        if '*' in types or cell.get(\"cell_type\") in types\n",
    "    ]\n",
    "\n",
    "    return cells\n",
    "\n",
    "__all__ = {\n",
    "  \"get_cells\": get_cells\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### run()\n",
    "\n",
    "run python cells?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "\n",
    "def run():\n",
    "    from Core import import_notebook\n",
    "    if len(sys.argv) < 3:\n",
    "        print(\"Usage: python script.py <notebook_path> <function_args>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    notebook_path = sys.argv[1]\n",
    "    inputs = sys.argv[2:]\n",
    "\n",
    "    # Import the notebook as a module\n",
    "    module = import_notebook(notebook_path)\n",
    "\n",
    "    # Find the first function in the module\n",
    "    func = None\n",
    "    for name in dir(module):\n",
    "        attr = getattr(module, name)\n",
    "        if callable(attr):\n",
    "            func = attr\n",
    "            break\n",
    "\n",
    "    if not func:\n",
    "        print(\"No function found in the notebook.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Extract parameters and map inputs\n",
    "    params = get_function_params(func)\n",
    "    mapped_inputs = []\n",
    "\n",
    "    for param in params:\n",
    "        for i, arg in enumerate(inputs):\n",
    "            if arg.startswith(f\"--{param}=\"):\n",
    "                mapped_inputs.append(arg.split(\"=\")[1])\n",
    "                break\n",
    "        else:\n",
    "            mapped_inputs.append(inputs.pop(0) if inputs else None)\n",
    "\n",
    "    # Convert types based on function annotations (if available)\n",
    "    func_annotations = func.__annotations__\n",
    "    for i, param in enumerate(params):\n",
    "        if param in func_annotations:\n",
    "            mapped_inputs[i] = func_annotations[param](mapped_inputs[i])\n",
    "\n",
    "    # Execute the function\n",
    "    result = func(*mapped_inputs)\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__run__\":\n",
    "    run()\n",
    "\n",
    "__all__ = {\n",
    "  \"run\": run\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
