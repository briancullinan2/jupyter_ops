<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>discord llm connector</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../discord_tools/index.html">discord tools</a></h3>
    <a href="./discord_pdf_converter.html">discord pdf converter</a>
<br /><br />
<a href="./discord_notebook_connector.html">discord notebook connector</a>
<br /><br />
<a href="./discord_llm_connector.html">discord llm connector</a>
<br /><br />
<a href="./discord_writing_llms.html">discord writing llms</a>
<br /><br />
<a href="./discord_mesh_generator.html">discord mesh generator</a>
<br /><br />
<a href="./discord_llm_interactions.html">discord llm interactions</a>
<br /><br />
<a href="./sync_discord_llm_tools.html">sync discord llm tools</a>
<br /><br />
<a href="./sync_deceptive_chat.html">sync deceptive chat</a>
<br /><br />
<a href="./discord_handy_tools.html">discord handy tools</a>
<br /><br />
<a href="./create_discord_message_with_attachments.html">create discord message with attachments</a>
<br /><br />

  </nav>
  <header>
    <a href="../discord_tools/index.html">discord tools</a> | <a href="./discord_notebook_connector.html">discord notebook connector</a> | <a href="./discord_writing_llms.html">discord writing llms</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>doDistill</code> function is an asynchronous workflow manager that interacts with LLM modules and the Discord API to generate and post responses to user prompts. It uses conditional statements to determine the next step in the workflow, selecting an LLM tool based on user input and generating a short or long answer accordingly.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "discord llm connector"</code></pre><h1>discord llm connector</h1>



<pre class="javascript"><code>const {postMessageImageAttachment} = importer.import("<a href="../../Frontends/discord_tools/create_discord_message_with_attachments.html">create message image attachments</a>")
const {triggerTyping, createMessage, updateInteraction} = importer.import("<a href="../../Frontends/discord/discord_api.html">disrcord api</a>")
const {Remarkable} = require('remarkable');
const md = new Remarkable({html: true, xhtmlOut: true, breaks: true});
const { safeurl } = importer.import("<a href="../../Analytics/data_collection/domain_cache_tools.html">domain cache tools</a>")
const selectModel = importer.import("<a href="../../Algorithms/llm_writing/select_llm.html">select llm</a>")
const askLlamaToWriteStory = importer.import("<a href="../../Algorithms/llm_writing/write_creatively_llm.html">write creatively llm</a>")
const askLlamaToWriteBusinessPlan = importer.import("<a href="../../Algorithms/llm_writing/business_plan_llm.html">business plan llm</a>")
const askLlamaWriteEssay = importer.import("<a href="../../Algorithms/llm_writing/research_paper_llm.html">research paper llm</a>")

async function doDistill(interaction, promptModel = 'DeepSeek') {

  if(typeof promptModel != 'function') {
    promptModel = await selectModel(promptModel)
  }

  console.log('using model', promptModel.name)

  await triggerTyping(interaction.channel_id)
  let q1 = 'Is this prompt looking for a very long answer, Yes or No?\n' + interaction.data.options[0].value
    + 'Only respond with Yes or No and nothing else, no reasoning or instructions.'
  if((await promptModel(q1)).match(/no/gi)) {
    let a1 = await promptModel(interaction.data.options[0].value)
    return await updateInteraction(a1.substring(0, 1800), interaction.id, interaction.token)
  }
  await updateInteraction('This could take a while...', interaction.id, interaction.token)

  let q2 = 'Should I use the tool "essay" writer, "business plan", "creative story", or "default" prompter for this response?\n' + interaction.data.options[0].value + '\nAnswer "essay", "business", or "story" and nothing else, no explaination or instructions.'
  let a2 = await promptModel(q2)

  console.log('AI: ' + a2)

  if(a2.match(/essay/gi)) {
    let essay = await askLlamaWriteEssay(interaction.data.options[0].value, null, promptModel)
    return await postMessageImageAttachment(interaction.data.options[0].value, Buffer.from(essay), interaction.channel_id, 'text/html')
  } else if(a2.match(/business/gi)) {
    let plan = await askLlamaToWriteBusinessPlan(interaction.data.options[0].value, null, promptModel)
    // TODO: already writes to business-plans
    return await postMessageImageAttachment(interaction.data.options[0].value, Buffer.from(plan), interaction.channel_id, 'text/html')
  } else if(a2.match(/story/gi)) {
    let plan = await askLlamaToWriteStory(interaction.data.options[0].value, null, promptModel)
    // TODO: already writes to business-plans
    return await postMessageImageAttachment(interaction.data.options[0].value, Buffer.from(plan), interaction.channel_id, 'text/html')
  } else {
    let answer = await promptModel(interaction.data.options[0].value)
    const mdHtml = md.render(answer);
    return await postMessageImageAttachment(interaction.data.options[0].value, Buffer.from(mdHtml), interaction.channel_id, 'text/html')
  }
}

async function doReason(interaction) {
  return doDistill(interaction, 'Qwen')
}

async function doInstruct(interaction) {
  return doDistill(interaction, 'Llama-3-70B')
}

async function doMistral(interaction) {
  return doDistill(interaction, 'Mistral')
}

module.exports = {
  doDistill,
  doReason,
  doInstruct,
  doMistral,

}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>// Import required modules and functions
const { 
  postMessageImageAttachment, 
  triggerTyping, 
  createMessage, 
  updateInteraction, 
} = require('./importer').import('discord-api');

const { safeurl } = require('./importer').import('domain-cache-tools');
const { selectModel, 
  askLlamaToWriteStory, 
  askLlamaToWriteBusinessPlan, 
  askLlamaWriteEssay, 
} = require('./importer').import([
 'select-llm',
  'write-creatively-llm',
  'business-plan-llm',
 'research-paper-llm',
]);

const md = new (require('remarkable'))({ html: true, xhtmlOut: true, breaks: true });

/**
 * Determines the response to a prompt based on the chosen model and prompt content.
 * 
 * @param {Object} interaction - The Discord interaction containing prompt and options.
 * @param {string} [promptModel='DeepSeek'] - The model to use for generating the response.
 * @returns {Promise} - A promise resolving to the generated response.
 */
async function doDistill(interaction, promptModel = 'DeepSeek') {
  if (typeof promptModel!== 'function') {
    promptModel = await selectModel(promptModel);
  }

  console.log(`Using model: ${promptModel.name}`);

  // Trigger typing indicator to show that the model is working
  await triggerTyping(interaction.channel_id);

  // Determine if the prompt requires a short or long response
  const q1 = `Is this prompt looking for a very long answer, Yes or No?\n${interaction.data.options[0].value}`;
  const a1 = await promptModel(q1);
  if (a1.match(/no/gi)) {
    // Short response, return the answer directly
    return updateInteraction(a1.substring(0, 1800), interaction.id, interaction.token);
  }

  // Long response, trigger interaction update to show progress
  await updateInteraction('This could take a while...', interaction.id, interaction.token);

  // Determine which tool to use for generating the response
  const q2 = `Should I use the tool "essay" writer, "business plan", "creative story", or "default" prompter for this response?\n${interaction.data.options[0].value}`;
  const a2 = await promptModel(q2);

  console.log(`AI: ${a2}`);

  // Select the appropriate tool based on the user's preference
  let result;
  switch (a2.toLowerCase()) {
    case 'essay':
      result = await askLlamaWriteEssay(interaction.data.options[0].value, null, promptModel);
      break;
    case 'business':
      result = await askLlamaToWriteBusinessPlan(interaction.data.options[0].value, null, promptModel);
      break;
    case'story':
      result = await askLlamaToWriteStory(interaction.data.options[0].value, null, promptModel);
      break;
    default:
      result = await promptModel(interaction.data.options[0].value);
      break;
  }

  // Render the response as HTML and post it to the channel
  const mdHtml = md.render(result);
  return postMessageImageAttachment(interaction.data.options[0].value, Buffer.from(mdHtml), interaction.channel_id, 'text/html');
}

/**
 * A wrapper function for doDistill with a default model.
 * 
 * @param {Object} interaction - The Discord interaction containing prompt and options.
 * @returns {Promise} - A promise resolving to the generated response.
 */
async function doReason(interaction) {
  return doDistill(interaction, 'Qwen');
}

/**
 * A wrapper function for doDistill with a default model.
 * 
 * @param {Object} interaction - The Discord interaction containing prompt and options.
 * @returns {Promise} - A promise resolving to the generated response.
 */
async function doInstruct(interaction) {
  return doDistill(interaction, 'Llama-3-70B');
}

/**
 * A wrapper function for doDistill with a default model.
 * 
 * @param {Object} interaction - The Discord interaction containing prompt and options.
 * @returns {Promise} - A promise resolving to the generated response.
 */
async function doMistral(interaction) {
  return doDistill(interaction, 'Mistral');
}

module.exports = {
  doDistill,
  doReason,
  doInstruct,
  doMistral,
};</code></pre></div><p><strong>Code Breakdown</strong></p>
<h3>Dependencies and Imports</h3>
<p>The code imports various functions and tools from other modules using the <code>importer</code> object. The imported functions include:</p>
<ul>
<li><code>postMessageImageAttachment</code> from <code>create message image attachments</code></li>
<li><code>triggerTyping</code>, <code>createMessage</code>, and <code>updateInteraction</code> from <code>disrcord api</code></li>
<li><code>safeurl</code> from <code>domain cache tools</code></li>
<li><code>selectModel</code> from <code>select llm</code></li>
<li><code>askLlamaToWriteStory</code>, <code>askLlamaToWriteBusinessPlan</code>, and <code>askLlamaWriteEssay</code> from various LLM (Large Language Model) modules</li>
</ul>
<h3>Remarkable Markdown Parser</h3>
<p>The code creates an instance of the <code>Remarkable</code> markdown parser, allowing for HTML output and line breaks.</p>
<h3>doDistill Function</h3>
<p>The <code>doDistill</code> function is an asynchronous function that takes an <code>interaction</code> object and an optional <code>promptModel</code> parameter. It appears to be a workflow manager that interacts with the LLM modules and Discord API to:</p>
<ol>
<li><strong>Trigger typing</strong>: Inform the user that a response is being generated.</li>
<li><strong>Ask for answer length</strong>: Determine if the user wants a short or long answer.</li>
<li><strong>Choose an LLM tool</strong>: Select a suitable LLM tool based on the user's response.</li>
<li><strong>Generate response</strong>: Use the chosen LLM tool to generate a response.</li>
<li><strong>Post response</strong>: Post the response to the Discord channel, possibly with an image attachment.</li>
</ol>
<h3>Logic Flow</h3>
<p>The function uses a series of conditional statements to determine the next step in the workflow. The main logic flow is:</p>
<ol>
<li>If the prompt is not a function, select a suitable LLM model using the <code>selectModel</code> function.</li>
<li>Ask the user if they want a short or long answer.</li>
<li>If the user wants a short answer, generate a response using the chosen LLM tool and post it to the Discord channel.</li>
<li>If the user wants a longer answer, choose an LLM tool and generate a response.</li>
<li>Post the response to the Discord channel, possibly with an image attachment.</li>
</ol>

</body>

</html>