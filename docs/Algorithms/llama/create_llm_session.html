<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>create llm session</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama/index.html">llama</a></h3>
    <a href="./search_llama.html">search llama</a>
<br /><br />
<a href="./ask_llm_about_categories.html">ask llm about categories</a>
<br /><br />
<a href="./ask_llm_to_generalize_categories.html">ask llm to generalize categories</a>
<br /><br />
<a href="./ask_llm_for_a_shorter_list_of_categories.html">ask llm for a shorter list of categories</a>
<br /><br />
<a href="./ask_llm_about_functions.html">ask llm about functions</a>
<br /><br />
<a href="./ask_llm_about_code.html">ask llm about code</a>
<br /><br />
<a href="./ask_llm_about_notebooks.html">ask llm about notebooks</a>
<br /><br />
<a href="./ask_llm_to_summerize.html">ask llm to summerize</a>
<br /><br />
<a href="./store_llama_function.html">store llama function</a>
<br /><br />
<a href="./store_all_notebook_llm_functions.html">store all notebook llm functions</a>
<br /><br />
<a href="./create_llm_session.html">create llm session</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama/index.html">llama</a> | <a href="./store_all_notebook_llm_functions.html">store all notebook llm functions</a> |  | <a href="../../search.html">Search</a>
  </header>

  <p>The code imports necessary modules, defines constants and variables, and creates LLaMA-related objects for initializing and interacting with a large language model. It includes two main functions: <code>createSession</code> for creating a new model instance and <code>initSession</code> for initializing a new chat session with the model.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "create llm session"</code></pre><h1>create llm session</h1>



<pre class="javascript"><code>import path from "path"
import {getLlama, LlamaChatSession} from "node-llama-cpp"
import process from "process"

const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE
//const __dirname = path.dirname(fileURLToPath(import.meta.url));

let llama
let model
let context
let session
let initialChatHistory


async function initSession(context2, prompt) {
  if(!context2) {
    context = await model.createContext()
  } else {
    context = context2
  }

  session = new LlamaChatSession({
      contextSequence: context.getSequence(),
      systemPrompt: prompt ? prompt : '```markdown\n# You are a\n##large language model\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\n&lt;/think&gt;\n...```\n'
  })
  // initialize the model
  //console.log(await session.prompt())
  initialChatHistory = session.getChatHistory();

  // Reset the chat history
  session.setChatHistory(initialChatHistory);
  return session
}

async function createSession(modelPath) {
  if(!llama) {
    llama = await getLlama();
  }

  // TODO: customizable model here

  if(!model) {
      model = await llama.loadModel({
          // "code-llama-7b-chat.gguf"
          // "Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"
          // 'ggml-model-f16.gguf'
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'ggml-model-f16.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'gemma-2-9b-it-Q6_K-Q8.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'deepseek-llm-7b-chat.Q5_K_M.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'Meta-Llama-3.1-70B-Instruct-Q5_K_S.gguf' ),
          modelPath: process.env.MODELPATH || modelPath || path.join(HOMEPATH, "llama.cpp", "models", 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'deepseek-llm-7b-chat.Q6_K.gguf' ),
          contextSize: 2048
      })
  }

  await initSession()

  return session
}

async function getSession(model) {
  if(!session) {
    await createSession(model)
  }
  return session
}

async function llmPrompt(prompt, model) {
  if(!model || typeof model == 'string') {
    // specify model with the incoming session
    model = await getSession(model)
  }
  let result = await model.prompt(prompt, {
    temperature: 0.8,
    maxTokens: context.contextSize,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  // special fix for deepseek R1 distill
  if(result.match(/Find the derivative/gi)) {
    console.log('Response error', result)
    await initSession()
    result = await session.prompt(prompt, {
      temperature: 0.8,
      maxTokens: context.contextSize,
      onTextChunk: function (text) {
        process.stdout.write(text)
      }
    })
  }
  //if(result.includes('&lt;/think&gt;')) {
  //  result = result.split('&lt;/think&gt;\n')[1]
  //}
  if(result.startsWith('```') &amp;&amp; result.endsWith('```')) {
    result = result.replace(/^```(markdown)\n*|\n```$/gi, '')
  }
  if(session == model)
    model.setChatHistory(initialChatHistory);
  return result
}

export default {
  llmPrompt,
  createSession,
  initSession,
  getSession,
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>import path from "path";
import { getLlama, LlamaChatSession } from "node-llama-cpp";
import process from "process";

const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE;

class Llama {
  constructor() {
    this.llama = null;
    this.model = null;
    this.context = null;
    this.session = null;
    this.initialChatHistory = null;
    this.modelPath = process.env.MODELPATH || path.join(HOMEPATH, "llama.cpp", "models", 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf');
  }

  async initSession(context = null, prompt = '# You are a\n##large language model\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\n</think>\n...') {
    this.context = context? context : await this.model.createContext();
    this.session = new LlamaChatSession({
      contextSequence: this.context.getSequence(),
      systemPrompt: prompt,
    });
    this.initialChatHistory = this.session.getChatHistory();
    this.session.setChatHistory(this.initialChatHistory);
    return this.session;
  }

  async createSession() {
    if (!this.llama) {
      this.llama = await getLlama();
    }

    if (!this.model) {
      this.model = await this.llama.loadModel({
        modelPath: this.modelPath,
        contextSize: 2048,
      });
    }

    await this.initSession();

    return this.session;
  }

  async getSession() {
    if (!this.session) {
      await this.createSession();
    }
    return this.session;
  }

  async llmPrompt(prompt, model = null) {
    if (!model) {
      model = await this.getSession();
    }
    if (model && typeof model ==='string') {
      model = await this.getSession(model);
    }
    let result = await model.prompt(prompt, {
      temperature: 0.8,
      maxTokens: this.context.contextSize,
      onTextChunk: (text) => process.stdout.write(text),
    });

    if (result.includes('```markdown') && result.endsWith('```')) {
      result = result.replace(/^```(markdown)*\n*|\n*```$/gi, '');
    }

    if (result.startsWith('Response error')) {
      await this.initSession();
      result = await this.session.prompt(prompt, {
        temperature: 0.8,
        maxTokens: this.context.contextSize,
        onTextChunk: (text) => process.stdout.write(text),
      });
    }

    this.model.setChatHistory(this.initialChatHistory);
    return result;
  }
}

export default new Llama({
  llmPrompt,
  createSession,
  initSession,
  getSession,
});</code></pre></div><p><strong>Code Breakdown</strong></p>
<h3>Import Statements</h3>
<p>The code imports the following modules:</p>
<ul>
<li><code>path</code> from the Node.js <code>path</code> module for working with file paths.</li>
<li><code>getLlama</code> and <code>LlamaChatSession</code> from the <code>node-llama-cpp</code> module, which is a Node.js wrapper for the LLaMA (Large Language Model Application) C++ API.</li>
<li><code>process</code> from the Node.js <code>process</code> module, which provides access to environment variables.</li>
</ul>
<h3>Constants and Variables</h3>
<p>The code defines the following constants and variables:</p>
<ul>
<li><code>HOMEPATH</code>: a string representing the user's home directory, determined by checking the <code>HOME</code>, <code>HOMEPATH</code>, and <code>USERPROFILE</code> environment variables.</li>
<li><code>llama</code>, <code>model</code>, <code>context</code>, <code>session</code>, and <code>initialChatHistory</code>: variables that will store instances of LLaMA-related objects.</li>
</ul>
<h3>Functions</h3>
<p>The code defines the following functions:</p>
<h4><code>initSession(context2, prompt)</code></h4>
<p>An asynchronous function that initializes a new chat session with the LLaMA model:</p>
<ul>
<li>If <code>context2</code> is provided, it uses that context. Otherwise, it creates a new context using the <code>createContext</code> method of the <code>model</code>.</li>
<li>It creates a new <code>LlamaChatSession</code> instance with the context sequence and a system prompt (if provided).</li>
<li>It initializes the model and gets the initial chat history.</li>
<li>It resets the chat history and returns the session.</li>
</ul>
<h4><code>createSession(modelPath)</code></h4>
<p>An asynchronous function that creates a new LLaMA model instance:</p>
<ul>
<li>If the <code>llama</code> instance is not already created, it creates a new instance using the <code>getLlama</code> function.</li>
<li>It loads a model from the specified <code>modelPath</code> using the <code>loadModel</code> method of the <code>llama</code> instance.</li>
<li>It sets the <code>model</code> variable to the loaded model instance.</li>
<li>It returns the model instance.</li>
</ul>
<h3>Notes</h3>
<p>The code appears to be part of a larger program that uses the LLaMA C++ API to interact with a large language model. The <code>createSession</code> function seems to be the entry point for creating a new model instance, while the <code>initSession</code> function is used to initialize a new chat session with the model.</p>

</body>

</html>