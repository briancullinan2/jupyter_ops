<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>create llm session</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
    }

    body {
      padding-left: 200px;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <a href="../llama/index.html">llama</a>
    <br /><br />
    <a href="./search_llama.html">search llama</a>
<br /><br />
<a href="./ask_llm_about_categories.html">ask llm about categories</a>
<br /><br />
<a href="./ask_llm_to_generalize_categories.html">ask llm to generalize categories</a>
<br /><br />
<a href="./ask_llm_for_a_shorter_list_of_categories.html">ask llm for a shorter list of categories</a>
<br /><br />
<a href="./ask_llm_about_functions.html">ask llm about functions</a>
<br /><br />
<a href="./ask_llm_about_code.html">ask llm about code</a>
<br /><br />
<a href="./ask_llm_about_notebooks.html">ask llm about notebooks</a>
<br /><br />
<a href="./ask_llm_to_summerize.html">ask llm to summerize</a>
<br /><br />
<a href="./store_llama_function.html">store llama function</a>
<br /><br />
<a href="./store_all_notebook_llm_functions.html">store all notebook llm functions</a>
<br /><br />
<a href="./create_llm_session.html">create llm session</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama/index.html">llama</a> ${PREV} ${NEXT}
  </header>

  <p>Here is a summary of the code in one sentence:</p>
<p>The code initializes a chat session with the LLaMA model by importing necessary modules, defining variables and constants, and creating two asynchronous functions to initialize and create a new session.</p>
<p>Or in two sentences:</p>
<p>The code sets up a chat session with the LLaMA model by importing required modules and defining key variables and constants. Two asynchronous functions, <code>initSession</code> and <code>createSession</code>, are defined to handle the initialization and creation of new chat sessions.</p>


<pre class="javascript"><code>import path from "path"
import {getLlama, LlamaChatSession} from "node-llama-cpp"
import process from "process"

const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE
//const __dirname = path.dirname(fileURLToPath(import.meta.url));

let llama
let model
let context
let session
let initialChatHistory


async function initSession(context2, prompt) {
  if(!context2) {
    context = await model.createContext()
  } else {
    context = context2
  }

  session = new LlamaChatSession({
      contextSequence: context.getSequence(),
      systemPrompt: prompt ? prompt : '```markdown\n# You are a\n##large language model\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\n</think>\n...```\n'
  })
  // initialize the model
  //console.log(await session.prompt())
  initialChatHistory = session.getChatHistory();

  // Reset the chat history
  session.setChatHistory(initialChatHistory);
  return session
}

async function createSession(modelPath) {
  if(!llama) {
    llama = await getLlama();
  }

  // TODO: customizable model here

  if(!model) {
      model = await llama.loadModel({
          // "code-llama-7b-chat.gguf"
          // "Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"
          // 'ggml-model-f16.gguf'
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'ggml-model-f16.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'gemma-2-9b-it-Q6_K-Q8.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'deepseek-llm-7b-chat.Q5_K_M.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'Meta-Llama-3.1-70B-Instruct-Q5_K_S.gguf' ),
          modelPath: process.env.MODELPATH || modelPath || path.join(HOMEPATH, "llama.cpp", "models", 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf' ),
          //modelPath: path.join(HOMEPATH, "llama.cpp", "models", 'deepseek-llm-7b-chat.Q6_K.gguf' ),
          contextSize: 2048
      })
  }

  await initSession()

  return session
}

async function getSession(model) {
  if(!session) {
    await createSession(model)
  }
  return session
}

async function llmPrompt(prompt, model) {
  if(!model || typeof model == 'string') {
    // specify model with the incoming session
    model = await getSession(model)
  }
  let result = await model.prompt(prompt, {
    temperature: 0.8,
    maxTokens: context.contextSize,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  // special fix for deepseek R1 distill
  if(result.match(/Find the derivative/gi)) {
    console.log('Response error', result)
    await initSession()
    result = await session.prompt(prompt, {
      temperature: 0.8,
      maxTokens: context.contextSize,
      onTextChunk: function (text) {
        process.stdout.write(text)
      }
    })
  }
  //if(result.includes('</think>')) {
  //  result = result.split('</think>\n')[1]
  //}
  if(result.startsWith('```') && result.endsWith('````')) {
    result = result.replace(/^```(markdown)*\n*|\n*```$/gi, '')
  }
  if(session == model)
    model.setChatHistory(initialChatHistory);
  return result
}

export default {
  llmPrompt,
  createSession,
  initSession,
  getSession,
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>

<script>hljs.highlightAll();</script>

<p><strong>Breakdown of the Code</strong></p>
<h3>Import Statements</h3>
<p>The code starts by importing the following modules:</p>
<ul>
<li><code>path</code> from the <code>path</code> module for working with file paths</li>
<li><code>getLlama</code> and <code>LlamaChatSession</code> from the <code>node-llama-cpp</code> module, which is a Node.js wrapper for the LLaMA (Large Language Model Applications) C++ library</li>
<li><code>process</code> from the <code>process</code> module for accessing environment variables</li>
</ul>
<h3>Variables and Constants</h3>
<p>The code defines the following variables and constants:</p>
<ul>
<li><code>HOMEPATH</code>: the user's home directory, which is obtained from environment variables <code>HOME</code>, <code>HOMEPATH</code>, and <code>USERPROFILE</code></li>
<li><code>llama</code>: an instance of the LLaMA model</li>
<li><code>model</code>: the loaded LLaMA model</li>
<li><code>context</code>: the context for the LLaMA model</li>
<li><code>session</code>: the chat session for the LLaMA model</li>
<li><code>initialChatHistory</code>: the initial chat history for the session</li>
</ul>
<h3>Functions</h3>
<p>The code defines two asynchronous functions:</p>
<ul>
<li><code>initSession(context2, prompt)</code>: initializes a new chat session with the given <code>context</code> and <code>prompt</code>. If <code>context2</code> is not provided, it creates a new context using the <code>model.createContext()</code> method.</li>
<li><code>createSession(modelPath)</code>: creates a new instance of the LLaMA model using the <code>getLlama()</code> method and loads a model from the specified <code>modelPath</code>. If no <code>modelPath</code> is provided, it uses the <code>MODELPATH</code> environment variable or a default model path.</li>
</ul>
<h3>Notes</h3>
<ul>
<li>The code uses the <code>async/await</code> syntax to handle asynchronous operations.</li>
<li>The <code>getLlama()</code> method is used to create an instance of the LLaMA model.</li>
<li>The <code>loadModel()</code> method is used to load a model from the specified <code>modelPath</code>.</li>
<li>The <code>createContext()</code> method is used to create a new context for the LLaMA model.</li>
<li>The <code>LlamaChatSession</code> class is used to represent a chat session with the LLaMA model.</li>
<li>The <code>setChatHistory()</code> method is used to reset the chat history for the session.</li>
</ul>

</body>

</html>