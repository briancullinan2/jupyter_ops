<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>ask llm about categories</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama/index.html">llama</a></h3>
    <a href="./search_llama.html">search llama</a>
<br /><br />
<a href="./ask_llm_about_categories.html">ask llm about categories</a>
<br /><br />
<a href="./ask_llm_to_generalize_categories.html">ask llm to generalize categories</a>
<br /><br />
<a href="./ask_llm_for_a_shorter_list_of_categories.html">ask llm for a shorter list of categories</a>
<br /><br />
<a href="./ask_llm_about_functions.html">ask llm about functions</a>
<br /><br />
<a href="./ask_llm_about_code.html">ask llm about code</a>
<br /><br />
<a href="./ask_llm_about_notebooks.html">ask llm about notebooks</a>
<br /><br />
<a href="./ask_llm_to_summerize.html">ask llm to summerize</a>
<br /><br />
<a href="./store_llama_function.html">store llama function</a>
<br /><br />
<a href="./store_all_notebook_llm_functions.html">store all notebook llm functions</a>
<br /><br />
<a href="./create_llm_session.html">create llm session</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama/index.html">llama</a> | <a href="./search_llama.html">search llama</a> | <a href="./ask_llm_to_generalize_categories.html">ask llm to generalize categories</a> | <a href="../../search.html">Search</a>
  </header>

  <p>This function, <code>askLlamaAboutCategories</code>, queries the Llama language model about categories related to a given query and returns a list of notebook filenames containing matching categories.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "ask llm about categories"</code></pre><h1>ask llm about categories</h1>



<pre class="javascript"><code>const { functionCache } = importer.import("<a href="../../Databases/caches/cache_rpc_functions_with_llm_descriptions.html">cache rpc functions with llm descriptions</a>")
const {askLlamaAboutFunctions} = importer.import("<a href="../../Algorithms/llama/ask_llm_about_functions.html">ask llm about functions</a>")


async function askLlamaAboutCategories(query) {
  // TODO: list all categories in database
  let keys = Object.keys(functionCache)
  let categorys = keys.map(k =&gt; functionCache[k].categories)
    .concat(keys.map(k =&gt; functionCache[k].category))
    .filter((a, i, arr) =&gt; a &amp;&amp; arr.indexOf(a) == i &amp;&amp; !a.includes('\n'))

  // TODO: ask llm if any of the categories match, don't choose best one, choose all matches
  let returnValues = []
  let functions = []
  for(let i = 0; i &lt; categorys.length; i++) {
    let category = categorys[i]
    functions[functions.length] = category

    if(functions.length == 20) {
      let result = await askLlamaAboutFunctions(query, functions, [], true)
      functions = []
      if(result)
        returnValues = returnValues.concat(result)
    }
  }
  if(functions.length &gt; 0) {
      let result = await askLlamaAboutFunctions(query, functions, [], true)
      functions = []
      if(result)
        returnValues = returnValues.concat(result)
  }

  // TODO: return notebook filenames that contain matching categories
  let matching = keys.filter(k =&gt; returnValues.includes(functionCache[k].category) || returnValues.includes(functionCache[k].categories))
    .map(k =&gt; k.replace(/\[[0-9]*\]/, ''))
    .filter((a, i, arr) =&gt; a &amp;&amp; arr.indexOf(a) == i)
  return matching
}

module.exports = {
  askLlamaAboutCategories
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>const { functionCache, cacheSize } = importer.import('cache rpc functions with llm descriptions');
const { askLlamaAboutFunctions } = importer.import('ask llm about functions');

/**
 * Asks the LLaMA model about categories and returns notebook filenames containing matching categories.
 *
 * @param {string} query - The query to ask the LLaMA model.
 * @returns {Promise<string[]>} - A promise resolving to an array of notebook filenames containing matching categories.
 */
async function askLlamaAboutCategories(query) {
  // Get unique categories from function cache
  const categories = Array.from(new Set(
    Object.values(functionCache).flatMap(fc => [fc.category,...fc.categories])
  ));

  // Ask LLaMA model for matching categories
  const results = [];
  const chunkSize = Math.min(cacheSize, 20);
  for (const chunk of chunked(categories, chunkSize)) {
    const response = await askLlamaAboutFunctions(query, chunk, [], true);
    if (response) results.push(...response);
  }

  // Get notebook filenames containing matching categories
  const matching = Object.keys(functionCache)
   .filter(k => results.includes(functionCache[k].category) || results.includes(functionCache[k].categories))
   .map(k => k.replace(/\[[0-9]*\]/, ''))
   .filter((a, i, arr) => a && arr.indexOf(a) == i);

  return matching;
}

// Chunk an array into smaller arrays of a specified size
function chunked(array, size) {
  return Array(Math.ceil(array.length / size))
   .fill()
   .map((_, i) => array.slice(i * size, (i + 1) * size));
}

module.exports = { askLlamaAboutCategories };</code></pre></div><p><strong>Code Breakdown</strong></p>
<h3>Function <code>askLlamaAboutCategories</code></h3>
<h4>Purpose</h4>
<p>Queries the Llama language model about categories related to a given query.</p>
<h4>Parameters</h4>
<ul>
<li><code>query</code> (string): The query to ask the Llama language model.</li>
</ul>
<h4>Functionality</h4>
<ol>
<li>Retrieves all cached function keys and their corresponding categories.</li>
<li>Concatenates and filters the categories to remove duplicates and newline characters.</li>
<li>Iterates through the categories, grouping them into batches of 20 functions.</li>
<li>For each batch, asks the Llama language model if any of the categories match the query, and collects the matching categories.</li>
<li>Returns a list of notebook filenames that contain matching categories.</li>
</ol>
<h4>Return Value</h4>
<ul>
<li>An array of notebook filenames containing matching categories.</li>
</ul>
<h4>Notes</h4>
<ul>
<li>The <code>TODO</code> comments indicate that the code is incomplete and requires further implementation.</li>
<li>The <code>askLlamaAboutFunctions</code> function is called with <code>true</code> as the last argument, which is not clear what this flag is supposed to do.</li>
<li>The <code>functionCache</code> object and <code>importer</code> variable are not defined in this code snippet.</li>
</ul>

</body>

</html>