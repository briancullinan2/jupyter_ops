<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>ask llm about functions</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama/index.html">llama</a></h3>
    <a href="./search_llama.html">search llama</a>
<br /><br />
<a href="./ask_llm_about_categories.html">ask llm about categories</a>
<br /><br />
<a href="./ask_llm_to_generalize_categories.html">ask llm to generalize categories</a>
<br /><br />
<a href="./ask_llm_for_a_shorter_list_of_categories.html">ask llm for a shorter list of categories</a>
<br /><br />
<a href="./ask_llm_about_functions.html">ask llm about functions</a>
<br /><br />
<a href="./ask_llm_about_code.html">ask llm about code</a>
<br /><br />
<a href="./ask_llm_about_notebooks.html">ask llm about notebooks</a>
<br /><br />
<a href="./ask_llm_to_summerize.html">ask llm to summerize</a>
<br /><br />
<a href="./store_llama_function.html">store llama function</a>
<br /><br />
<a href="./store_all_notebook_llm_functions.html">store all notebook llm functions</a>
<br /><br />
<a href="./create_llm_session.html">create llm session</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama/index.html">llama</a> | <a href="./ask_llm_for_a_shorter_list_of_categories.html">ask llm for a shorter list of categories</a> | <a href="./ask_llm_about_code.html">ask llm about code</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>askLlamaAboutFunctions</code> function is an asynchronous query that matches a given input with a function listed in an array, providing an optional description and categorization. It constructs a query string, sends it to a Large Language Model, and returns the matched function name.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "ask llm about functions"</code></pre><h1>ask llm about functions</h1>



<pre class="javascript"><code>
async function askLlamaAboutFunctions(query, functions, descriptions, categories = false) {
  const {llmPrompt} = await importer.import("<a href="../../Algorithms/llama/create_llm_session.html">create llm session</a>")

  let q1 = "Given a list of " + (categories ? 'categories' : "functions") + ":\n";
  for(let i = 0; i &lt; functions.length; i++) {
      if(descriptions[i]) {
          q1 += functions[i] + ' - ' + descriptions[i] + '\n'
      } else {
          q1 += functions[i] + '\n'
      }
  }
  q1 += 'which one most closely matches the query \"' + query + '\"?'
  console.log("User: " + q1);
  const a1 = await llmPrompt(q1);
  console.log("AI: " + a1);
  // TODO: parse function name
  let result = a1.trim().split(/["`*'\n]/gi).filter(x =&gt; functions.indexOf(x) &gt; -1)[0]
  return result
}

module.exports = {
  askLlamaAboutFunctions,
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>/**
 * Asks Llama about functions or categories to determine which one most closely matches the given query.
 *
 * @param {string} query - The query to match.
 * @param {string[]} functions - A list of functions or categories to consider.
 * @param {string[]} [descriptions] - A list of descriptions for the functions or categories.
 * @param {boolean} [categories=false] - Flag to indicate if the input is categories.
 * @returns {string} The function or category that most closely matches the query.
 */
async function askLlamaAboutFunctions(query, functions, descriptions = [], categories = false) {
  const {
    llmPrompt,
  } = await import('create-llm-session');

  // Create the prompt with the list of functions or categories
  const prompt = functions
   .map((func, index) => {
      return categories
       ? `${func} - ${descriptions[index] || ''}`
        : func;
    })
   .join('\n');

  // Add the final question to the prompt
  const finalPrompt = `Given the list of ${categories? 'categories' : 'functions'}:\n${prompt}\nWhich one most closely matches the query "${query}"?`;

  console.log('User:', finalPrompt);

  // Get the AI response
  const aiResponse = await llmPrompt(finalPrompt);
  console.log('AI:', aiResponse);

  // Extract the matched function or category from the AI response
  const result = functions.find(func => aiResponse.toLowerCase().includes(func.toLowerCase()));

  return result;
}

export default askLlamaAboutFunctions;</code></pre></div><h2>Code Breakdown</h2>
<h3>Function: <code>askLlamaAboutFunctions</code></h3>
<h4>Purpose</h4>
<p>An asynchronous function that queries a Large Language Model (LLM) about a list of functions.</p>
<h4>Parameters</h4>
<ul>
<li><code>query</code>: The input query to be matched with a function.</li>
<li><code>functions</code>: An array of function names.</li>
<li><code>descriptions</code>: An optional array of descriptions for the functions (default: <code>false</code>).</li>
<li><code>categories</code>: An optional boolean to specify whether the input is a list of categories (default: <code>false</code>).</li>
</ul>
<h4>Steps</h4>
<ol>
<li>Imports the <code>llmPrompt</code> function from the <code>create llm session</code> module.</li>
<li>Constructs a query string by listing the functions and their descriptions (if provided).</li>
<li>Sends the query to the LLM and receives a response.</li>
<li>Parses the response to find the function name that matches the query.</li>
<li>Returns the matched function name.</li>
</ol>
<h4>Export</h4>
<p>The <code>askLlamaAboutFunctions</code> function is exported as a module.</p>

</body>

</html>