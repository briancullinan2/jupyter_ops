<p>Here is a summary of the code in two sentences:</p>
<p>The code defines two asynchronous functions, <code>askLlamaAboutConversation</code> and <code>askLlamaAboutCategory</code>, which use a large language model (LLM) to summarize and categorize a conversation. Both functions interact with the LLM by importing a session, creating a prompt, logging user and LLM responses, and returning the LLM's response after processing.</p>


<pre><code>
async function askLlamaAboutConversation(currentMessages) {
  const {llmPrompt} = await importer.import('create llm session')
  let q1 = 'Can you summarize in two sentences what this conversation is about:\n' + 
  currentMessages.join('\n') + '\nPlease discard any pleasantries, documentation only.'
  console.log("User: " + q1);
  const a1 = await llmPrompt(q1);
  console.log("AI: " + a1);
  return a1.trim()
}

async function askLlamaAboutCategory(currentMessages) {
  const {llmPrompt} = await importer.import('create llm session')
  let q1 = 'Categorize this conversation in two or three words:\n' + 
  currentMessages.join('\n') + '\nOnly respond with the category.'
  console.log("User: " + q1);
  const a1 = await llmPrompt(q1);
  console.log("AI: " + a1);
  return a1.trim().split(/\s*\n\s*|,\s*|\s*- |\s*\* /gi)[0]
}

module.exports = {
  askLlamaAboutConversation,
  askLlamaAboutCategory
}

</code></pre>

<h2>Code Breakdown</h2>
<h3>Overview</h3>
<p>The provided code defines two asynchronous functions <code>askLlamaAboutConversation</code> and <code>askLlamaAboutCategory</code> that interact with a large language model (LLM) to gather information about a conversation.</p>
<h3>Function 1: <code>askLlamaAboutConversation</code></h3>
<h4>Purpose</h4>
<p>Asks the LLM to summarize a conversation in two sentences.</p>
<h4>Parameters</h4>
<ul>
<li><code>currentMessages</code>: An array of messages in the conversation.</li>
</ul>
<h4>Flow</h4>
<ol>
<li>Imports the LLM session using <code>importer.import('create llm session')</code>.</li>
<li>Creates a prompt asking the LLM to summarize the conversation.</li>
<li>Logs the user's prompt to the console.</li>
<li>Calls the LLM session with the prompt and retrieves the response.</li>
<li>Logs the LLM's response to the console.</li>
<li>Returns the LLM's response after trimming any unnecessary whitespace.</li>
</ol>
<h3>Function 2: <code>askLlamaAboutCategory</code></h3>
<h4>Purpose</h4>
<p>Asks the LLM to categorize a conversation in two or three words.</p>
<h4>Parameters</h4>
<ul>
<li><code>currentMessages</code>: An array of messages in the conversation.</li>
</ul>
<h4>Flow</h4>
<ol>
<li>Imports the LLM session using <code>importer.import('create llm session')</code>.</li>
<li>Creates a prompt asking the LLM to categorize the conversation.</li>
<li>Logs the user's prompt to the console.</li>
<li>Calls the LLM session with the prompt and retrieves the response.</li>
<li>Logs the LLM's response to the console.</li>
<li>Returns the first word of the LLM's response after removing any unnecessary whitespace and punctuation.</li>
</ol>
<h3>Module Exports</h3>
<p>The code exports both functions <code>askLlamaAboutConversation</code> and <code>askLlamaAboutCategory</code> as part of the <code>module.exports</code> object.</p>
