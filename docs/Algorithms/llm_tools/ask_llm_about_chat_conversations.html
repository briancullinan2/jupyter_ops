<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>ask llm about chat conversations</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llm_tools/index.html">llm tools</a></h3>
    <a href="./scan_chat_logs.html">scan chat logs</a>
<br /><br />
<a href="./ask_llm_about_emotions.html">ask llm about emotions</a>
<br /><br />
<a href="./cache_chat_logs.html">cache chat logs</a>
<br /><br />
<a href="./ask_llm_about_chat_conversations.html">ask llm about chat conversations</a>
<br /><br />
<a href="./llm_respond_like_a_personality.html">llm respond like a personality</a>
<br /><br />

  </nav>
  <header>
    <a href="../llm_tools/index.html">llm tools</a> | <a href="./cache_chat_logs.html">cache chat logs</a> | <a href="./llm_respond_like_a_personality.html">llm respond like a personality</a> | <a href="../../search.html">Search</a>
  </header>

  <p>This code defines two asynchronous functions, <code>askLlamaAboutConversation</code> and <code>askLlamaAboutCategory</code>, that utilize a large language model for text analysis and summarization. The functions are designed to work with a <code>create llm session</code> module and are exported as a JavaScript module.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "ask llm about chat conversations"</code></pre><h1>ask llm about chat conversations</h1>



<pre class="javascript"><code>
async function askLlamaAboutConversation(currentMessages) {
  const {llmPrompt} = await importer.import("<a href="../../Algorithms/llama/create_llm_session.html">create llm session</a>")
  let q1 = 'Can you summarize in two sentences what this conversation is about:\n' + 
  currentMessages.join('\n') + '\nPlease discard any pleasantries, documentation only.'
  console.log("User: " + q1);
  const a1 = await llmPrompt(q1);
  console.log("AI: " + a1);
  return a1.trim()
}

async function askLlamaAboutCategory(currentMessages) {
  const {llmPrompt} = await importer.import("<a href="../../Algorithms/llama/create_llm_session.html">create llm session</a>")
  let q1 = 'Categorize this conversation in two or three words:\n' + 
  currentMessages.join('\n') + '\nOnly respond with the category.'
  console.log("User: " + q1);
  const a1 = await llmPrompt(q1);
  console.log("AI: " + a1);
  return a1.trim().split(/\s*\n\s*|,\s*|\s*- |\s*\* /gi)[0]
}

module.exports = {
  askLlamaAboutConversation,
  askLlamaAboutCategory
}

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>const importer = require('./importer'); // assuming importer is defined in a separate file
const logger = require('./logger'); // assuming a logger is defined in a separate file

/**
 * Asks Llama about the conversation category.
 *
 * @param {string[]} currentMessages - The current messages in the conversation.
 * @returns {Promise<string>} The category of the conversation.
 */
async function askLlamaAboutConversation(currentMessages) {
  // Import the LLM prompt function
  const { llmPrompt } = await importer.import('createLlmSession');

  // Prepare the prompt with the current messages
  const prompt = `Can you summarize in two sentences what this conversation is about:
${currentMessages.join('\n')}
Please discard any pleasantries, documentation only.`;

  // Log the user message
  logger.log(`User: ${prompt}`);

  try {
    // Ask Llama for a response
    const response = await llmPrompt(prompt);
    // Log the AI response
    logger.log(`AI: ${response}`);
    // Return the response
    return response.trim();
  } catch (error) {
    // Handle any errors that occur during the request
    logger.error(`Error asking Llama about conversation: ${error.message}`);
    throw error;
  }
}

/**
 * Asks Llama about the conversation category.
 *
 * @param {string[]} currentMessages - The current messages in the conversation.
 * @returns {Promise<string>} The category of the conversation.
 */
async function askLlamaAboutCategory(currentMessages) {
  // Import the LLM prompt function
  const { llmPrompt } = await importer.import('createLlmSession');

  // Prepare the prompt with the current messages
  const prompt = `Categorize this conversation in two or three words:
${currentMessages.join('\n')}
Only respond with the category.`;

  // Log the user message
  logger.log(`User: ${prompt}`);

  try {
    // Ask Llama for a response
    const response = await llmPrompt(prompt);
    // Log the AI response
    logger.log(`AI: ${response}`);

    // Extract the category from the response
    const category = response.trim().split(/\s*\n\s*|,\s*|\s*- |\s*\* /gi)[0];

    // Return the category
    return category;
  } catch (error) {
    // Handle any errors that occur during the request
    logger.error(`Error asking Llama about category: ${error.message}`);
    throw error;
  }
}

module.exports = {
  askLlamaAboutConversation,
  askLlamaAboutCategory,
};</code></pre></div><p><strong>Code Breakdown</strong></p>
<p>The provided code defines two asynchronous functions <code>askLlamaAboutConversation</code> and <code>askLlamaAboutCategory</code> that utilize a large language model (LLM) for text analysis and summarization. These functions are designed to work with a <code>create llm session</code> module.</p>
<h3>askLlamaAboutConversation Function</h3>
<p>This function:</p>
<ol>
<li>Imports the <code>create llm session</code> module using <code>importer</code>.</li>
<li>Creates a prompt (<code>q1</code>) that asks the LLM to summarize the conversation in two sentences, discarding any non-essential information.</li>
<li>Logs the user prompt to the console.</li>
<li>Sends the prompt to the LLM using the <code>llmPrompt</code> function and logs the AI response.</li>
<li>Returns the AI response after removing leading and trailing whitespace.</li>
</ol>
<h3>askLlamaAboutCategory Function</h3>
<p>This function:</p>
<ol>
<li>Imports the <code>create llm session</code> module using <code>importer</code>.</li>
<li>Creates a prompt (<code>q1</code>) that asks the LLM to categorize the conversation in two or three words.</li>
<li>Logs the user prompt to the console.</li>
<li>Sends the prompt to the LLM using the <code>llmPrompt</code> function and logs the AI response.</li>
<li>Returns the first word of the AI response after removing leading and trailing whitespace, as well as any non-word characters or whitespace between the category and the rest of the response.</li>
</ol>
<h3>Exported Module</h3>
<p>The code exports both functions as a module, allowing them to be used in other JavaScript files.</p>
<pre><code class="language-javascript">module.exports = {
  askLlamaAboutConversation,
  askLlamaAboutCategory
}
</code></pre>

</body>

</html>