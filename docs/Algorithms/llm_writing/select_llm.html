<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>select llm</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llm_writing/index.html">llm writing</a></h3>
    <a href="./business_plan_llm.html">business plan llm</a>
<br /><br />
<a href="./write_creatively_llm.html">write creatively llm</a>
<br /><br />
<a href="./argue_with_multiple_llms.html">argue with multiple llms</a>
<br /><br />
<a href="./research_paper_llm.html">research paper llm</a>
<br /><br />
<a href="./select_llm.html">select llm</a>
<br /><br />
<a href="./ask_llm_to_write_chapter_titles_and_descriptions.html">ask llm to write chapter titles and descriptions</a>
<br /><br />
<a href="./ask_llm_to_brainstorm.html">ask llm to brainstorm</a>
<br /><br />
<a href="./write_a_grant_proposal_with_llm.html">write a grant proposal with llm</a>
<br /><br />
<a href="./decode_xlsx_spreadsheet.html">decode xlsx spreadsheet</a>
<br /><br />
<a href="./elaborate_list_with_llm.html">elaborate list with llm</a>
<br /><br />

  </nav>
  <header>
    <a href="../llm_writing/index.html">llm writing</a> | <a href="./research_paper_llm.html">research paper llm</a> | <a href="./ask_llm_to_write_chapter_titles_and_descriptions.html">ask llm to write chapter titles and descriptions</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The GGUF SPECIFICATIONS object contains a list of model names and their corresponding specifications, while the GGUF_INSTRUCTIONS object contains a list of model names and their corresponding instructions or behaviors. The instructions for specific models, such as 'Code', provide templates for the response, while others are set to 'void 0' indicating no specific instruction is defined.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "select llm"</code></pre><h1>select llm</h1>



<pre class="javascript"><code>
const GGUF_SPECIFICATIONS = {
  'Meta': 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf',
  'Default': 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf',
  'DeepSeek': 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf',
  'Llama': 'Meta-Llama-3-70B-Instruct-Q5_K_M.gguf',
  'Qwen': 'Qwen2.5-14B-Instruct-1M-Q6_K_L.gguf',
  'Code': 'DeepSeek-R1-Distill-Llama-70B-Q5_K_S.gguf',
  'Mistral': 'Mistral-Small-24B-Instruct-2501-Q6_K_L.gguf',
  'Mesh': 'LLaMA-Mesh-Q6_K_L.gguf',
  'Meta-Llama-3.1-8B-Instruct-Q6_K_L': 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf',
  'DeepSeek-R1-Distill-Llama-8B-Q6_K': 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf',
  'Meta-Llama-3-70B-Instruct-Q5_K_M': 'Meta-Llama-3-70B-Instruct-Q5_K_M.gguf',
  'Qwen2.5-14B-Instruct-1M-Q6_K_L': 'Qwen2.5-14B-Instruct-1M-Q6_K_L.gguf',
  'DeepSeek-R1-Distill-Llama-70B-Q5_K_S': 'DeepSeek-R1-Distill-Llama-70B-Q5_K_S.gguf',
  'Mistral-Small-24B-Instruct-2501-Q6_K_L': 'Mistral-Small-24B-Instruct-2501-Q6_K_L.gguf',
  'LLaMA-Mesh-Q6_K_L': 'LLaMA-Mesh-Q6_K_L.gguf'
}


const GGUF_INSTRUCTIONS = {
  'Meta': void 0,
  'Default': void 0,
  'DeepSeek': void 0,
  'Llama':  void 0,
  'Qwen':  void 0,
  'Code': '```markdown\n# You are a\n##helpful coding module\nnamed ' + (process.env.MODEL_NAME || 'Llama') + ' that responds to every request using beautifully crafted `markdown`. Return only a single code block in the specified language without reasoning or instructions if requested.\n&lt;/think&gt;\n...```\n',
  'Mistral':  void 0,
  'Mesh': 'You are an LLM that responds with high quality detailed OBJ format models',
  'Meta-Llama-3.1-8B-Instruct-Q6_K_L':  void 0,
  'DeepSeek-R1-Distill-Llama-8B-Q6_K':  void 0,
  'Meta-Llama-3-70B-Instruct-Q5_K_M': void 0,
  'Qwen2.5-14B-Instruct-1M-Q6_K_L':  void 0,
  'DeepSeek-R1-Distill-Llama-70B-Q5_K_S': '```markdown\n# You are a\n##helpful coding module\nnamed ' + (process.env.MODEL_NAME || 'Llama') + ' that responds to every request using beautifully crafted `markdown`. Return only a single code block in the specified language without reasoning or instructions if requested.\n&lt;/think&gt;\n...```\n',
  'Mistral-Small-24B-Instruct-2501-Q6_K_L':  void 0,
  'LLaMA-Mesh-Q6_K_L': 'You are an LLM that responds with high quality detailed OBJ format models'
}

async function promptModel(temperature, initialChatHistory, session, prompt) {
  let result = await session.prompt(prompt, {
    temperature: temperature,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  session.setChatHistory(initialChatHistory);
  return result
}

// TODO: call llm rpc service with selectModel?
const modelSessions = {}
const modelInit = {}

async function selectModel(model, temperature) {
  const {createSession} = await importer.import('llama vision')

  if(typeof model == 'string') {
    let modelNames = Object.keys(GGUF_SPECIFICATIONS)
    for(let i = 0; i &lt; modelNames.length; i++) {
      if(modelNames[i].match(model)) {
        let gguf = GGUF_SPECIFICATIONS[modelNames[i]]
        if(!modelSessions[modelNames[i]]) {
          modelSessions[modelNames[i]] = await createSession(gguf, GGUF_INSTRUCTIONS[modelNames[i]])
          modelInit[modelNames[i]] = modelSessions[modelNames[i]].getChatHistory()
        }
        return promptModel.bind(null, temperature, modelInit[modelNames[i]], modelSessions[modelNames[i]])
      }
    }
  }

  throw new Error('Model not found: ' + model)
}

module.exports = selectModel
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>

<script>hljs.highlightAll();</script>

<p><strong>GGUF SPECIFICATIONS AND INSTRUCTIONS</strong></p>
<p>This code defines two objects: <code>GGUF_SPECIFICATIONS</code> and <code>GGUF_INSTRUCTIONS</code>.</p>
<h3>GGUF_SPECIFICATIONS</h3>
<p>This object contains a list of model specifications, where each key is a model name and the corresponding value is a string representing the model's specification.</p>
<pre><code class="language-markdown">const GGUF_SPECIFICATIONS = {
  'Meta': 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf',
  //...
}
</code></pre>
<h3>GGUF_INSTRUCTIONS</h3>
<p>This object contains a list of model instructions, where each key is a model name and the corresponding value is a string representing the model's behavior or response.</p>
<pre><code class="language-markdown">const GGUF_INSTRUCTIONS = {
  'Meta': void 0, // no instruction specified for 'Meta'
  'Default': void 0, // no instruction specified for 'Default'
  'Code': '```markdown\n#...```', // specific instruction for 'Code'
  //...
}
</code></pre>
<p><strong>NOTABLE POINTS</strong></p>
<ul>
<li>The <code>process.env.MODEL_NAME</code> variable is used to retrieve the model name from the environment variables, and defaults to <code>'Llama'</code> if not found.</li>
<li>The instructions for 'Code' and 'DeepSeek-R1-Distill-Llama-70B-Q5_K_S' specify the use of Markdown and provide a template for the response.</li>
<li>The other model instructions are set to <code>void 0</code>, indicating that no specific instruction is defined for those models.</li>
</ul>

</body>

</html>