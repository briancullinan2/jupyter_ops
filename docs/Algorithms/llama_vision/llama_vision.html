<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>llama vision</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./cell_6.html">Cell 6</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> |  | <a href="./analyze_image_with_llm.html">analyze image with llm</a> | <a href="../../search.html">Search</a>
  </header>

  <p>This JavaScript code implements a large language model using the <code>node-llama-cpp</code> library, providing functions to initialize and interact with the model. It includes functions for initializing chat sessions, loading LLM models, and analyzing prompts, with optional error handling.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "llama vision"</code></pre><h1>llama vision</h1>



<pre class="javascript"><code>
import path from "path"
import {getLlama, LlamaChatSession} from "node-llama-cpp"
import process from "process"

const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE
//const __dirname = path.dirname(fileURLToPath(import.meta.url));

let llama
let model
let context
let session
let initialChatHistory

const DEFAULT_PROMPT = '```markdown\n# You are a\n##large language model\nnamed ' + (process.env.MODEL_NAME || 'Llama') + ' that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\n&lt;/think&gt;\n...```\n'

const DEFAULT_MODEL = process.env.DEFAULT_GGUF || 'Qwen2-VL-7B-Instruct-Q6_K.gguf'


async function initSession(prompt, context2) {
  if(!context2) {
    context = await model.createContext()
  } else {
    context = context2
  }

  session = new LlamaChatSession({
      contextSequence: context.getSequence(),
      systemPrompt: prompt ? prompt : DEFAULT_PROMPT
  })
  // initialize the model
  //console.log(await session.prompt())
  initialChatHistory = session.getChatHistory();

  // Reset the chat history
  session.setChatHistory(initialChatHistory);
  return session
}

async function createSession(modelName, prompt) {
  if(!llama) {
    llama = await getLlama();
  }
  if(!model) {
      model = await llama.loadModel({
          modelPath: path.join(HOMEPATH, "llama.cpp", "models", modelName ? modelName : DEFAULT_MODEL ),
          //contextSize: 2048
      });
  }

  await initSession(prompt, await model.createContext())

  return session
}

async function getSession(model, prompt) {
  if(!session) {
    await createSession(model, prompt)
  }
  return session
}

async function llmAnalyze(prompt, session2) {
  if(!session2) {
    session2 = await getSession()
  }
  let result = await session2.prompt(prompt, {
    //maxTokens: context.contextSize,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  if(session == session2)
    session2.setChatHistory(initialChatHistory);
  return result
}

export default {
  llmAnalyze,
  createSession,
  getSession,
}

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>import { createRequire } from'module';
import path from 'path';
import { getLlama, LlamaChatSession } from 'node-llama-cpp';

const require = createRequire(import.meta.url);
const process = require('process');

const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE;
const __dirname = path.dirname(import.meta.url);

let llama;
let model;
let context;
let session;
let initialChatHistory;

const DEFAULT_MODEL = process.env.DEFAULT_GGUF || 'Qwen2-VL-7B-Instruct-Q6_K.gguf';
const DEFAULT_PROMPT = (text) => ````markdown\n# You are a\n##large language model\nnamed ${process.env.MODEL_NAME || 'Llama'} that provides clear and concise answers in beautifully crafted ` + (text? `markdown unless otherwise instructed.\n</think>\n${text}\n` :'markdown\n</think>\n...```\n');

class LLM {
  async createSession(modelName, prompt) {
    if (!llama) {
      llama = await getLlama();
    }
    if (!model) {
      model = await llama.loadModel({
        modelPath: path.join(HOMEPATH, "llama.cpp", "models", modelName? modelName : DEFAULT_MODEL),
        //contextSize: 2048
      });
    }

    const context2 = await model.createContext();
    const session = new LlamaChatSession({
      contextSequence: context2.getSequence(),
      systemPrompt: prompt? prompt : DEFAULT_PROMPT(''),
    });

    initialChatHistory = session.getChatHistory();
    session.setChatHistory(initialChatHistory);

    this.model = model;
    this.context = context2;
    this.session = session;

    return session;
  }

  async getSession(session) {
    if (!session) {
      await this.createSession();
    }
    return this.session;
  }

  async llmAnalyze(prompt, session2) {
    if (!session2) {
      session2 = await this.getSession();
    }

    let result = await session2.prompt(prompt, {
      //maxTokens: context.contextSize,
      onTextChunk: (text) => process.stdout.write(text),
    });

    if (session2 === this.session) {
      session2.setChatHistory(initialChatHistory);
    }

    return result;
  }

  async llmAnalyzeWithSession(prompt, session) {
    try {
      return await session.prompt(prompt, {
        //maxTokens: context.contextSize,
        onTextChunk: (text) => process.stdout.write(text),
      });
    } catch (err) {
      console.error('Error analyzing prompt:', err);
    }
  }
}

const llm = new LLM();
export default {
  llm: llm,
  createSession: llm.createSession.bind(llm),
  getSession: llm.getSession.bind(llm),
  llmAnalyze: llm.llmAnalyze.bind(llm),
  llmAnalyzeWithSession: llm.llmAnalyzeWithSession.bind(llm),
};</code></pre></div><p><strong>Breakdown of the Code</strong></p>
<p>This JavaScript code is for a large language model (LLM) implementation using the <code>node-llama-cpp</code> library. It establishes a conversation interface with the LLM and provides functions to initialize and interact with the model.</p>
<p><strong>Key Components:</strong></p>
<ol>
<li><strong>Environment Variables:</strong>
<ul>
<li><code>HOMEPATH</code>: resolves to the user's home directory</li>
<li><code>MODEL_NAME</code>: optional model name for the LLM</li>
<li><code>DEFAULT_GGUF</code>: default model name for the LLM (Qwen2-VL-7B-Instruct-Q6_K.gguf)</li>
</ul></li>
<li><strong>Constants:</strong>
<ul>
<li><code>DEFAULT_PROMPT</code>: default prompt for the LLM</li>
<li><code>DEFAULT_MODEL</code>: default model name for the LLM</li>
</ul></li>
<li><strong>Functions:</strong>
<ul>
<li><code>initSession(prompt, context2)</code>: initializes a new chat session with a given prompt and context (or generates a new context if none is provided)</li>
<li><code>createSession(modelName, prompt)</code>: loads the LLM model and initializes a new chat session with a given model name and prompt</li>
<li><code>getSession(model, prompt)</code>: gets an existing session or creates a new one if none is available</li>
<li><code>llmAnalyze(prompt, session2)</code>: interacts with the LLM to analyze a given prompt and prints the response to the console</li>
</ul></li>
</ol>
<p><strong>Notes:</strong></p>
<ul>
<li>The code uses the <code>node-llama-cpp</code> library to interact with the LLM.</li>
<li>The <code>createSession</code> function loads the LLM model and initializes a new chat session, while the <code>getSession</code> function gets an existing session or creates a new one if none is available.</li>
<li>The <code>llmAnalyze</code> function interacts with the LLM to analyze a given prompt and prints the response to the console.</li>
<li>The code does not handle errors or exceptions explicitly.</li>
</ul>

</body>

</html>