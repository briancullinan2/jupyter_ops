<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>ollama vision request</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./cell_6.html">Cell 6</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./cell_6.html">Cell 6</a> | <a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a> | <a href="../../search.html">Search</a>
  </header>

  <p>Makes a request to the LLaMA Vision API with an optional image and prompt, returning the response message from the API. The function uses async/await syntax and assumes the LLaMA Vision API is running on <code>http://localhost:11434/api/chat</code>.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "ollama vision request"</code></pre><h1>ollama vision request</h1>



<pre class="javascript"><code>const { request } = require('gaxios')
const fs = require('fs')

async function requestOllamaVision(image, prompt) {
  if (!image) {
    console.error('image not set!')
    return
  }

  let base64_image
  if(typeof image == 'string') {
    if(image.startsWith('data:image/'))
      image = image.replace(/^data:image\/.*?;base64,/gi, '')
  
    if(image.includes('://')) {
      let result = await request({
        url: image,
        method: 'GET',
      })
      base64_image = Buffer.from(await result.data.arrayBuffer()).toString('base64')
    } else if (!fs.existsSync(image)) {
      base64_image = Buffer.from(image, 'base64').toString('base64')
    } else {
      base64_image = fs.readFileSync(image).toString('base64')
    }  
  } else {
    base64_image = image.toString('base64')
  }

  let result = await request({
    url: 'http://localhost:11434/api/chat',
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    data: JSON.stringify({
      "model": "llama3.2-vision",
      "stream": false,
      "messages": [
        {
          "role": "user",
          "content": prompt ? prompt : "Describe the image in great detail.",
          //"content": (
          //    "Extract all text from the image and return it as markdown.\n"
          //    "Do not describe the image or add extra text.\n"
          //    "Only return the text found in the image."
          //),
          "images": [base64_image]
        }
      ]
    })
  })
  //let buff = Buffer.from(result.data.images[0], 'base64');
  if(result.data &amp;&amp; result.data.message)
    return result.data.message.content
  else
    return
}

module.exports = requestOllamaVision

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>const { google } = require('googleapis');
const fs = require('fs');
const fetch = require('isomorphic-fetch');

/**
 * Makes a request to the LLaMA Vision API.
 * 
 * @param {Buffer|string} image - The image to be processed.
 * @param {string} prompt - The prompt for the image description.
 * @returns {Promise<string>} The response from the API.
 */
async function requestLlamaVision(image, prompt = 'Describe the image in great detail.') {
  // Check if the image is valid
  if (!image) {
    console.error('Image not set!');
    return;
  }

  let base64Image;
  if (typeof image ==='string') {
    try {
      // Try to parse the image as a base64 string
      if (image.startsWith('data:image/')) {
        base64Image = image.replace(/^data:image\/.*?;base64,/gi, '');
      } else if (image.includes('://')) {
        // If it's a URL, fetch the image
        const response = await fetch(image);
        base64Image = await response.arrayBuffer().then(buffer => Buffer.from(buffer).toString('base64'));
      } else {
        // If it's a local file, read it
        base64Image = fs.readFileSync(image).toString('base64');
      }
    } catch (error) {
      // If any of the above steps fail, return an error
      console.error('Error processing image:', error);
      return;
    }
  } else {
    // If it's a Buffer, convert it to base64
    base64Image = image.toString('base64');
  }

  try {
    // Make the request to the LLaMA Vision API
    const apiResponse = await google.chat('v1').messages.list({
      'parent': 'projects/-/locations/-/agents/-',
      'body': {
       'model': 'llama3.2-vision',
       'stream': false,
       'messages': [
          {
            'role': 'user',
            'content': prompt,
            'images': [base64Image]
          }
        ]
      }
    });
    const result = apiResponse.data.messages;
    return result[0].text;
  } catch (error) {
    // If the API request fails, return an error
    console.error('Error requesting LLaMA Vision API:', error);
    return;
  }
}

module.exports = requestLlamaVision;</code></pre></div><h2>Function: <code>requestOllamaVision</code></h2>
<h3>Description</h3>
<p>Makes a request to the LLaMA Vision API with an optional image and prompt.</p>
<h3>Parameters</h3>
<ul>
<li><code>image</code>: The image to be processed. Can be a:
<ul>
<li><code>Buffer</code> object</li>
<li><code>string</code> representing a base64 encoded image</li>
<li><code>string</code> representing a URL to an image (will be downloaded and processed)</li>
<li><code>string</code> representing a local file path to an image (will be read and processed)</li>
</ul></li>
<li><code>prompt</code>: The prompt to be sent to the LLaMA Vision API. Defaults to &quot;Describe the image in great detail.&quot;</li>
</ul>
<h3>Returns</h3>
<ul>
<li><code>string</code>: The response message from the LLaMA Vision API.</li>
</ul>
<h3>Throws</h3>
<ul>
<li><code>Error</code>: If the image is not set or cannot be processed.</li>
</ul>
<h3>Dependencies</h3>
<ul>
<li><code>gaxios</code> for making HTTP requests</li>
<li><code>fs</code> for reading local files</li>
</ul>
<h3>Notes</h3>
<ul>
<li>The function uses the <code>async/await</code> syntax to handle promises.</li>
<li>The <code>request</code> function from <code>gaxios</code> is used to make HTTP requests.</li>
<li>The function assumes that the LLaMA Vision API is running on <code>http://localhost:11434/api/chat</code>.</li>
<li>The function returns the response message from the LLaMA Vision API, or an empty string if the response is invalid.</li>
</ul>

</body>

</html>