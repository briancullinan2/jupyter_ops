<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>Cell 3</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./cell_6.html">Cell 6</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./llm_deceive.html">llm deceive</a> | <a href="./cell_4.html">Cell 4</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>llmVoice</code> function generates text-to-speech output using the LLaMA model, taking a prompt and optional session object as parameters. It sends the prompt to the model, returning the generated result and resetting the chat history if the provided session is the same as the current session.</p>
<h1>Cell 3</h1>



<pre class="javascript"><code>
async function llmVoice(prompt, session2) {
  if(!session2) {
    const {createSession} = await importer.import("<a href="../../Algorithms/llama_vision/llama_vision.html">llama vision</a>")
    session2 = await createSession('OuteTTS-0.3-1B-Q8_0.gguf', 'you are an llm that responds with medium quality text to voice\n')
  }
  let result = await session2.prompt(prompt, {
    //maxTokens: context.contextSize,
    temperature: 0.1,
    repetition_penalty: 1.1,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  if(session == session2)
    session2.setChatHistory(initialChatHistory);
  return result
}

module.exports = llmVoice

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>/**
 * Generate voice output from a given prompt using LLaMA.
 * 
 * @param {string} prompt The input text to generate voice output for.
 * @param {object} session2 The session object, optional.
 * @returns {Promise&lt;string&gt;} The generated voice output.
 */
async function llmVoice(prompt, session2) {
  // Check if a session object is provided, otherwise create a new one
  if (!session2) {
    const { createSession } = await import('llama-vision');
    // Comment: Importing the library using ES6 import syntax
    session2 = await createSession(
      'OuteTTS-0.3-1B-Q8_0.gguf', 
      'You are an LLM that responds with medium quality text to voice.\n'
    );
    // Comment: Store the session2 object to avoid circular dependency
    this.session2 = session2;
  }

  // Set the chat history for the session
  const initialChatHistory = 'Initial conversation history.';
  session2.setChatHistory(initialChatHistory);

  // Prompt LLaMA to generate voice output
  try {
    const result = await session2.prompt(prompt, {
      // Temperature value between 0 (very conservative) and 1 (very sampling-heavy)
      temperature: 0.1,
      // A value used to penalize repeated tokens and encourage greater variety
      repetition_penalty: 1.1,
      // Function called for each chunk of the output text
      onTextChunk: (text) =&gt; {
        // Write the chunk of text to the standard output
        process.stdout.write(text);
      }
    });
    return result;
  } catch (error) {
    // Log any errors that occur during the prompt
    console.error('Error generating voice output:', error);
    throw error;
  }
}

module.exports = llmVoice;</code></pre></div><h2><code>llmVoice</code> Function</h2>
<h3>Overview</h3>
<p>An asynchronous function <code>llmVoice</code> that generates text-to-speech output using the LLaMA model.</p>
<h3>Function Parameters</h3>
<ul>
<li><code>prompt</code>: The input text prompt to generate speech from.</li>
<li><code>session2</code>: An optional session object. If not provided, a new session will be created.</li>
</ul>
<h3>Function Behavior</h3>
<ol>
<li>If <code>session2</code> is not provided, a new session is created using the <code>importer.import('llama vision')</code> module.</li>
<li>The function sends the <code>prompt</code> to the LLaMA model using the <code>session2.prompt</code> method with the following parameters:
<ul>
<li><code>temperature</code>: Set to 0.1.</li>
<li><code>repetition_penalty</code>: Set to 1.1.</li>
<li><code>onTextChunk</code>: A callback function that writes the generated text to the standard output.</li>
</ul></li>
<li>The generated result is then returned.</li>
<li>If the provided <code>session2</code> is the same as the current session, the chat history is reset.</li>
</ol>
<h3>Export</h3>
<p>The <code>llmVoice</code> function is exported as a module.</p>

</body>

</html>