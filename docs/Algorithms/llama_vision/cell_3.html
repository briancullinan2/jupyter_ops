<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>Cell 3</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./llm_deceive.html">llm deceive</a> | <a href="./cell_4.html">Cell 4</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>llmVoice</code> function generates voice output from a given text prompt using LLaMA, utilizing an optional pre-existing session or creating a new one if necessary. It exports the function as a module, allowing for use in other JavaScript files, and returns the resulting voice output.</p>
<h1>Cell 3</h1>



<pre class="javascript"><code>
async function llmVoice(prompt, session2) {
  if(!session2) {
    const {createSession} = await importer.import('llama vision')
    session2 = await createSession('OuteTTS-0.3-1B-Q8_0.gguf', 'you are an llm that responds with medium quality text to voice\n')
  }
  let result = await session2.prompt(prompt, {
    //maxTokens: context.contextSize,
    temperature: 0.1,
    repetition_penalty: 1.1,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  if(session == session2)
    session2.setChatHistory(initialChatHistory);
  return result
}

module.exports = llmVoice

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>

<script>hljs.highlightAll();</script>

<h3>Function Description</h3>
<p><code>llmVoice</code> is an asynchronous function that generates voice output from a given text prompt using LLaMA.</p>
<h3>Parameters</h3>
<ul>
<li><code>prompt</code>: The text to be converted to voice output.</li>
<li><code>session2</code>: The LLaMA session to use for voice generation. If <code>session2</code> is not provided, a new session will be created.</li>
</ul>
<h3>Function Flow</h3>
<ol>
<li>If <code>session2</code> is not provided, a new LLaMA session is created using the <code>createSession</code> function from the <code>llama vision</code> module.</li>
<li>The <code>prompt</code> is sent to the LLaMA session using the <code>prompt</code> method.</li>
<li>The <code>temperature</code>, <code>repetition_penalty</code>, and <code>onTextChunk</code> options are passed to the <code>prompt</code> method to customize the voice output.</li>
<li>If the current session is the same as the created session, the chat history is reset using the <code>setChatHistory</code> method.</li>
<li>The resulting voice output is returned.</li>
</ol>
<h3>Module Export</h3>
<p>The <code>llmVoice</code> function is exported as a module, making it available for use in other JavaScript files.</p>

</body>

</html>