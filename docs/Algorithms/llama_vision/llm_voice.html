<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>llm voice</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./cell_4.html">Cell 4</a> | <a href="./ollama_vision_request.html">ollama vision request</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The code defines a function <code>llmSpeech</code> that generates speech from a given <code>prompt</code> using the <code>outetts</code> library, saving the output to a file named &quot;output.wav&quot;. The function is exported as a module attribute, allowing for external access, and takes a single parameter <code>prompt</code> with no return value.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "llm voice"</code></pre><h1>llm voice</h1>



<pre class="python"><code>import outetts

# Configure the model
model_config = outetts.HFModelConfig_v2(
    model_path="OuteAI/OuteTTS-0.3-1B",
    tokenizer_path="OuteAI/OuteTTS-0.3-1B"
)

globals()["interface"] = None

def llmSpeech(prompt):

  if globals()["interface"] is None:
    # Initialize the interface
    interface = outetts.InterfaceHF(model_version="0.3", cfg=model_config)

    # You can create a speaker profile for voice cloning, which is compatible across all backends.
    # speaker = interface.create_speaker(audio_path="path/to/audio/file.wav")
    # interface.save_speaker(speaker, "speaker.json")
    # speaker = interface.load_speaker("speaker.json")

    # Print available default speakers
    interface.print_default_speakers()
    # Load a default speaker
    speaker = interface.load_default_speaker(name="en_male_1")

    globals()["interface"] = interface
    globals()["speaker"] = speaker
  else:
    interface = globals()["interface"]
    speaker = globals()["speaker"]

  # Generate speech
  gen_cfg = outetts.GenerationConfig(
      text=prompt,
      temperature=0.3,
      repetition_penalty=1.1,
      max_length=4096,
      speaker=speaker,
  )
  output = interface.generate(config=gen_cfg)

  # Save the generated speech to a file
  output.save("output.wav")

__all__ = {
  "llmSpeech": llmSpeech
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>

<script>hljs.highlightAll();</script>

<p><strong>Code Breakdown</strong></p>
<h3>Import and Configuration</h3>
<ul>
<li>The code imports the <code>outetts</code> library.</li>
<li>A model configuration object is created using <code>outetts.HFModelConfig_v2</code>.</li>
<li>The <code>interface</code> variable is set to <code>None</code> in the global scope.</li>
</ul>
<h3>llmSpeech Function</h3>
<ul>
<li>The <code>llmSpeech</code> function takes a <code>prompt</code> as input and generates speech from it.</li>
<li>If the <code>interface</code> variable is <code>None</code>, the interface is initialized using <code>outetts.InterfaceHF</code>.</li>
<li>A speaker profile is loaded from a default speaker.</li>
<li>The interface and speaker are stored in the global scope.</li>
<li>If the <code>interface</code> variable is not <code>None</code>, it is retrieved from the global scope.</li>
<li>Speech is generated using <code>outetts.GenerationConfig</code> and the <code>interface.generate</code> method.</li>
<li>The generated speech is saved to a file named &quot;output.wav&quot;.</li>
</ul>
<h3>Export</h3>
<ul>
<li>The <code>llmSpeech</code> function is exported as a module attribute (<code>__all__</code>) for external access.</li>
</ul>
<h3>Unexported Code</h3>
<ul>
<li>Commented-out code for voice cloning and speaker profile management is present, but not used.</li>
</ul>
<h3>Function Parameters and Return Values</h3>
<ul>
<li><code>llmSpeech</code> takes a single parameter <code>prompt</code>.</li>
<li>It returns <code>None</code>, as the generated speech is saved to a file rather than returned as output.</li>
</ul>

</body>

</html>