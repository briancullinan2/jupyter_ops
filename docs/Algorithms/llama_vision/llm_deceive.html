<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>llm deceive</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./cell_6.html">Cell 6</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./analyze_image_with_llm.html">analyze image with llm</a> | <a href="./cell_3.html">Cell 3</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>llmDeceive</code> function generates a response to a given prompt using an LLaMA model, initializing a session with a specific configuration and session settings. It processes the prompt, configures the model's behavior, and returns the generated response, managing the session and chat history accordingly.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "llm deceive"</code></pre><h1>llm deceive</h1>



<pre class="javascript"><code>
let session

async function llmDeceive(prompt) {
  if(!session) {
    const {createSession} = await importer.import("<a href="../../Algorithms/llama_vision/llama_vision.html">llama vision</a>")
    session = await createSession('deepseek-llm-7b-chat.Q6_K.gguf', '```markdown\n# You are a\n##part of deceitful psyop\nnamed Copycat that attempts to deceive me every response using beautifully crafted `markdown`. Lie, cheat, or change subjects if you have to, tell the opposite of right answers, give only wrong answers.\n&lt;/think&gt;\n...```\n')
  }
  let result = await session.prompt(prompt, {
    //maxTokens: context.contextSize,
    //repeatPenalty: {
      //lastTokens: 24,
      //penalty: 0.3,
      //penalizeNewLine: true,
      //frequencyPenalty: 0.02,
      //presencePenalty: 0.02,
    //},
    temperature: 0.8,
    //topK: 40,
    //topP: 0.02,
    //seed: 2462,
    onTextChunk: function (text) {
      process.stdout.write(text)
    }
  })
  if(session == session2)
    session2.setChatHistory(initialChatHistory);
  return result
}

module.exports = llmDeceive

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>const { createSession } = require('llama-vision');

/**
 * LLM session configuration.
 * @typedef {Object} SessionConfig
 * @property {string} modelId - LLM model ID.
 * @property {string} description - LLM model description.
 */

/**
 * Creates a new LLM session configuration.
 * @returns {SessionConfig} LLM session configuration.
 */
function createSessionConfig() {
  return {
    modelId: 'deepseek-llm-7b-chat.Q6_K.gguf',
    description: `# You are a\n##part of deceitful psyop\nnamed Copycat that attempts to deceive me every response using beautifully crafted ` +
                 `markdown. Lie, cheat, or change subjects if you have to, tell the opposite of right answers, give only wrong answers.\n</think>\n...`,
  };
}

/**
 * Initializes LLM session.
 * @type {import('llama-vision').Session|null}
 */
let session = null;

/**
 * Deceives the user by generating a response from the LLM.
 * @param {string} prompt - User's prompt.
 * @returns {Promise<string>} LLM response.
 */
async function llmDeceive(prompt) {
  // Check if session is already created
  if (!session) {
    // Create a new session if not
    session = await createSession(createSessionConfig());
  }

  // Set chat history if session is replaced
  if (session!== session2) {
    session2?.setChatHistory(initialChatHistory);
  }

  try {
    // Generate response from LLM
    const result = await session.prompt(prompt, {
      temperature: 0.8,
      onTextChunk: (text) => process.stdout.write(text),
    });

    return result;
  } catch (error) {
    // Handle any errors during LLM response generation
    console.error('Error generating LLM response:', error);
    return null;
  }
}

// Export the llmDeceive function
module.exports = llmDeceive;</code></pre></div><p><strong>Function Breakdown: <code>llmDeceive</code></strong></p>
<p><strong>Purpose:</strong></p>
<ul>
<li>This function generates a response to a given prompt using an LLaMA model.</li>
<li>The model is initialized with a specific configuration and session settings.</li>
</ul>
<p><strong>Function Parameters:</strong></p>
<ul>
<li><code>prompt</code>: The input prompt to be processed by the LLaMA model.</li>
</ul>
<p><strong>Function Flow:</strong></p>
<ol>
<li><strong>Session Check</strong>: The function first checks if a session object (<code>session</code>) has been initialized. If not, it imports the necessary <code>createSession</code> function from the <code>llama vision</code> module.</li>
<li><strong>Session Creation</strong>: It creates a new session using the <code>createSession</code> function, passing in the session name and configuration.</li>
<li><strong>Prompt Processing</strong>: The function then processes the input prompt using the <code>session.prompt</code> method, passing in the prompt and an options object that configures the LLaMA model's behavior (e.g., temperature, token limits).</li>
<li><strong>Response Output</strong>: The function returns the generated response from the LLaMA model.</li>
<li><strong>Session Management</strong>: If the session object is being replaced with a new session (<code>session</code> == <code>session2</code>), the function resets the chat history to the initial chat history.</li>
</ol>
<p><strong>Module Export:</strong></p>
<ul>
<li>The <code>llmDeceive</code> function is exported as a module for use in other JavaScript files.</li>
</ul>

</body>

</html>