<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>Cell 6</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../llama_vision/index.html">llama vision</a></h3>
    <a href="./llama_vision.html">llama vision</a>
<br /><br />
<a href="./analyze_image_with_llm.html">analyze image with llm</a>
<br /><br />
<a href="./llm_deceive.html">llm deceive</a>
<br /><br />
<a href="./cell_3.html">Cell 3</a>
<br /><br />
<a href="./cell_4.html">Cell 4</a>
<br /><br />
<a href="./llm_voice.html">llm voice</a>
<br /><br />
<a href="./cell_6.html">Cell 6</a>
<br /><br />
<a href="./ollama_vision_request.html">ollama vision request</a>
<br /><br />
<a href="./start_a_bunch_of_llm_rpc_services.html">start a bunch of llm rpc services</a>
<br /><br />
<a href="./stable_diffusion_request.html">stable diffusion request</a>
<br /><br />
<a href="./mask_image.html">mask image</a>
<br /><br />
<a href="./inpaint_mask.html">inpaint mask</a>
<br /><br />
<a href="./image_2_image.html">image 2 image</a>
<br /><br />
<a href="./whisk_images.html">whisk images</a>
<br /><br />

  </nav>
  <header>
    <a href="../llama_vision/index.html">llama vision</a> | <a href="./llm_voice.html">llm voice</a> | <a href="./ollama_vision_request.html">ollama vision request</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The code consists of imported modules, functions, and notes for an asynchronous Python script that uses custom modules <code>browser_use</code> and <code>langchain_ollama</code> to execute a search task on the r/LocalLLaMA subreddit. The script defines two asynchronous functions: <code>run_search()</code> and <code>main()</code>.</p>
<h1>Cell 6</h1>



<pre class="python"><code>
import asyncio
from browser_use import Agent
from browser_use.agent.views import AgentHistoryList
from langchain_ollama import ChatOllama


async def run_search() -&gt; AgentHistoryList:
    agent = Agent(
        task="Search for a 'browser use' post on the r/LocalLLaMA subreddit and open it.",
        llm=ChatOllama(
            # model="qwen2.5:32b-instruct-q4_K_M",
            # model="deepseek-r1:14b",
            model="qwen2.5:latest",
            num_ctx=32000,
        ),
    )

    result = await agent.run()
    return result


async def main():
    result = await run_search()
    print("\n\n", result)

__all__ = {
  "main": main,
  "run_search": run_search
}
</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="python"><code># Import necessary libraries
import asyncio
from browser_use import Agent
from browser_use.agent.views import AgentHistoryList
from langchain_ollama import ChatOllama

# Define constants for LLama model and context
DEFAULT_LLM_MODEL = "qwen2.5:latest"
DEFAULT_NUM_CTX = 32000

class LlamaAgent:
    """An agent using LLama to perform tasks."""
    
    def __init__(self, model=DEFAULT_LLM_MODEL, num_ctx=DEFAULT_NUM_CTX):
        """Initialize the LLama agent.

        Args:
            model (str): The name of the LLama model to use. Defaults to DEFAULT_LLM_MODEL.
            num_ctx (int): The number of context to use. Defaults to DEFAULT_NUM_CTX.
        """
        self.agent = Agent(
            task="Search for a 'browser use' post on the r/LocalLLaMA subreddit and open it.",
            llm=ChatOllama(model=model, num_ctx=num_ctx),
        )

    async def run_search(self) -> AgentHistoryList:
        """Run the search task using the LLama agent.

        Returns:
            AgentHistoryList: The result of the search task.
        """
        result = await self.agent.run()
        return result


async def main():
    """The main entry point of the script."""
    # Create a new LLama agent
    agent = LlamaAgent()

    # Run the search task
    result = await agent.run_search()

    # Print the result
    print("\n\n", result)


if __name__ == "__main__":
    # Run the main function
    asyncio.run(main())</code></pre></div><h2>Code Breakdown</h2>
<h3>Imported Modules</h3>
<ul>
<li><code>asyncio</code>: a built-in Python module for writing single-threaded concurrent code using coroutines, multiplexing I/O access over sockets and other resources, and implementing network clients and servers.</li>
<li><code>browser_use</code>: a custom module (not a built-in Python module) containing the <code>Agent</code> class and its related views.</li>
<li><code>langchain_ollama</code>: a custom module (not a built-in Python module) containing the <code>ChatOllama</code> class.</li>
</ul>
<h3>Functions</h3>
<h4><code>run_search()</code></h4>
<ul>
<li>An asynchronous function that returns an instance of <code>AgentHistoryList</code>.</li>
<li>It creates an instance of the <code>Agent</code> class with a task to search for a 'browser use' post on the r/LocalLLaMA subreddit and open it.</li>
<li>The agent uses a chat Ollama model with a specified model name (<code>qwen2.5:latest</code>) and a large context buffer (<code>num_ctx=32000</code>).</li>
<li>It executes the agent using the <code>run()</code> method and returns the result.</li>
</ul>
<h4><code>main()</code></h4>
<ul>
<li>An asynchronous function that calls <code>run_search()</code> and prints the result.</li>
</ul>
<h3>Exported Functions</h3>
<ul>
<li>The <code>__all__</code> variable is set to a dictionary containing the <code>main</code> and <code>run_search</code> functions, making them available for import from other modules.</li>
</ul>
<h3>Notes</h3>
<ul>
<li>The code uses asynchronous programming with the <code>asyncio</code> module.</li>
<li>The <code>langchain_ollama</code> and <code>browser_use</code> modules are not standard Python modules and are likely custom implementations.</li>
<li>The code defines two functions: <code>run_search()</code> and <code>main()</code>, which are exported for use in other modules.</li>
</ul>

</body>

</html>