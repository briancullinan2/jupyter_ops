<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>analyze cache file</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../data_collection/index.html">data collection</a></h3>
    <a href="./Crime_reports.html">Crime reports</a>
<br /><br />
<a href="./https___www_amazon_com_gp_yourstore_iyr_ref_pd_ys_iyr_nextie_UTF8_collection_watched_iyrGroup__maxIt.html">https://www.amazon.com/gp/yourstore/iyr/ref=pd_ys_iyr_nextie=UTF8&collection=watched&iyrGroup=&maxItem=616&minItem=600</a>
<br /><br />
<a href="./meta_search_all.html">meta search all</a>
<br /><br />
<a href="./schedule_search_all.html">schedule search all</a>
<br /><br />
<a href="./tell_joke.html">tell joke</a>
<br /><br />
<a href="./multi_crawl.html">multi crawl</a>
<br /><br />
<a href="./crawl_domain.html">crawl domain</a>
<br /><br />
<a href="./domain_cache_tools.html">domain cache tools</a>
<br /><br />
<a href="./browser_crawler_tools.html">browser crawler tools</a>
<br /><br />
<a href="./analyze_cache_file.html">analyze cache file</a>
<br /><br />
<a href="./schedule_crawl_domain.html">schedule crawl domain</a>
<br /><br />
<a href="./collect_all_bookmarks.html">collect all bookmarks</a>
<br /><br />
<a href="./search_results_as_json.html">search results as json</a>
<br /><br />

  </nav>
  <header>
    <a href="../data_collection/index.html">data collection</a> | <a href="./browser_crawler_tools.html">browser crawler tools</a> | <a href="./schedule_crawl_domain.html">schedule crawl domain</a> | <a href="../../search.html">Search</a>
  </header>

  <p>The <code>analyzeCache</code> function analyzes the cache file for a given URL, extracting statistics such as the number of cache objects, distinct domains, and repeated URLs. It returns an object with various statistics, including the count of pages, caches, and domains, as well as the URLs for the 10 largest objects and repeated URLs.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "analyze cache file"</code></pre><h1>analyze cache file</h1>



<pre class="javascript"><code>var {URL} = require('url')
var fs = require('fs')
var {findCache} = importer.import("<a href="../../Analytics/data_collection/domain_cache_tools.html">domain crawler tools</a>")

function analyzeCache(url) {
    var cache = findCache(url)
    if(cache.length === 0) {
        return {
            error: `No cache file found ${url}`
        }
    }
    
    var json = JSON.parse(fs.readFileSync(cache[0]))
    var domains = json.map(s =&gt; new URL(s.url).hostname)
        .filter((h, i, arr) =&gt; arr.indexOf(h) === i)
    var largeness = json.sort((a, b) =&gt; b.content.length - a.content.length)
        .slice(0, 10)
    var urls = json.map(s =&gt; s.url)
    var repeats = json.filter((s, i, arr) =&gt; i &gt; 0 &amp;&amp; urls.indexOf(s.url) === i)
    fs.writeFileSync(cache[0], JSON.stringify(repeats, null, 2))
    return {
        countPages: json.length,
        countCaches: cache.length,
        target: json[0].url,
        countDomains: domains.length,
        domains: domains,
        countLargest: largeness.reduce((cur, l) =&gt; cur + l.content.length, 0),
        largest10: largeness.map(l =&gt; l.url),
        repeats: json.length - repeats.length,
    }
}

module.exports = analyzeCache</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>```javascript
const { URL } = require('url');
const fs = require('fs');
const { findCache } = require('./domain-crawler-tools'); // import using a module resolver

/**
 * Analyze a cache file for a given URL.
 * 
 * @param {string} url - The URL to analyze.
 * @returns {object} An object containing analysis results.
 */
function analyzeCache(url) {
    const cache = findCache(url);

    if (cache.length === 0) {
        // TODO: Consider using a more specific error message or a custom error object.
        return { error: `No cache file found for ${url}` };
    }

    try {
        const json = JSON.parse(fs.readFileSync(cache[0]));
        const domains = Array.from(new Set(json.map(item =&gt; new URL(item.url).hostname)));
        const largeness = json.slice().sort((a, b) =&gt; b.content.length - a.content.length).slice(0, 10);
        const urls = json.map(item =&gt; item.url);
        const repeats = json.filter((item, index) =&gt; index &gt; 0 &amp;&amp; urls.indexOf(item.url) === index);

        // TODO: Consider adding error handling for write operations.
        fs.writeFileSync(cache[0], JSON.stringify(repeats, null, 2));

        return {
            countPages: json.length,
            countCaches: cache.length,
            target: json[0].url,
            countDomains: domains.length,
            domains,
            countLargest: largeness.reduce((sum, item) =&gt; sum + item.content.length, 0),
            largest10: largeness.map(item =&gt; item.url),
            repeats: json.length - repeats.length,
        };
    } catch (error) {
        // TODO: Consider adding error handling for JSON parsing or file reading errors.
        return { error: `Failed to analyze cache: ${error.message}` };
    }
}

module.exports = analyzeCache;
```</code></pre></div><p><strong>Code Breakdown</strong></p>
<h3>Dependencies</h3>
<ul>
<li><code>var {URL} = require('url')</code>: Imports the <code>URL</code> class from the built-in Node.js <code>url</code> module.</li>
<li><code>var fs = require('fs')</code>: Imports the file system module.</li>
<li><code>var {findCache} = importer.import('domain crawler tools')</code>: Imports the <code>findCache</code> function from an external module using the <code>importer</code> object.</li>
</ul>
<h3>Function <code>analyzeCache(url)</code></h3>
<h4>Purpose</h4>
<p>Analyzes the cache file for the given <code>url</code> and returns an object with various statistics.</p>
<h4>Steps</h4>
<ol>
<li>Finds the cache file for the given <code>url</code> using <code>findCache(url)</code>.</li>
<li>If no cache file is found, returns an object with an error message.</li>
<li>Reads the cache file as JSON and maps each object to its hostname using <code>new URL(s.url).hostname</code>.</li>
<li>Filters the unique hostnames to get the distinct domains.</li>
<li>Sorts the cache objects in descending order by content length and takes the first 10 largest objects.</li>
<li>Maps the JSON objects to their URLs.</li>
<li>Filters the cache objects to find repeated URLs and writes the result back to the cache file.</li>
<li>Returns an object with various statistics:
<ul>
<li><code>countPages</code>: The number of cache objects.</li>
<li><code>countCaches</code>: The number of cache files.</li>
<li><code>target</code>: The URL of the first cache object.</li>
<li><code>countDomains</code>: The number of distinct domains.</li>
<li><code>domains</code>: An array of distinct domains.</li>
<li><code>countLargest</code>: The total content length of the 10 largest objects.</li>
<li><code>largest10</code>: An array of URLs for the 10 largest objects.</li>
<li><code>repeats</code>: The number of unique URLs in the cache.</li>
</ul></li>
</ol>
<h3>Export</h3>
<p>The <code>analyzeCache</code> function is exported as a module.</p>

</body>

</html>