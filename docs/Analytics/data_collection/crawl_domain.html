<!DOCTYPE html>
<html>

<head>
  <meta charset='utf-8'>
  <meta http-equiv='X-UA-Compatible' content='IE=edge'>
  <title>crawl domain</title>
  <meta name='viewport' content='width=device-width, initial-scale=1'>
  <link rel='stylesheet' type='text/css' media='screen' href='main.css'>
  <style>
    html {
      padding: 0;
      margin: 0;
    }

    nav {
      position: fixed;
      overflow: auto;
      top: 0;
      left: 0;
      right: auto;
      bottom: 0;
      width: 200px;
    }

    header {
      background-color: #EEE;
      padding: 10px;
    }

    body {
      padding: 0 0 0 200px;
      margin: 0;
    }

    .gold pre code,
    .gold pre code span,
    .gold code pre,
    .gold code pre span {
      color: gold;
    }

    @media screen and (max-width: 600px) {
      body {
        padding-left: 0;
      }

      nav {
        display: none;
      }
    }
  </style>
</head>

<body>
  <nav>
    <h3><a href="../data_collection/index.html">data collection</a></h3>
    <a href="./Crime_reports.html">Crime reports</a>
<br /><br />
<a href="./https___www_amazon_com_gp_yourstore_iyr_ref_pd_ys_iyr_nextie_UTF8_collection_watched_iyrGroup__maxIt.html">https://www.amazon.com/gp/yourstore/iyr/ref=pd_ys_iyr_nextie=UTF8&collection=watched&iyrGroup=&maxItem=616&minItem=600</a>
<br /><br />
<a href="./meta_search_all.html">meta search all</a>
<br /><br />
<a href="./schedule_search_all.html">schedule search all</a>
<br /><br />
<a href="./tell_joke.html">tell joke</a>
<br /><br />
<a href="./multi_crawl.html">multi crawl</a>
<br /><br />
<a href="./crawl_domain.html">crawl domain</a>
<br /><br />
<a href="./domain_cache_tools.html">domain cache tools</a>
<br /><br />
<a href="./browser_crawler_tools.html">browser crawler tools</a>
<br /><br />
<a href="./analyze_cache_file.html">analyze cache file</a>
<br /><br />
<a href="./schedule_crawl_domain.html">schedule crawl domain</a>
<br /><br />
<a href="./collect_all_bookmarks.html">collect all bookmarks</a>
<br /><br />
<a href="./search_results_as_json.html">search results as json</a>
<br /><br />

  </nav>
  <header>
    <a href="../data_collection/index.html">data collection</a> | <a href="./multi_crawl.html">multi crawl</a> | <a href="./domain_cache_tools.html">domain cache tools</a> | <a href="../../search.html">Search</a>
  </header>

  <p>crawlRecursive(url, depth, searches)**:</p>
<p>The <code>crawlRecursive</code> function is a recursive web crawler that starts at a specified initial URL, iteratively retrieves links from the crawled pages, and stores them in a cache, with the ability to manage recursion depth and cache updates. The function uses a series of steps, including crawling, cache management, link extraction, recursion, and termination, to fetch and process links from the web pages.</p>
<h2>Run example</h2>

<pre language="bash"><code>npm run import -- "crawl domain"</code></pre><h1>crawl domain</h1>



<pre class="javascript"><code>var {URL} = require('url')
var fs = require('fs');
var path = require('path');
var importer = require('../Core');
var {doBrowserRequest} = importer.import("<a href="../../Analytics/data_collection/browser_crawler_tools.html">browser crawler tools</a>")
var {
    cacheFilename,
    existingCache,
    storeCache,
    readCache,
    rmhash
} = importer.import("<a href="../../Analytics/data_collection/domain_cache_tools.html">domain cache tools</a>")

async function crawlRecursive(url, depth, searches) {
    if(!depth) depth = 3 // TODO: minutes depth using time range?
    url = (typeof url === 'string' ? [url] : url )
    // searches2 keeps track of new pages that should be added if searches is not provided
    //   this guaruntees at least one page will be requested when this is called
    const searches2 = []
    for(var i = 0; i &lt; url.length; i++) {
        var l = url[i]
        try {
            await doBrowserRequest(l, readCache.bind(null, searches || searches2),
                                   storeCache.bind(null, searches || searches2))
        } catch (e) {
            console.log(e)
        }
    }
    
    // TODO: fix this
    
    // push old cache on to bottom of current searches,
    //   so we are always getting at least the page requested
    if(!searches) searches = searches2.concat(existingCache(url[0]))
    
    var existing = searches.map(s =&gt; rmhash(s.url))
    var links = searches2
    // TODO: pattern defensive programming
        .map(s =&gt; {
            var styles = s.styles || []
            var links = s.links || []
            return styles.concat(links)
        })
        .flat()
        // do not include hash in actual link to the page
        .map(s =&gt; rmhash(s))
        // filter out first occurence
        .filter((l, i, arr) =&gt; arr.indexOf(l) === i
        // filter out all existing urls
                &amp;&amp; !existing.includes(rmhash(l))
        // filter out data uris
                &amp;&amp; !l.includes('data:') &amp;&amp; !l.includes('mailto:')
                &amp;&amp; !l.includes('javascript:') &amp;&amp; !l.includes('ios-app:'))
    
    if(depth &gt; 1) {
        return await crawlRecursive(links, depth - 1, searches)
    }
    
    // close the browser
    await doBrowserRequest(false)
    
    // save the database
    var filePath = cacheFilename(searches[0] ? searches[0].url : url[0])
    fs.writeFileSync(filePath, JSON.stringify(searches, null, 2))
}

async function crawlAll(url, depth, searches) {
    try {
        await crawlRecursive(url, depth, searches)
    } catch (e) {
        console.log(e)
        await doBrowserRequest(false)
    }
}

module.exports = crawlAll

//var importer = require('../Core')
//var crawlAll = importer.import("<a href="../../Analytics/data_collection/crawl_domain.html">crawl domain</a>")

if(typeof $ !== 'undefined') {
    $.async()
    crawlAll('https://google.com', 2)
        .then(r =&gt; $.sendResult('done'))
        .catch(e =&gt; $.sendError(e))
}

</code></pre>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">


<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- and it's easy to individually load additional languages -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script src="../../mergehtml.js"></script>

<script>
hljs.addPlugin(mergeHTMLPlugin);
hljs.highlightAll();
</script>

<div class="gold"><h2>What the code could have been:</h2>
<pre class="javascript"><code>const { URL } = require('url');
const fs = require('fs');
const path = require('path');
const importer = require('../Core');

const { doBrowserRequest, closeBrowser } = importer.import('browser crawler tools');
const {
  cacheFilename,
  existingCache,
  storeCache,
  readCache,
  rmhash,
} = importer.import('domain cache tools');

/**
 * Crawls a URL recursively.
 * 
 * @param {string|Array<string>} url - The URL(s) to crawl.
 * @param {number} depth - The maximum depth to crawl.
 * @param {Array} searches - The current search results.
 * @returns {Promise} A promise that resolves with the search results.
 */
async function crawlRecursive(url, depth, searches) {
  if (depth === undefined) depth = 3; // Set default depth
  url = (typeof url ==='string'? [url] : url);

  const searches2 = new Set(); // Use a Set to keep track of new pages

  for (const l of url) {
    try {
      await doBrowserRequest(l, readCache.bind(null, searches || searches2), storeCache.bind(null, searches || searches2));
    } catch (e) {
      console.error(e);
    }
  }

  if (!searches) {
    searches = existingCache(url[0]).concat(searches2);
  }

  const existing = new Set(searches.map((s) => rmhash(s.url)));
  const links = new Set(searches2);

  const stylesAndLinks = links
   .map((s) => {
      const styles = s.styles || [];
      const links = s.links || [];
      return styles.concat(links);
    })
   .flat()
   .map((s) => rmhash(s))
   .filter((l, i, arr) => arr.indexOf(l) === i)
   .filter((l) =>!existing.has(rmhash(l)))
   .filter((l) =>!l.includes('data:') &&!l.includes('mailto:') &&!l.includes('javascript:') &&!l.includes('ios-app:'))
   .filter((l) => l.includes('http')); // Filter out invalid URLs

  if (depth > 1) {
    return await crawlRecursive(Array.from(stylesAndLinks), depth - 1, searches);
  }

  await closeBrowser(); // Close the browser
  const filePath = cacheFilename(searches[0]? searches[0].url : url[0]);
  fs.writeFileSync(filePath, JSON.stringify(searches, null, 2));
}

/**
 * Crawls a URL and its links recursively.
 * 
 * @param {string|Array<string>} url - The URL(s) to crawl.
 * @param {number} depth - The maximum depth to crawl.
 * @param {Array} searches - The current search results.
 * @returns {Promise} A promise that resolves with the search results.
 */
async function crawlAll(url, depth, searches) {
  try {
    await crawlRecursive(url, depth, searches);
  } catch (e) {
    console.error(e);
    await closeBrowser();
  }
}

module.exports = crawlAll;</code></pre></div><p><strong>Function Breakdown: crawlRecursive(url, depth, searches)</strong></p>
<p><strong>Parameters:</strong></p>
<ul>
<li><code>url</code>: The initial URL to start crawling from. Can be a string or an array of strings.</li>
<li><code>depth</code>: The maximum recursion depth. Defaults to 3.</li>
<li><code>searches</code>: An array of previously crawled search results.</li>
</ul>
<p><strong>Functionality:</strong></p>
<ol>
<li><p><strong>Initialization:</strong></p>
<ul>
<li>If <code>depth</code> is not provided, it defaults to 3.</li>
<li>If <code>url</code> is a string, it is wrapped in an array.</li>
<li>An empty array <code>searches2</code> is created to store new pages to be added.</li>
</ul></li>
<li><p><strong>Crawling:</strong></p>
<ul>
<li>The function iterates over the <code>url</code> array and makes a browser request to each URL using <code>doBrowserRequest</code>.</li>
<li>The cache is read and stored using <code>readCache</code> and <code>storeCache</code> respectively.</li>
<li>If an error occurs, it is logged to the console.</li>
</ul></li>
<li><p><strong>Cache Management:</strong></p>
<ul>
<li>If no <code>searches</code> array is provided, it is updated with the new pages stored in <code>searches2</code>.</li>
<li>The old cache is pushed to the bottom of the current <code>searches</code> array.</li>
</ul></li>
<li><p><strong>Link Extraction:</strong></p>
<ul>
<li>The <code>searches2</code> array is processed to extract links.</li>
<li>Links are filtered to exclude existing URLs, data URIs, and specific types of links.</li>
</ul></li>
<li><p><strong>Recursion:</strong></p>
<ul>
<li>If <code>depth</code> is greater than 1, the function calls itself recursively with the extracted links, decreased <code>depth</code>, and the updated <code>searches</code> array.</li>
</ul></li>
<li><p><strong>Termination:</strong></p>
<ul>
<li>If <code>depth</code> is 1, the function returns the extracted links.</li>
</ul></li>
</ol>
<p><strong>Notable Notes:</strong></p>
<ul>
<li>The function uses <code>bind</code> to pass <code>searches</code> (or <code>searches2</code>) as the cache callback to <code>doBrowserRequest</code>.</li>
<li>The <code>rmhash</code> function is used to remove the hash from URLs.</li>
<li>The <code>existingCache</code> function is used to retrieve the existing cache for a given URL.</li>
<li>The <code>storeCache</code> function is used to store the new cache.</li>
<li>The <code>readCache</code> function is used to read the cache.</li>
</ul>

</body>

</html>