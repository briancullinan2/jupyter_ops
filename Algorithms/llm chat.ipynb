{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO: move chat analysis tools here, separate from brian's resume, generalize so history can be found in discord and both bots can benefit from the additional context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Persistance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### store conversation\n",
    "\n",
    "store llm response?\n",
    "\n",
    "store chat history so that it can be recalled later and added to the context window, this gives the LLM more context about how the conversation is going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const selectModel = importer.import('select llm')\n",
    "const {askLlamaAboutEmotions} = importer.import('ask llm about emotions')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function storeResponse(user, session, content, context, otr) {\n",
    "  let promptModel = await selectModel(DEFAULT_MODEL)\n",
    "\n",
    "  if(!session) {\n",
    "    return {\n",
    "      emotions: await askLlamaAboutEmotions(content)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let convoFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  if(typeof ACTIVE_CONVERSATIONS[convoFile] == 'undefined') {\n",
    "    if(fs.existsSync(convoFile)) {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let contextContainsImage = false\n",
    "  if(context && context.startsWith('data:image/png;base64,')) {\n",
    "    contextContainsImage = true\n",
    "  }\n",
    "\n",
    "  let summary\n",
    "  if(!otr) {\n",
    "    summary = await promptModel('Summerize this prompt in one short sentence:\\n' \n",
    "      + content + '\\nOnly respond with the summary, no pleasantries.')\n",
    "  }\n",
    "  let keywords = await promptModel('List a few key words that categorize this prompt:\\n' \n",
    "    + content + '\\nOnly respond with a single category, no pleasantries.')\n",
    "  let emotions = await askLlamaAboutEmotions(content)\n",
    "\n",
    "  let result = ACTIVE_CONVERSATIONS[convoFile][Date.now()] = {\n",
    "    user: user,\n",
    "    content: otr ? void 0 : content,\n",
    "    context: contextContainsImage ? void 0 : context,\n",
    "    summary: summary,\n",
    "    keywords: keywords,\n",
    "    emotions: emotions,\n",
    "    otr: otr ? true : false,\n",
    "  }\n",
    "  fs.writeFileSync(convoFile, JSON.stringify(ACTIVE_CONVERSATIONS[convoFile], null, 4))\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = storeResponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### general discussion\n",
    "\n",
    "general chit chat?\n",
    "\n",
    "everything else falls into this category. context will be populated with recent message conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "\n",
    "\n",
    "const PROJECT_PATH = path.join(__dirname, '..', 'Resources', 'Projects', 'conversations')\n",
    "const ACTIVE_CONVERSATIONS = {}\n",
    "const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'Default'\n",
    "\n",
    "// TODO: load some previous summaries and contents\n",
    "\n",
    "async function messageRecents(promptModel, session) {\n",
    "\n",
    "  if(!session) {\n",
    "    return ''\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let convoFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  // TODO: reload chat\n",
    "  if(typeof ACTIVE_CONVERSATIONS[convoFile] == 'undefined') {\n",
    "    if(fs.existsSync(convoFile)) {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let messageTimes = Object.keys(ACTIVE_CONVERSATIONS[convoFile])\n",
    "    .filter(k => k != 'summaries' && k != 'memories')\n",
    "  messageTimes.sort((a, b) => b - a)\n",
    "  let count = 0\n",
    "  let messages = 'Current date: ' + (new Date).toISOString() \n",
    "    + '\\nOur recent conversion:\\n'\n",
    "  for(let i = 0; i < messageTimes.length && count < 25; i++) {\n",
    "    let message = ACTIVE_CONVERSATIONS[convoFile][messageTimes[i]]\n",
    "    let useSummary = i > 10 || messages.length > 1024 || (message.content && message.content.length > 1024)\n",
    "    let useKeywords = !message.content\n",
    "    messages += (message.user ? 'User: ' : 'AI: ') \n",
    "      + (useKeywords ? message.keywords : (useSummary ? message.summary : message.content))\n",
    "      + '\\n\\n'\n",
    "    if(messages.length > 2048) {\n",
    "      break\n",
    "    }\n",
    "    count++\n",
    "  }\n",
    "\n",
    "  return messages\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  messageRecents,\n",
    "  PROJECT_PATH,\n",
    "  ACTIVE_CONVERSATIONS,\n",
    "  DEFAULT_MODEL\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### load history\n",
    "\n",
    "relevant llm history?\n",
    "\n",
    "Ask the llm to pick out history files that are related to the current prompt. This way if they refer to last month, week, yesterday, the correct history files will be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function relevantHistory(promptModel, session, prompt) {\n",
    "\n",
    "  if(!session) {\n",
    "    return ''\n",
    "  }\n",
    "\n",
    "  // TODO: extract time frame to, from prompt\n",
    "  let history = fs.readdirSync(PROJECT_PATH)\n",
    "  let historyFiles = []\n",
    "  for(let i = 0; i < history.length; i++) {\n",
    "    if(history[i].match('-' + DEFAULT_MODEL + '-' + session + '.json')) {\n",
    "      historyFiles.push(path.basename(history[i]))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let q1 = 'Current date: ' + now.getFullYear() + '-' + String(now.getMonth() + 1).padStart(2, '0')\n",
    "    + '\\nGiven the following chat history:\\n'\n",
    "    + historyFiles.map(f => f.substring(0, 7)).join('\\n')\n",
    "    + '\\nGiven the following prompt:\\n'\n",
    "    + prompt + '\\nReturn a short range of Year-Month for related dates.'\n",
    "  console.log('User: ' + q1)\n",
    "  let a1 = await promptModel(q1)\n",
    "  console.log('AI: ' + a1)\n",
    "\n",
    "  // TODO: create a range of month out of the response\n",
    "  let relevantFiles = []\n",
    "  for(let i = 0; i < historyFiles.length; i++) {\n",
    "    if(a1.match(historyFiles[i])) {\n",
    "      let convoFile = path.join(PROJECT_PATH, historyFiles[i])\n",
    "      if(fs.existsSync(convoFile)) {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "      } else {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "      }\n",
    "      relevantFiles.push(historyFiles[i])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return relevantFiles\n",
    "}\n",
    "\n",
    "module.exports = relevantHistory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### chat keywords\n",
    "\n",
    "relevant chat keywords?\n",
    "\n",
    "Pick out the relevant keywords to search in chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function relevantKeywords(promptModel, session, prompt) {\n",
    "  \n",
    "  // TODO: search all loaded user histories for keywords\n",
    "  let keywords = []\n",
    "  let loadedConversations = Object.keys(ACTIVE_CONVERSATIONS)\n",
    "    .filter(key => key.match('-' + DEFAULT_MODEL + '-' + session + '.json'))\n",
    "  loadedConversations.sort((a, b) => b - a)\n",
    "  for(let i = 0; i < loadedConversations.length; i++) {\n",
    "    let conversation = ACTIVE_CONVERSATIONS[loadedConversations[i]]\n",
    "    let timestamps = Object.keys(conversation).filter(k => k != 'summaries' && k != 'memories')\n",
    "    timestamps.sort((a, b) => b - a)\n",
    "    for(let j = 0; j < timestamps.length; j++) {\n",
    "      let message = conversation[timestamps[j]]\n",
    "      let newKeywords = message.keywords\n",
    "      newKeywords = newKeywords.replace(/.*?:\\s*\\n\\s*/gi, '')\n",
    "      newKeywords = newKeywords.replaceAll('*', '').replaceAll('# ', '').replaceAll('#', '').replaceAll(/[0-9]+\\.\\s*/gi, '')\n",
    "      newKeywords = newKeywords.split(/\\s*\\n\\s*|\\s*,\\s*/gi)\n",
    "      keywords = keywords.concat(newKeywords.map(k => k.trim()))\n",
    "    }\n",
    "  }\n",
    "  keywords = keywords.filter((k, i, arr) => k && arr.indexOf(k) == i)\n",
    "\n",
    "\n",
    "  let q2 = 'Given the following keywords:\\n'\n",
    "    + keywords.join('\\n')\n",
    "    + '\\nGiven the following prompt:\\n'\n",
    "    + prompt + '\\nReturn only a few keywords related to the prompt.'\n",
    "  console.log('User: ' + q2)\n",
    "  let a2 = await promptModel(q2)\n",
    "  console.log('AI: ' + a2)\n",
    "\n",
    "  let matching = []\n",
    "  for(let i = 0; i < keywords.length; i++) {\n",
    "    if(a2.match(keywords[i])) {\n",
    "      matching.push(keywords[i])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return matching\n",
    "}\n",
    "\n",
    "module.exports = relevantKeywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specific timestamps\n",
    "\n",
    "relevant history timestamps?\n",
    "\n",
    "Get a list of timestamps that might be related to the current prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "async function askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps) {\n",
    "  let q3 = 'Given the following keywords:\\n'\n",
    "    + keywords.join('\\n')\n",
    "    + '\\n' + messages\n",
    "    + prompt + '\\nOnly respond with related and unique timestamps, no explanations.'\n",
    "  console.log('User: ' + q3)\n",
    "  let a3 = await promptModel(q3)\n",
    "  console.log('AI: ' + a3)\n",
    "\n",
    "  return timestamps\n",
    "    .filter(time => a3.match(time) || a3.match(new Date(parseInt(time)).toISOString()))\n",
    "}\n",
    "\n",
    "\n",
    "async function matchingTimestamps(promptModel, session, prompt, keywords) {\n",
    "  \n",
    "  let matchingTimestamps = []\n",
    "  let messages = 'Current date: ' + (new Date).toISOString() \n",
    "    + '\\nOur recent topics:\\n'\n",
    "  let originalTimestamp = messages\n",
    "  let loadedConversations = Object.keys(ACTIVE_CONVERSATIONS)\n",
    "    .filter(key => key.match('-' + DEFAULT_MODEL + '-' + session + '.json'))\n",
    "  for(let i = 0; i < loadedConversations.length; i++) {\n",
    "    let conversation = ACTIVE_CONVERSATIONS[loadedConversations[i]]\n",
    "    let timestamps = Object.keys(conversation).filter(k => k != 'summaries' && k != 'memories')\n",
    "    timestamps.sort((a, b) => b - a)\n",
    "    for(let j = 0; j < timestamps.length; j++) {\n",
    "      let message = conversation[timestamps[j]]\n",
    "      let topics = keywords.filter(key => message.keywords.match(key))\n",
    "      if(!prompt.match(timestamps[j]) && topics.length == 0) {\n",
    "        continue\n",
    "      }\n",
    "\n",
    "      messages += new Date(parseInt(timestamps[j])).toISOString() \n",
    "        + ' - ' + topics.join(', ') \n",
    "        + (message.summary ? (' - ' + message.summary) : '') \n",
    "        + '\\n'\n",
    "\n",
    "      if(messages.length > 2048) {\n",
    "        let newTimestamps = await askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps)\n",
    "\n",
    "        matchingTimestamps = matchingTimestamps.concat(newTimestamps)\n",
    "\n",
    "        messages = originalTimestamp\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if(messages.length > originalTimestamp.length) {\n",
    "      let newTimestamps = await askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps)\n",
    "\n",
    "      matchingTimestamps = matchingTimestamps.concat(newTimestamps)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return matchingTimestamps\n",
    "}\n",
    "\n",
    "module.exports = matchingTimestamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conversation Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### classify prompt\n",
    "\n",
    "classify llm prompt?\n",
    "\n",
    "A basic entry point to allow LLM to access select notebook functions.\n",
    "\n",
    "TODO: merge this code with \"ask llm matching function\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const getExports = importer.import('get exports from source')\n",
    "\n",
    "\n",
    "const API_FUNCTIONS = [\n",
    "  'load message history',\n",
    "  'general chit chat',\n",
    "  'brians resume',\n",
    "  'file system access',\n",
    "  'access web information',\n",
    "  'stable diffusion request',\n",
    "  'request ollama vision',\n",
    "  //'ask another llm for help',\n",
    "  'add conversation context',\n",
    "  'llm save memories'\n",
    "  // 'switch llm models'\n",
    "]\n",
    "\n",
    "const API_EXPORTS = API_FUNCTIONS.map(func => getExports(importer.interpret(func).code)[0])\n",
    "\n",
    "const API_DESCRIPTION = API_FUNCTIONS\n",
    "  .map(func => importer.interpret(func))\n",
    "  .filter((code, i) => API_EXPORTS[i])\n",
    "  .map((code, i) => API_EXPORTS[i] + ' - ' + code.markdown.join('').trim()\n",
    "    .replaceAll(/^#.*?\\n/gi, '')\n",
    "    .replaceAll(code.questions, '')\n",
    "    .replaceAll(/\\s*[\\n]+[\\s\\r\\n]*/gi, '-'))\n",
    "  .join('\\n')\n",
    "\n",
    "async function askLlamaMatchingFunction(promptModel, prompt, image) {\n",
    "\n",
    "  let q1 = 'Given the following functions:\\n'\n",
    "    + API_DESCRIPTION + '\\nDo any of these functions relate to this prompt:\\n'\n",
    "    + (image ? 'An image has been provided as a part of the input.' : '')\n",
    "    + prompt + '\\nOnly return the function names in order of relevance.'\n",
    "  console.log('User: ' + q1)\n",
    "  let a1 = await promptModel(q1)\n",
    "  console.log('AI: ' + a1)\n",
    "\n",
    "  let matchingFunctions = a1.split(/\\s*\\n\\s*|\\s*,\\s*/gi)\n",
    "    .map((matchStr) => {\n",
    "      for (let i = 0; i < API_FUNCTIONS.length; i++) {\n",
    "        if (!API_EXPORTS[i]) {\n",
    "          continue\n",
    "        }\n",
    "        if (API_EXPORTS[i].includes(matchStr) || matchStr.match(API_EXPORTS[i])) {\n",
    "          return ({\n",
    "            importStr: API_FUNCTIONS[i],\n",
    "            exportStr: API_EXPORTS[i]\n",
    "          })\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    .filter(func => a1.length > 0 && func)\n",
    "    .map(func => func.importStr)\n",
    "\n",
    "  return matchingFunctions\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaMatchingFunction,\n",
    "  API_FUNCTIONS,\n",
    "  API_EXPORTS,\n",
    "  API_DESCRIPTION,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### conversation handler\n",
    "\n",
    "handle conversation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const classifyPrompt = importer.import('add conversation context')\n",
    "const storeResponse = importer.import('store llm response')\n",
    "\n",
    "async function handleConversation(promptModel, session, prompt, image, otr) {\n",
    "  let context = await classifyPrompt(promptModel, session, prompt, image)\n",
    "\n",
    "  let messageStorage = await storeResponse(true, session, prompt, \n",
    "    context.content + (context.memories ? ('\\n' + context.memories.join('\\n')) : ''), \n",
    "    otr)\n",
    "\n",
    "  let q1 = (context && context.content ? context.content : '') \n",
    "  + (context.image ? '\\nThe response image is provided.\\n' : '')\n",
    "  + '\\nRespond to this message and pretend to be emotional (e.g. ' \n",
    "  + messageStorage.emotions.join(', ') + '):\\n' \n",
    "  + prompt\n",
    "  + (context.memories ? ('\\nApply any relevant memories to your reponse:\\n' + context.memories.join('\\n')) : '')\n",
    "  console.log('User: ' + q1)\n",
    "  let result = await promptModel(q1)\n",
    "\n",
    "  if(context.imagePath) {\n",
    "    if(result.match(/!\\[[^\\]]*\\]\\((.*?)\\s*(\"(?:.*[^\"])\")?\\s*\\)/gi)) {\n",
    "      result = result.replace(/!\\[[^\\]]*\\]\\((.*?)\\s*(\"(?:.*[^\"])\")?\\s*\\)/gi, \n",
    "        // TODO: accomodate discord by removing the markdown? \n",
    "        //   or providing a server address to brian-chat? \n",
    "        //   return entire context with result to doMention()?\n",
    "        '![' + context.prompt + '](/' + context.imagePath.replaceAll(' ', '%20') + ')')\n",
    "    } else {\n",
    "      result += '\\n\\n![' + context.prompt + '](/' + context.imagePath.replaceAll(' ', '%20') + ')'\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if(session) {\n",
    "    await storeResponse(false, session, result, void 0, otr)\n",
    "  }\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = handleConversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### add conversation\n",
    "\n",
    "add conversation context?\n",
    "\n",
    "create a conversation context by selecting a runnable function available to call by the llm. this is the entry point for all inquiries about functionality or what functions are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const { messageRecents } = importer.import('general chit chat')\n",
    "const {doStableRequest} = importer.import('stable diffusion request')\n",
    "//const messageOllamaVision = importer.import('describe an image')\n",
    "const {askLlamaMatchingFunction, API_DESCRIPTION} = importer.import('classify llm prompt')\n",
    "const getParameters = importer.import('function parameters')\n",
    "const {listMemories} = importer.import('llm load memories')\n",
    "\n",
    "async function classifyPrompt(promptModel, session, prompt, image, otr) {\n",
    "\n",
    "  let matchingFunctions = await askLlamaMatchingFunction(promptModel, prompt, image)\n",
    "\n",
    "  let importedFunction\n",
    "  let answer = ''\n",
    "  let context = {\n",
    "    promptModel, session, prompt, image, otr\n",
    "  }\n",
    "\n",
    "  // drop out early if the matching function is ourselves, this is how we return our function description for inquiries about capabilities.\n",
    "  if(matchingFunctions == classifyPrompt) {\n",
    "    return {\n",
    "      memories: await listMemories(session),\n",
    "      content: 'Given the following functions:\\n' + API_DESCRIPTION\n",
    "    }\n",
    "  }\n",
    "\n",
    "  //let historyFiles = await relevantHistory(promptModel, session, prompt)\n",
    "\n",
    "  // TODO: convert to available parameters like Core/import.ipynb:run() style parameterization\n",
    "  for (let i = 0; i < matchingFunctions.length; i++) {\n",
    "    importedFunction = importer.import(matchingFunctions[i])\n",
    "    if (typeof importedFunction == 'object' && typeof Object.values(importedFunction)[0] == 'function') {\n",
    "      importedFunction = Object.values(importedFunction)[0]\n",
    "    }\n",
    "    // call parameterized\n",
    "    let params = getParameters(importedFunction.toString()).slice(1)\n",
    "    let inputs = []\n",
    "    for(let j = 0; j < params.length; j++)\n",
    "      inputs[j] = context[params[j]]\n",
    "    answer = await importedFunction.apply(null, inputs)\n",
    "    break\n",
    "  }\n",
    "\n",
    "  if(importedFunction == doStableRequest) {\n",
    "    return Object.assign(answer, {\n",
    "      memories: await listMemories(session),\n",
    "      content: await messageRecents(session, prompt)\n",
    "    })\n",
    "  } else if (importedFunction != messageRecents) {\n",
    "    if(typeof answer == 'object') {\n",
    "      return Object.assign({}, {\n",
    "        memories: await listMemories(session),\n",
    "        content: await messageRecents(session, prompt)\n",
    "      }, answer)\n",
    "    } else {\n",
    "      return {\n",
    "        memories: await listMemories(session),\n",
    "        content: (answer ? answer : '') + await messageRecents(session, prompt)\n",
    "      }\n",
    "    }\n",
    "  } else if (!answer) {\n",
    "    return {\n",
    "      memories: await listMemories(session),\n",
    "      content: await messageRecents(session, prompt)\n",
    "    }\n",
    "  } else {\n",
    "    return {\n",
    "      memories: await listMemories(session),\n",
    "      content: answer\n",
    "    }\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = classifyPrompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Memory bank\n",
    "\n",
    "TODO: short and long term memory banks, short term can be a single month, not copied and for a single run/prompt stored by intermittent functions. long term are set by the user and copied from month to month.\n",
    "\n",
    "Memories are appended after the prompt as additional instructions, these functions should ask for which memories are relevant intructions for the prompt. memories are applied to the last prompt in the sequence before returning to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### loading memories\n",
    "\n",
    "llm load memories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function listMemories(session) {\n",
    "\n",
    "  let memories = await findMemories(session)\n",
    "  return Object.keys(memories).map(key => {\n",
    "    // provide the date the memory was made\n",
    "    let date = new Date(parseInt(key))\n",
    "    return date.getFullYear() + '-' + String(date.getMonth() + 1).padStart(2, '0') + '-' + date.getDate()\n",
    "      + ' - ' + memories[key]\n",
    "  })\n",
    "}\n",
    "\n",
    "async function findMemories(session) {\n",
    "\n",
    "  if(!session) {\n",
    "    return {}\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let currentFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  // TODO: reload chat\n",
    "  if(typeof ACTIVE_CONVERSATIONS[currentFile] == 'undefined') {\n",
    "    if(fs.existsSync(currentFile)) {\n",
    "      ACTIVE_CONVERSATIONS[currentFile] = JSON.parse(fs.readFileSync(currentFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[currentFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if(typeof ACTIVE_CONVERSATIONS[currentFile]['memories'] != 'undefined') {\n",
    "    debugger\n",
    "    return ACTIVE_CONVERSATIONS[currentFile]['memories']\n",
    "  }\n",
    "\n",
    "\n",
    "  let history = fs.readdirSync(PROJECT_PATH)\n",
    "  // newest to oldest\n",
    "  history.sort((a, b) => b - a)\n",
    "  for(let i = 0; i < history.length; i++) {\n",
    "    if(!history[i].match('-' + DEFAULT_MODEL + '-' + session + '.json')) {\n",
    "      continue\n",
    "    }\n",
    "\n",
    "    let convoFile = path.join(PROJECT_PATH, history[i])\n",
    "    if(typeof ACTIVE_CONVERSATIONS[convoFile] == 'undefined') {\n",
    "      if(fs.existsSync(convoFile)) {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "      } else {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "      }\n",
    "    }\n",
    "\n",
    "    let conversation = ACTIVE_CONVERSATIONS[convoFile]\n",
    "    if(typeof conversation['memories'] != 'undefined') {\n",
    "      return (ACTIVE_CONVERSATIONS[currentFile]['memories'] = conversation['memories'])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return (ACTIVE_CONVERSATIONS[currentFile]['memories'] = {})\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  findMemories,\n",
    "  listMemories,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### saving memories\n",
    "\n",
    "llm save memories?\n",
    "\n",
    "make changes to short and long term memories per the user's instructions. save a personal memory, always remember, remove a memory, modify internal memory, list recent memories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "const {findMemories, listMemories} = importer.import('llm load memories')\n",
    "\n",
    "\n",
    "async function manageMemories(promptModel, session, prompt) {\n",
    "\n",
    "  if(!session) {\n",
    "    return ''\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let currentFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  // TODO: reload chat\n",
    "  if(typeof ACTIVE_CONVERSATIONS[currentFile] == 'undefined') {\n",
    "    if(fs.existsSync(currentFile)) {\n",
    "      ACTIVE_CONVERSATIONS[currentFile] = JSON.parse(fs.readFileSync(currentFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[currentFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  const MEMORY_FUNCTIONS = [\n",
    "    'listMemories() - display a list of stored memories at the users request.',\n",
    "    'saveMemory() - saves a new memory given the instructions to always remember something.',\n",
    "    'removeMemory() - removes a matching memory from the storage bank.',\n",
    "    'clearMemory() - removes all memories but only if the user is absolutely certain.',\n",
    "  ]\n",
    "\n",
    "  let q1 = 'Given the following memory related functions\\n'\n",
    "    + MEMORY_FUNCTIONS.join('\\n')\n",
    "    + '\\nWhich function best matches this prompt?\\n'\n",
    "    + prompt\n",
    "    + '\\nOnly return the function name and nothing else.'\n",
    "  let a1 = await promptModel(q1)\n",
    "\n",
    "  if(a1.match('listMemories')) {\n",
    "\n",
    "  } else if(a1.match('saveMemory')) {\n",
    "    let q2 = 'Summerize this memory in one very precise sentance in first person:\\n' + prompt + '\\nOnly return the summary, no explanation.'\n",
    "    let a2 = await promptModel(q2)\n",
    "\n",
    "    ACTIVE_CONVERSATIONS[currentFile]['memories'] = await findMemories(session)\n",
    "    ACTIVE_CONVERSATIONS[currentFile]['memories'][Date.now()] = a2\n",
    "    fs.writeFileSync(currentFile, JSON.stringify(ACTIVE_CONVERSATIONS[currentFile], null, 4))\n",
    "  } else if (a1.match('removeMemory')) {\n",
    "    // TODO: make a functional list of memories\n",
    "  } else if (a1.match('clearMemory')) {\n",
    "    // TODO: did the user indicate they are absolutely sure about this action? Yes or No\n",
    "    \n",
    "  }\n",
    "  return 'This is the list of available memories:\\n' + await listMemories(session)\n",
    "}\n",
    "\n",
    "module.exports = manageMemories\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
