{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO: move chat analysis tools here, separate from brian's resume, generalize so history can be found in discord and both bots can benefit from the additional context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Persistance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### store conversation\n",
    "\n",
    "store llm response?\n",
    "\n",
    "store chat history so that it can be recalled later and added to the context window, this gives the LLM more context about how the conversation is going.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const selectModel = importer.import('select llm')\n",
    "const {askLlamaAboutEmotions} = importer.import('ask llm about emotions')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function storeResponse(user, session, content, context, otr) {\n",
    "  let promptModel = await selectModel(DEFAULT_MODEL)\n",
    "\n",
    "  let now = new Date()\n",
    "  let convoFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  if(typeof ACTIVE_CONVERSATIONS[convoFile] == 'undefined') {\n",
    "    if(fs.existsSync(convoFile)) {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let contextContainsImage = false\n",
    "  if(context.startsWith('data:image/png;base64,')) {\n",
    "    contextContainsImage = true\n",
    "  }\n",
    "\n",
    "  let summary\n",
    "  if(!otr) {\n",
    "    summary = await promptModel('Summerize this prompt in one short sentence:\\n' \n",
    "      + content + '\\nOnly respond with the summary, no pleasantries.')\n",
    "  }\n",
    "  let keywords = await promptModel('List a few key words that categorize this prompt:\\n' \n",
    "    + content + '\\nOnly respond with a single category, no pleasantries.')\n",
    "  let emotions = await askLlamaAboutEmotions(content)\n",
    "\n",
    "  ACTIVE_CONVERSATIONS[convoFile][Date.now()] = {\n",
    "    user: user,\n",
    "    content: otr ? void 0 : content,\n",
    "    context: contextContainsImage ? void 0 : context,\n",
    "    summary: summary,\n",
    "    keywords: keywords,\n",
    "    emotions: emotions,\n",
    "    otr: otr ? true : false,\n",
    "  }\n",
    "  fs.writeFileSync(convoFile, JSON.stringify(ACTIVE_CONVERSATIONS[convoFile], null, 4))\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = storeResponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### general discussion\n",
    "\n",
    "general chit chat?\n",
    "\n",
    "everything else falls into this category. context will be populated with recent message conversation history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "\n",
    "\n",
    "const PROJECT_PATH = path.join(__dirname, '..', 'Resources', 'Projects', 'conversations')\n",
    "const ACTIVE_CONVERSATIONS = {}\n",
    "const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'Default'\n",
    "\n",
    "// TODO: load some previous summaries and contents\n",
    "\n",
    "async function messageRecents(promptModel, session) {\n",
    "\n",
    "  if(!session) {\n",
    "    return ''\n",
    "  }\n",
    "\n",
    "  let now = new Date()\n",
    "  let convoFile = path.join(PROJECT_PATH, now.getFullYear() + '-' \n",
    "    + String(now.getMonth() + 1).padStart(2, '0') \n",
    "    + '-' + DEFAULT_MODEL\n",
    "    + '-' + session + '.json')\n",
    "  // TODO: reload chat\n",
    "  if(typeof ACTIVE_CONVERSATIONS[convoFile] == 'undefined') {\n",
    "    if(fs.existsSync(convoFile)) {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "    } else {\n",
    "      ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let messageTimes = Object.keys(ACTIVE_CONVERSATIONS[convoFile]).filter(k => k != 'summaries')\n",
    "  messageTimes.sort((a, b) => b - a)\n",
    "  let count = 0\n",
    "  let messages = 'Current date: ' + (new Date).toISOString() \n",
    "    + '\\nOur recent conversion:\\n'\n",
    "  for(let i = 0; i < messageTimes.length && count < 25; i++) {\n",
    "    let message = ACTIVE_CONVERSATIONS[convoFile][messageTimes[i]]\n",
    "    let useSummary = i > 10 || messages.length > 1024 || (message.content && message.content.length > 1024)\n",
    "    let useKeywords = !message.content\n",
    "    messages += (message.user ? 'User: ' : 'AI: ') \n",
    "      + (useKeywords ? message.keywords : (useSummary ? message.summary : message.content))\n",
    "      + '\\n\\n'\n",
    "    if(messages.length > 2048) {\n",
    "      break\n",
    "    }\n",
    "    count++\n",
    "  }\n",
    "\n",
    "  return messages\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  messageRecents,\n",
    "  PROJECT_PATH,\n",
    "  ACTIVE_CONVERSATIONS,\n",
    "  DEFAULT_MODEL\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### load history\n",
    "\n",
    "relevant llm history?\n",
    "\n",
    "Ask the llm to pick out history files that are related to the current prompt. This way if they refer to last month, week, yesterday, the correct history files will be loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function relevantHistory(promptModel, session, prompt) {\n",
    "\n",
    "  if(!session) {\n",
    "    return ''\n",
    "  }\n",
    "\n",
    "  // TODO: extract time frame to, from prompt\n",
    "  let history = fs.readdirSync(PROJECT_PATH)\n",
    "  let historyFiles = []\n",
    "  for(let i = 0; i < history.length; i++) {\n",
    "    if(history[i].match('-' + DEFAULT_MODEL + '-' + session + '.json')) {\n",
    "      historyFiles.push(path.basename(history[i]))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  let q1 = 'Current date: ' + (new Date).toISOString()\n",
    "    + '\\nGiven the following chat history:\\n'\n",
    "    + historyFiles.map(f => f.substring(0, 7)).join('\\n')\n",
    "    + '\\nGiven the following prompt:\\n'\n",
    "    + prompt + '\\nReturn a short range of Year-Month for related dates.'\n",
    "  console.log('User: ' + q1)\n",
    "  let a1 = await promptModel(q1)\n",
    "  console.log('AI: ' + a1)\n",
    "\n",
    "  // TODO: create a range of month out of the response\n",
    "  let relevantFiles = []\n",
    "  for(let i = 0; i < historyFiles.length; i++) {\n",
    "    if(a1.match(historyFiles[i])) {\n",
    "      let convoFile = path.join(PROJECT_PATH, historyFiles[i])\n",
    "      if(fs.existsSync(convoFile)) {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = JSON.parse(fs.readFileSync(convoFile))\n",
    "      } else {\n",
    "        ACTIVE_CONVERSATIONS[convoFile] = {}\n",
    "      }\n",
    "      relevantFiles.push(historyFiles[i])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return relevantFiles\n",
    "}\n",
    "\n",
    "module.export = relevantHistory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### chat keywords\n",
    "\n",
    "relevant chat keywords?\n",
    "\n",
    "Pick out the relevant keywords to search in chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const path = require('path')\n",
    "const fs = require('fs')\n",
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "\n",
    "async function relevantKeywords(promptModel, session, prompt) {\n",
    "  \n",
    "  // TODO: search all loaded user histories for keywords\n",
    "  let keywords = []\n",
    "  let loadedConversations = Object.keys(ACTIVE_CONVERSATIONS)\n",
    "    .filter(key => key.match('-' + DEFAULT_MODEL + '-' + session + '.json'))\n",
    "  loadedConversations.sort((a, b) => b - a)\n",
    "  for(let i = 0; i < loadedConversations.length; i++) {\n",
    "    let conversation = ACTIVE_CONVERSATIONS[loadedConversations[i]]\n",
    "    let timestamps = Object.keys(conversation)\n",
    "    timestamps.sort((a, b) => b - a)\n",
    "    for(let j = 0; j < timestamps.length; j++) {\n",
    "      let message = conversation[timestamps[j]]\n",
    "      let newKeywords = message.keywords\n",
    "      newKeywords = newKeywords.replace(/.*?:\\s*\\n\\s*/gi, '')\n",
    "      newKeywords = newKeywords.replaceAll('*', '').replaceAll('# ', '').replaceAll('#', '').replaceAll(/[0-9]+\\.\\s*/gi, '')\n",
    "      newKeywords = newKeywords.split(/\\s*\\n\\s*|\\s*,\\s*/gi)\n",
    "      keywords = keywords.concat(newKeywords.map(k => k.trim()))\n",
    "    }\n",
    "  }\n",
    "  keywords = keywords.filter((k, i, arr) => k && arr.indexOf(k) == i)\n",
    "\n",
    "\n",
    "  let q2 = 'Given the following keywords:\\n'\n",
    "    + keywords.join('\\n')\n",
    "    + '\\nGiven the following prompt:\\n'\n",
    "    + prompt + '\\nReturn only a few keywords related to the prompt.'\n",
    "  console.log('User: ' + q2)\n",
    "  let a2 = await promptModel(q2)\n",
    "  console.log('AI: ' + a2)\n",
    "\n",
    "  let matching = []\n",
    "  for(let i = 0; i < keywords.length; i++) {\n",
    "    if(a2.match(keywords[i])) {\n",
    "      matching.push(keywords[i])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return matching\n",
    "}\n",
    "\n",
    "module.exports = relevantKeywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specific timestamps\n",
    "\n",
    "relevant history timestamps?\n",
    "\n",
    "Get a list of timestamps that might be related to the current prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const {ACTIVE_CONVERSATIONS, PROJECT_PATH, DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "\n",
    "async function askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps) {\n",
    "  let q3 = 'Given the following keywords:\\n'\n",
    "    + keywords.join('\\n')\n",
    "    + '\\n' + messages\n",
    "    + prompt + '\\nOnly respond with related and unique timestamps, no explanations.'\n",
    "  console.log('User: ' + q3)\n",
    "  let a3 = await promptModel(q3)\n",
    "  console.log('AI: ' + a3)\n",
    "\n",
    "  return timestamps\n",
    "    .filter(time => a3.match(time) || a3.match(new Date(parseInt(time)).toISOString()))\n",
    "}\n",
    "\n",
    "\n",
    "async function matchingTimestamps(promptModel, session, prompt, keywords) {\n",
    "  \n",
    "  let matchingTimestamps = []\n",
    "  let messages = 'Current date: ' + (new Date).toISOString() \n",
    "    + '\\nOur recent topics:\\n'\n",
    "  let originalTimestamp = messages\n",
    "  let loadedConversations = Object.keys(ACTIVE_CONVERSATIONS)\n",
    "    .filter(key => key.match('-' + DEFAULT_MODEL + '-' + session + '.json'))\n",
    "  for(let i = 0; i < loadedConversations.length; i++) {\n",
    "    let conversation = ACTIVE_CONVERSATIONS[loadedConversations[i]]\n",
    "    let timestamps = Object.keys(conversation)\n",
    "    timestamps.sort((a, b) => b - a)\n",
    "    for(let j = 0; j < timestamps.length; j++) {\n",
    "      let message = conversation[timestamps[j]]\n",
    "      let topics = keywords.filter(key => message.keywords.match(key))\n",
    "      if(!prompt.match(timestamps[j]) && topics.length == 0) {\n",
    "        continue\n",
    "      }\n",
    "\n",
    "      messages += new Date(parseInt(timestamps[j])).toISOString() \n",
    "        + ' - ' + topics.join(', ') \n",
    "        + (message.summary ? (' - ' + message.summary) : '') \n",
    "        + '\\n'\n",
    "\n",
    "      if(messages.length > 2048) {\n",
    "        let newTimestamps = await askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps)\n",
    "\n",
    "        matchingTimestamps = matchingTimestamps.concat(newTimestamps)\n",
    "\n",
    "        messages = originalTimestamp\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if(messages.length > originalTimestamp.length) {\n",
    "      let newTimestamps = await askLlamaMatchTimestamps(promptModel, messages, keywords, prompt, timestamps)\n",
    "\n",
    "      matchingTimestamps = matchingTimestamps.concat(newTimestamps)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return matchingTimestamps\n",
    "}\n",
    "\n",
    "module.exports = matchingTimestamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### classify prompt\n",
    "\n",
    "classify llm prompt?\n",
    "\n",
    "A basic entry point to allow LLM to access select notebook functions.\n",
    "\n",
    "TODO: merge this code with \"ask llm matching function\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const getExports = importer.import('get exports from source')\n",
    "\n",
    "\n",
    "const API_FUNCTIONS = [\n",
    "  'load message history',\n",
    "  'general chit chat',\n",
    "  'brians resume',\n",
    "  'file system access',\n",
    "  'access web information',\n",
    "  'stable diffusion request',\n",
    "  'request ollama vision',\n",
    "  'ask another llm for help',\n",
    "  // 'switch llm models'\n",
    "]\n",
    "\n",
    "const API_EXPORTS = API_FUNCTIONS.map(func => getExports(importer.interpret(func).code)[0])\n",
    "\n",
    "const API_DESCRIPTION = API_FUNCTIONS\n",
    "  .map(func => importer.interpret(func))\n",
    "  .filter((code, i) => API_EXPORTS[i])\n",
    "  .map((code, i) => API_EXPORTS[i] + ' - ' + code.markdown.join('').trim()\n",
    "    .replaceAll(/^#.*?\\n/gi, '')\n",
    "    .replaceAll(code.questions, '')\n",
    "    .replaceAll(/\\s*[\\n]+[\\s\\r\\n]*/gi, '-'))\n",
    "  .join('\\n')\n",
    "\n",
    "async function askLlamaMatchingFunction(prompt, image) {\n",
    "\n",
    "  let q1 = 'Given the following functions:\\n'\n",
    "    + API_DESCRIPTION + '\\nDo any of these functions relate to this prompt:\\n'\n",
    "    + (image ? 'An image has been provided as a part of the input.' : '')\n",
    "    + prompt + '\\nOnly return the function names in order of relevance.'\n",
    "  console.log('User: ' + q1)\n",
    "  let a1 = await promptModel(q1)\n",
    "  console.log('AI: ' + a1)\n",
    "\n",
    "  let matchingFunctions = a1.split(/\\s*\\n\\s*|\\s*,\\s*/gi)\n",
    "    .map((matchStr) => {\n",
    "      for (let i = 0; i < API_FUNCTIONS.length; i++) {\n",
    "        if (!API_EXPORTS[i]) {\n",
    "          continue\n",
    "        }\n",
    "        if (API_EXPORTS[i].includes(matchStr) || matchStr.match(API_EXPORTS[i])) {\n",
    "          return ({\n",
    "            importStr: API_FUNCTIONS[i],\n",
    "            exportStr: API_EXPORTS[i]\n",
    "          })\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    .filter(func => a1.length > 0 && func)\n",
    "    .map(func => func.importStr)\n",
    "\n",
    "  return matchingFunctions\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaMatchingFunction,\n",
    "  API_FUNCTIONS,\n",
    "  API_EXPORTS,\n",
    "  API_DESCRIPTION,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### conversation handler\n",
    "\n",
    "handle conversation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const selectModel = importer.import('select llm')\n",
    "const classifyPrompt = importer.import('add conversation context')\n",
    "const {DEFAULT_MODEL} = importer.import('general chit chat')\n",
    "const storeResponse = importer.import('store llm response')\n",
    "\n",
    "async function handleConversation(promptModel, session, prompt, image, otr) {\n",
    "  let promptModel = await selectModel(DEFAULT_MODEL)\n",
    "\n",
    "  let context = await classifyPrompt(promptModel, req.body.session, req.body.content, req.body.image)\n",
    "\n",
    "  if(req.body.session) {\n",
    "    await storeResponse(true, req.body.session, req.body.content, context.content, req.body.otr)\n",
    "  }\n",
    "\n",
    "  let q1 = (context && context.content ? context.content : '') \n",
    "  + (context.image ? '\\nThe response image is provided.\\n' : '')\n",
    "  + '\\nRespond to this message and pretend to be emotional (e.g. ' \n",
    "  + emotions.join(', ') + '):\\n' \n",
    "  + req.body.content\n",
    "  console.log('User: ' + q1)\n",
    "  let result = await promptModel(q1)\n",
    "\n",
    "  if(req.body.session) {\n",
    "    await storeResponse(false, req.body.session, result, void 0, req.body.otr)\n",
    "  }\n",
    "\n",
    "  if(req.headers.accept == 'application/json') {\n",
    "    return res.json({ content: result });\n",
    "  }\n",
    "\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = handleConversation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### add conversation\n",
    "\n",
    "add conversation context?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const { messageRecents } = importer.import('general chit chat')\n",
    "const messageStability = importer.import('imagine or generate image')\n",
    "//const messageOllamaVision = importer.import('describe an image')\n",
    "const {askLlamaMatchingFunction} = importer.import('classify llm prompt')\n",
    "\n",
    "async function classifyPrompt(promptModel, session, prompt, image, otr) {\n",
    "\n",
    "  let matchingFunctions = await askLlamaMatchingFunction(prompt, image)\n",
    "\n",
    "  let importedFunction\n",
    "  let answer = ''\n",
    "  let context = {\n",
    "    promptModel, session, prompt, image, otr\n",
    "  }\n",
    "  // TODO: convert to available parameters like Core/import.ipynb:run() style parameterization\n",
    "  for (let i = 0; i < matchingFunctions.length; i++) {\n",
    "    importedFunction = importer.import(matchingFunctions[i])\n",
    "    if (typeof importedFunction == 'object' && typeof Object.values(importedFunction)[0] == 'function') {\n",
    "      importedFunction = Object.values(importedFunction)[0]\n",
    "    }\n",
    "    // call parameterized\n",
    "    let params = getParameters(result.toString()).slice(1)\n",
    "    let inputs = []\n",
    "    for(let j = 0; j < params.length; j++)\n",
    "      inputs[j] = context[params[j]]\n",
    "    answer = await importedFunction.apply(null, inputs)\n",
    "    break\n",
    "  }\n",
    "\n",
    "  if(importedFunction == messageStability) {\n",
    "    return {\n",
    "      image: 'data:image/png;base64,' + answer\n",
    "    }\n",
    "  } else if (importedFunction != messageRecents) {\n",
    "    return {\n",
    "      content: (answer ? answer : '') + await messageRecents(session, prompt)\n",
    "    }\n",
    "  } else if (!answer) {\n",
    "    return {\n",
    "      content: await messageRecents(session, prompt)\n",
    "    }\n",
    "  } else {\n",
    "    return {\n",
    "      content: answer\n",
    "    }\n",
    "  }\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = classifyPrompt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
