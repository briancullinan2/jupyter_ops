{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "search jupyter notebooks using llama-gpt?\n",
    "\n",
    "search llama?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import {fileURLToPath} from \"url\";\n",
    "import path from \"path\";\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\";\n",
    "import process from \"process\"\n",
    "const importNotebook = importer.import\n",
    "const storeLlamaFunction = importNotebook('store llama function')\n",
    "const {getExports, selectAst, cacheCells} = importNotebook([\n",
    "    'select code tree', 'get exports from source', 'cache notebook'])\n",
    "const lookupCell = importer.lookupCell\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE;\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama;\n",
    "let model;\n",
    "\n",
    "async function askLlamaAboutFunctions(query, functions, descriptions, session) {\n",
    "    let q1 = \"Given a list of functions:\\n\";\n",
    "    for(let i = 0; i < functions.length; i++) {\n",
    "        if(descriptions[i]) {\n",
    "            q1 += functions[i] + ' - ' + descriptions[i] + '\\n'\n",
    "        } else {\n",
    "            q1 += functions[i] + '\\n'\n",
    "        }\n",
    "    }\n",
    "    q1 += 'which one most closely matches the query \\\"' + query + '\\\"?'\n",
    "    console.log(\"User: \" + q1);\n",
    "    const a1 = await session.prompt(q1);\n",
    "    console.log(\"AI: \" + a1);\n",
    "    // TODO: parse function name\n",
    "    let result = a1.split('\"').filter(x => functions.indexOf(x) > -1)[0]\n",
    "    return result\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "async function searchLlama(query) {\n",
    "    if(!llama) {\n",
    "        llama = await getLlama();\n",
    "    }\n",
    "    if(!model) {\n",
    "        model = await llama.loadModel({\n",
    "            modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", \"code-llama-7b-chat.gguf\" /*\"Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"*/)\n",
    "        });\n",
    "    }\n",
    "\n",
    "    const context = await model.createContext();\n",
    "    const session = new LlamaChatSession({\n",
    "        contextSequence: context.getSequence()\n",
    "    });\n",
    "\n",
    "    // TODO: use AI to binary/boolean merge search groups of functions for the query\n",
    "    let cellCache = importNotebook('cell cache').cellCache\n",
    "    let functions = []\n",
    "    let descriptions = []\n",
    "    for(let i = 0; i < cellCache.length; i++) {\n",
    "        let cell = cellCache[i]\n",
    "        if(!cell[2].questions || !cell[2].questions[0]) continue\n",
    "        let code = lookupCell(cell[1], cacheCells)\n",
    "        if(code.language != 'javascript') continue\n",
    "        let rpcFunction\n",
    "        try {\n",
    "            rpcFunction = getExports(code.code)[0]\n",
    "        } catch (e) {\n",
    "\n",
    "        }\n",
    "        if(!rpcFunction) continue\n",
    "\n",
    "        functions[functions.length] = rpcFunction\n",
    "        descriptions[descriptions.length] = code.questions.sort((a, b) => b.length - a.length)[0]\n",
    "        \n",
    "        if(functions.length == 20) {\n",
    "            let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "            descriptions = [descriptions[functions.indexOf(result)]]\n",
    "            functions = [result]\n",
    "        }\n",
    "    }\n",
    "    if(functions.length > 1) {\n",
    "        let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "        functions = [result]\n",
    "    }\n",
    "    \n",
    "    console.log('Result: ', functions)\n",
    "    // TODO: load multiple models based on the tool i'm using\n",
    "    \n",
    "    \n",
    "    \n",
    "    const q2 = \"What did you just say?\";\n",
    "    console.log(\"User: \" + q2);\n",
    "    \n",
    "    const a2 = await session.prompt(q2);\n",
    "    console.log(\"AI: \" + a2);\n",
    "}\n",
    "\n",
    "export default searchLlama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan sqlite tools?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan chat logs?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan code history?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "store llama function?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function storeLlamaFunction () {\n",
    "  \n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  storeLlamaFunction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan calendar journal entries?\n",
    "\n",
    "how to ask if my theories are correct?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "javascript"
  },
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
