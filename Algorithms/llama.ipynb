{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "search jupyter notebooks using llama-gpt?\n",
    "\n",
    "search llama?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//import {fileURLToPath} from \"url\";\n",
    "const importNotebook = importer.import\n",
    "const storeLlamaFunction = importNotebook('store llama function')\n",
    "const {getExports, selectAst, cacheCells} = importNotebook([\n",
    "    'select code tree', 'get exports from source', 'cache notebook', 'cache all'])\n",
    "const llama = importNotebook('create llm session')\n",
    "const {askLlamaAboutNotebooks} = importNotebook('ask llm about notebooks')\n",
    "const {askLlamaAboutCode} = importNotebook('ask llm about code')\n",
    "const {askLlamaToSummerize} = importNotebook('ask llm to summerize')\n",
    "const {askLlamaMatchingFunction} = importNotebook('ask llm matching function')\n",
    "\n",
    "async function searchLlama(query, session) {\n",
    "    if(!session) {\n",
    "        let {getSession} = (await llama)\n",
    "        session = await getSession()\n",
    "    }\n",
    "    // TODO: use AI to binary/boolean merge search groups of functions for the query\n",
    "    let cellCache = importNotebook('cell cache').cellCache\n",
    "    let functions = await askLlamaMatchingFunction(query, session)\n",
    "\n",
    "    /*\n",
    "    for(let i = 0; i < cellCache.length; i++) {\n",
    "        let cell = cellCache[i]\n",
    "        if(!cell[2].questions || !cell[2].questions[0]) continue\n",
    "        let code = lookupCell(cell[1], cacheCells)\n",
    "        break;\n",
    "    }\n",
    "    */\n",
    "\n",
    "    console.log('Result: ', functions)\n",
    "    // TODO: load multiple models based on the tool i'm using\n",
    "    \n",
    "    \n",
    "    \n",
    "    const q2 = \"What did you just say?\";\n",
    "    console.log(\"User: \" + q2);\n",
    "    \n",
    "    const a2 = await session.prompt(q2);\n",
    "    console.log(\"AI: \" + a2);\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "    searchLlama\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm which function matches?\n",
    "\n",
    "ask llm matching function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const importNotebook = importer.import\n",
    "const llama = importNotebook('create llm session')\n",
    "const {askLlamaAboutFunctions} = importNotebook('ask llm about functions')\n",
    "const {askLlamaAboutCode} = importNotebook('ask llm about code')\n",
    "const {askLlamaToSummerize} = importNotebook('ask llm to summerize')\n",
    "const lookupCell = importer.lookupCell\n",
    "const {getExports, cacheCells} = importNotebook([\n",
    "  'select code tree', 'get exports from source', 'cache notebook', 'cache all'])\n",
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const { storeLlamaFunction } = importer.import('store llama function')\n",
    "\n",
    "async function askLlamaMatchingFunction(query, session) {\n",
    "  if(!session) {\n",
    "    let {getSession} = (await llama)\n",
    "    session = await getSession()\n",
    "  }\n",
    "\n",
    "  let cellCache = importNotebook('cell cache').cellCache\n",
    "  let functions = []\n",
    "  let descriptions = []\n",
    "\n",
    "  for(let i = 0; i < cellCache.length; i++) {\n",
    "    let cell = cellCache[i]\n",
    "    if(!cell[2].questions || !cell[2].questions[0]) continue\n",
    "    let code = lookupCell(cell[1], cacheCells)\n",
    "    let summary\n",
    "    let shortened\n",
    "    let rpcFunction\n",
    "    if(typeof functionCache[cell[1]] != 'undefined' && code.mtime <= functionCache[cell[1]].mtime) {\n",
    "      summary = functionCache[cell[1]].summary\n",
    "      shortened = functionCache[cell[1]].shortened\n",
    "      rpcFunction = functionCache[cell[1]].rpcFunction\n",
    "    } else {\n",
    "      summary = await askLlamaAboutCode(code.code, session)\n",
    "      shortened = await askLlamaToSummerize(summary, session)\n",
    "      try {\n",
    "        if(code.language == 'javascript')\n",
    "          rpcFunction = getExports(code.code)\n",
    "      } catch (e) {}\n",
    "    }\n",
    "    storeLlamaFunction(cell[1], code.mtime, rpcFunction, summary, shortened.split('-').slice(1).join('-'), shortened.split('-')[0].trim())\n",
    "    if(!rpcFunction) continue\n",
    "    // TODO: insert rpc function into sqlite database to make subsequent lookups faster\n",
    "\n",
    "    functions[functions.length] = rpcFunction[0]\n",
    "    descriptions[descriptions.length] = code.questions.sort((a, b) => b.length - a.length)[0]\n",
    "    \n",
    "    if(functions.length == 20) {\n",
    "        let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "        descriptions = [descriptions[functions.indexOf(result)]]\n",
    "        functions = [result]\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 1) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "      functions = [result]\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaMatchingFunction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ask llm about functions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const importNotebook = importer.import\n",
    "const llama = importNotebook('create llm session')\n",
    "\n",
    "async function askLlamaAboutFunctions(query, functions, descriptions, session) {\n",
    "  if(!session) {\n",
    "    let {getSession} = (await llama)\n",
    "    session = await getSession()\n",
    "  }\n",
    "\n",
    "  let q1 = \"Given a list of functions:\\n\";\n",
    "  for(let i = 0; i < functions.length; i++) {\n",
    "      if(descriptions[i]) {\n",
    "          q1 += functions[i] + '\\n - ' + descriptions[i] + '\\n'\n",
    "      } else {\n",
    "          q1 += functions[i] + '\\n'\n",
    "      }\n",
    "  }\n",
    "  q1 += 'which one most closely matches the query \\\"' + query + '\\\"?'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await session.prompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "  // TODO: parse function name\n",
    "  let result = a1.split(/[\"`*']/gi).filter(x => functions.indexOf(x) > -1)[0]\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutFunctions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ask llm about code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const importNotebook = importer.import\n",
    "const llama = importNotebook('create llm session')\n",
    "\n",
    "\n",
    "async function askLlamaAboutCode(code, session) {\n",
    "  if(!session) {\n",
    "    let {getSession} = (await llama)\n",
    "    session = await getSession()\n",
    "  }\n",
    "\n",
    "  const q2 = \"Give me a very short description of this code?\\n\" + code.substr(0, 2048);\n",
    "  console.log(\"User: \" + q2);\n",
    "\n",
    "  const a2 = await session.prompt(q2);\n",
    "  console.log(\"AI: \" + a2);\n",
    "\n",
    "  return a2\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutCode,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm about notebooks?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const path = require('path')\n",
    "const importNotebook = importer.import\n",
    "const llama = importNotebook('create llm session')\n",
    "const {listInProject} = importer.import('list project files');\n",
    "const {askLlamaAboutFunctions} = importNotebook('ask llm about functions')\n",
    "\n",
    "function findNotebooks(dirname) {\n",
    "    return listInProject(dirname, '{,*,*/,*/*/*,*/*/*/*}*.ipynb')\n",
    "}\n",
    "\n",
    "async function askLlamaAboutNotebooks(query, session) {\n",
    "  if(!session) {\n",
    "    let {getSession} = (await llama)\n",
    "    session = await getSession()\n",
    "  }\n",
    "\n",
    "  let notebooks = findNotebooks(path.resolve(__dirname, '../'))\n",
    "  let functions = []\n",
    "  let descriptions = []\n",
    "\n",
    "  for(let i = 0; i < notebooks.length; i++) {\n",
    "    let notebookName = path.basename(notebooks[i]).replace('.ipynb', '')\n",
    "    let notebookCamel = notebookName.substring(0, 1).toLocaleUpperCase() + notebookName.substring(1)\n",
    "    let folderName = path.basename(path.dirname(notebooks[i]))\n",
    "    let folderCamel = folderName.substring(0, 1).toLocaleUpperCase() + folderName.substring(1)\n",
    "\n",
    "    functions[functions.length] = 'search' + folderCamel.replace(/[^a-z0-9]/gi, '') + notebookCamel.replace(/[^a-z0-9]/gi, '').substring(0, 15)\n",
    "    descriptions[descriptions.length] = folderCamel + ' ' + notebookCamel\n",
    "\n",
    "    // TODO: convert this to something like function.length + description.length + 4 < 2048\n",
    "    if(functions.length == 20) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "      if(result) {\n",
    "        descriptions = [descriptions[functions.indexOf(result)]]\n",
    "        functions = [result]\n",
    "      } else {\n",
    "        descriptions = []\n",
    "        functions = []\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 1) {\n",
    "    let result = await askLlamaAboutFunctions(query, functions, descriptions, session)\n",
    "    if(result) functions = [result]\n",
    "    else functions = []\n",
    "  }\n",
    "\n",
    "  return functions\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutNotebooks,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm to summerize?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const importNotebook = importer.import\n",
    "const llama = importNotebook('create llm session')\n",
    "\n",
    "\n",
    "async function askLlamaToSummerize(query, session) {\n",
    "  if(!session) {\n",
    "    let {getSession} = (await llama)\n",
    "    session = await getSession()\n",
    "  }\n",
    "\n",
    "  let q1 = 'Summerize this into one or two sentences:\\n' + query\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await session.prompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "\n",
    "  let q2 = 'How would you categorize this in two or three words:\\n' + a1\n",
    "  console.log(\"User: \" + q2);\n",
    "  const a2 = await session.prompt(q2);\n",
    "  console.log(\"AI: \" + a2);\n",
    "\n",
    "  return a2 + ' - ' + a1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaToSummerize\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan sqlite tools?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan chat logs?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan code history?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "store llama function?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var { functionCache, updateCode } = importer.import([\n",
    "  'cache rpc functions with llm descriptions', 'update code cell'\n",
    "])\n",
    "\n",
    "function storeLlamaFunction (cellId, mtime, exports, description, summary, categories) {\n",
    "  if(typeof functionCache != 'object') {\n",
    "    functionCache = {}\n",
    "  }\n",
    "  functionCache[cellId] = {\n",
    "    mtime,\n",
    "    exports,\n",
    "    description,\n",
    "    summary,\n",
    "    categories\n",
    "  }\n",
    "  var code = `\n",
    "// cell cache automatically replaced\n",
    "var functionCache = ${JSON.stringify(functionCache, null, 4)}\n",
    "\n",
    "module.exports = {\n",
    "  functionCache\n",
    "}\n",
    "`\n",
    "  var cacheCell = importer.interpret('cache rpc functions with llm descriptions')\n",
    "  updateCode(cacheCell, code)\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  storeLlamaFunction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan calendar journal entries?\n",
    "\n",
    "how to ask if my theories are correct?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "create llm session?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'gemma-2-9b-it-Q6_K-Q8.gguf' /* \"code-llama-7b-chat.gguf\"*/ /*\"Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"*/)\n",
    "      });\n",
    "  }\n",
    "\n",
    "  context = await model.createContext();\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence()\n",
    "  });\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  getSession,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "javascript"
  },
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
