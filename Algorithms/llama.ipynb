{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search jupyter notebooks using llama-gpt?\n",
    "\n",
    "search llama?\n",
    "\n",
    "\n",
    "ask llm which function matches?\n",
    "\n",
    "ask llm matching function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const {askLlamaAboutFunctions} = importer.import('ask llm about functions')\n",
    "const {askLlamaAboutCategories} = importer.import('ask llm about categories')\n",
    "const lookupCell = importer.lookupCell\n",
    "const { cacheCells } = importer.import('cache notebook')\n",
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const { cellCache } = importer.import('cell cache')\n",
    "const {storeAllLlamaFunctions} = importer.import('store all notebook llm functions')\n",
    "\n",
    "async function askLlamaMatchingFunction(query) {\n",
    "  await storeAllLlamaFunctions()\n",
    "\n",
    "  let filterCategories = await askLlamaAboutCategories(query)\n",
    "  if(filterCategories.length > 0) {\n",
    "    console.log(filterCategories)\n",
    "  }\n",
    "\n",
    "  let matchingFunction = []\n",
    "  let functions = []\n",
    "  let descriptions = []\n",
    "\n",
    "  for(let i = 0; i < cellCache.length; i++) {\n",
    "    let cell = cellCache[i]\n",
    "    if(!cell[2].questions || !cell[2].questions[0]) continue\n",
    "    let code = lookupCell(cell[1], cacheCells)\n",
    "\n",
    "    if(!functionCache[cell[1]].exports) continue\n",
    "    if(!functionCache[cell[1]].exports[0]) continue\n",
    "\n",
    "    // limit number of calls based on filter\n",
    "    if(filterCategories.length > 0 && !filterCategories.includes(code.filename)) {\n",
    "      continue\n",
    "    }\n",
    "\n",
    "    // finally append to array that will be used to find the function\n",
    "    functions[functions.length] = functionCache[cell[1]].exports[0]\n",
    "    descriptions[descriptions.length] = functionCache[cell[1]].summary\n",
    "\n",
    "    if(functions.length == 20) {\n",
    "        let result = await askLlamaAboutFunctions(query, functions, descriptions)\n",
    "        descriptions = []\n",
    "        functions = []\n",
    "        if(result)\n",
    "          matchingFunction = matchingFunction.concat(result)\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 0) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, descriptions)\n",
    "      descriptions = []\n",
    "      functions = []\n",
    "      if(result)\n",
    "        matchingFunction = matchingFunction.concat(result)\n",
    "  }\n",
    "  return matchingFunction\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaMatchingFunction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm about categories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const {askLlamaAboutFunctions} = importer.import('ask llm about functions')\n",
    "\n",
    "\n",
    "async function askLlamaAboutCategories(query) {\n",
    "  // TODO: list all categories in database\n",
    "  let keys = Object.keys(functionCache)\n",
    "  let categorys = keys.map(k => functionCache[k].categories)\n",
    "    .concat(keys.map(k => functionCache[k].category))\n",
    "    .filter((a, i, arr) => a && arr.indexOf(a) == i && !a.includes('\\n'))\n",
    "\n",
    "  // TODO: ask llm if any of the categories match, don't choose best one, choose all matches\n",
    "  let returnValues = []\n",
    "  let functions = []\n",
    "  for(let i = 0; i < categorys.length; i++) {\n",
    "    let category = categorys[i]\n",
    "    functions[functions.length] = category\n",
    "\n",
    "    if(functions.length == 20) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, [], true)\n",
    "      functions = []\n",
    "      if(result)\n",
    "        returnValues = returnValues.concat(result)\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 0) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, [], true)\n",
    "      functions = []\n",
    "      if(result)\n",
    "        returnValues = returnValues.concat(result)\n",
    "  }\n",
    "\n",
    "  // TODO: return notebook filenames that contain matching categories\n",
    "  let matching = keys.filter(k => returnValues.includes(functionCache[k].category) || returnValues.includes(functionCache[k].categories))\n",
    "    .map(k => k.replace(/\\[[0-9]*\\]/, ''))\n",
    "    .filter((a, i, arr) => a && arr.indexOf(a) == i)\n",
    "  return matching\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutCategories\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm to generalize categories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const { askLlamaAboutFunctions } = importer.import('ask llm about functions')\n",
    "const { storeLlamaFunction } = importer.import('store llama function')\n",
    "const { askLlamaToGeneralizeAll } = importer.import('ask llm for a shorter list of categories')\n",
    "\n",
    "async function askLlamaGeneralizeCategories(categories) {\n",
    "  let update = false\n",
    "  let keys = Object.keys(functionCache)\n",
    "  if(!categories) {\n",
    "    categories = keys.map(k => functionCache[k].categories)\n",
    "      .filter((a, i, arr) => a && arr.indexOf(a) == i && !a.includes('\\n'))\n",
    "    update = true\n",
    "  }\n",
    "\n",
    "  if(categories.length == 0) {\n",
    "    return []\n",
    "  }\n",
    "\n",
    "  let uniqueValues = await askLlamaToGeneralizeAll(categories)\n",
    "  console.log(uniqueValues)\n",
    "\n",
    "  if(uniqueValues.length > 100) {\n",
    "    uniqueValues = (await askLlamaGeneralizeCategories(uniqueValues))\n",
    "      .filter((a, i, arr) => arr.indexOf(a) == i)\n",
    "  }\n",
    "\n",
    "  if(update) {\n",
    "    let convertedList = []\n",
    "    for(let i = 0; i < keys.length; i++) {\n",
    "      let newCategory = await askLlamaAboutFunctions(functionCache[keys[i]].categories, uniqueValues, [], true)\n",
    "      if(newCategory) {\n",
    "        storeLlamaFunction(keys[i], functionCache[keys[i]].mtime, \n",
    "          functionCache[keys[i]].exports, functionCache[keys[i]].description, \n",
    "          functionCache[keys[i]].summary, functionCache[keys[i]].categories, newCategory)\n",
    "      }\n",
    "      convertedList[i] = newCategory\n",
    "    }\n",
    "    console.log(convertedList)\n",
    "  }\n",
    "\n",
    "  return uniqueValues\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaGeneralizeCategories\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm for a shorter list of categories?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async function askLlamaToGeneralize(categories) {\n",
    "  const {llmPrompt} = await importer.import('create llm session')\n",
    "  let q1 = 'Generalize this list of categories into a shorter list:\\n' \n",
    "    + categories.join('\\n') + '\\nOnly return the list and nothing else.'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "\n",
    "  return a1.split(/\\s*\\n\\s*|,\\s*|\\s*- |\\s*\\* /gi)\n",
    "    .filter((a, i, arr) => a.trim().length > 0 /*&& arr.indexOf(a) == i*/)\n",
    "}\n",
    "\n",
    "async function askLlamaToGeneralizeAll(categories, groupSize = 10) {\n",
    "  let resultValues = []\n",
    "  let functions = []\n",
    "  for(let i = 0; i < categories.length; i++) {\n",
    "    let category = categories[i]\n",
    "    functions[functions.length] = category\n",
    "\n",
    "    if(functions.length == groupSize) {\n",
    "      let result = await askLlamaToGeneralize(functions)\n",
    "      functions = []\n",
    "      resultValues = resultValues.concat(result)\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 0) {\n",
    "    let result = await askLlamaToGeneralize(functions)\n",
    "    functions = []\n",
    "    resultValues = resultValues.concat(result)\n",
    "  }\n",
    "\n",
    "  // TODO: if the list is still longer than say 100, call this function recursively until its short, haha\n",
    "  let uniqueValues = resultValues.filter((a, i, arr) => arr.indexOf(a) == i)\n",
    "  return uniqueValues\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaToGeneralizeAll,\n",
    "  askLlamaToGeneralize\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ask llm about functions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async function askLlamaAboutFunctions(query, functions, descriptions, categories = false) {\n",
    "  const {llmPrompt} = await importer.import('create llm session')\n",
    "\n",
    "  let q1 = \"Given a list of \" + (categories ? 'categories' : \"functions\") + \":\\n\";\n",
    "  for(let i = 0; i < functions.length; i++) {\n",
    "      if(descriptions[i]) {\n",
    "          q1 += functions[i] + ' - ' + descriptions[i] + '\\n'\n",
    "      } else {\n",
    "          q1 += functions[i] + '\\n'\n",
    "      }\n",
    "  }\n",
    "  q1 += 'which one most closely matches the query \\\"' + query + '\\\"?'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "  // TODO: parse function name\n",
    "  let result = a1.trim().split(/[\"`*'\\n]/gi).filter(x => functions.indexOf(x) > -1)[0]\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutFunctions,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ask llm about code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async function askLlamaAboutCode(code) {\n",
    "  const {llmPrompt} = await importer.import('create llm session')\n",
    "\n",
    "  const q2 = \"Give me a short breakdown of this code:\\n\" + code.substr(0, 2048) + \"\\nDocumentation only, discard any friendly remarks.\";\n",
    "  console.log(\"User: \" + q2);\n",
    "\n",
    "  const a2 = await llmPrompt(q2);\n",
    "  console.log(\"AI: \" + a2);\n",
    "\n",
    "  return a2.trim()\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutCode,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm about notebooks?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "const path = require('path')\n",
    "const {listInProject} = importer.import('list project files');\n",
    "const {askLlamaAboutFunctions} = importer.import('ask llm about functions')\n",
    "\n",
    "function findNotebooks(dirname) {\n",
    "    return listInProject(dirname, '{,*,*/,*/*/*,*/*/*/*}*.ipynb')\n",
    "}\n",
    "\n",
    "async function askLlamaAboutNotebooks(query) {\n",
    "  let notebooks = findNotebooks(path.resolve(__dirname, '../'))\n",
    "  let functions = []\n",
    "  let descriptions = []\n",
    "\n",
    "  for(let i = 0; i < notebooks.length; i++) {\n",
    "    let notebookName = path.basename(notebooks[i]).replace('.ipynb', '')\n",
    "    let notebookCamel = notebookName.substring(0, 1).toLocaleUpperCase() + notebookName.substring(1)\n",
    "    let folderName = path.basename(path.dirname(notebooks[i]))\n",
    "    let folderCamel = folderName.substring(0, 1).toLocaleUpperCase() + folderName.substring(1)\n",
    "\n",
    "    functions[functions.length] = 'search' + folderCamel.replace(/[^a-z0-9]/gi, '') + notebookCamel.replace(/[^a-z0-9]/gi, '').substring(0, 15)\n",
    "    descriptions[descriptions.length] = folderCamel + ' ' + notebookCamel\n",
    "\n",
    "    // TODO: convert this to something like function.length + description.length + 4 < 2048\n",
    "    if(functions.length == 20) {\n",
    "      let result = await askLlamaAboutFunctions(query, functions, descriptions)\n",
    "      if(result) {\n",
    "        descriptions = [descriptions[functions.indexOf(result)]]\n",
    "        functions = [result]\n",
    "      } else {\n",
    "        descriptions = []\n",
    "        functions = []\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 1) {\n",
    "    let result = await askLlamaAboutFunctions(query, functions, descriptions)\n",
    "    if(result) functions = [result]\n",
    "    else functions = []\n",
    "  }\n",
    "\n",
    "  return functions\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutNotebooks,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm to summerize?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async function askLlamaToSummerize(query) {\n",
    "  const {llmPrompt} = await importer.import('create llm session')\n",
    "  let q1 = 'Summerize this into one or two sentences:\\n' + query\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "\n",
    "  return a1.trim()\n",
    "}\n",
    "\n",
    "async function askLlamaToGeneralize(query) {\n",
    "  const {llmPrompt} = await importer.import('create llm session')\n",
    "  let q2 = 'How would you categorize this in two or three words:\\n' + query + '\\nReturn only the category.'\n",
    "  console.log(\"User: \" + q2);\n",
    "  const a2 = await llmPrompt(q2);\n",
    "  console.log(\"AI: \" + a2);\n",
    "\n",
    "  return a2.trim()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaToSummerize,\n",
    "  askLlamaToGeneralize\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "store llama function?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const { updateCode } = importer.import('update code cell')\n",
    "\n",
    "function storeLlamaFunction (cellId, mtime, exports, description, summary, categories, category) {\n",
    "  functionCache[cellId] = {\n",
    "    mtime,\n",
    "    exports,\n",
    "    description,\n",
    "    summary,\n",
    "    categories,\n",
    "    category\n",
    "  }\n",
    "  var code = `\n",
    "// cell cache automatically replaced\n",
    "var functionCache = ${JSON.stringify(functionCache, null, 4)}\n",
    "\n",
    "module.exports = {\n",
    "  functionCache\n",
    "}\n",
    "`\n",
    "  var cacheCell = importer.interpret('cache rpc functions with llm descriptions')\n",
    "  updateCode(cacheCell, code)\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  storeLlamaFunction\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "store all notebook llm functions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const {askLlamaAboutCode} = importer.import('ask llm about code')\n",
    "const {askLlamaToSummerize, askLlamaToGeneralize} = importer.import('ask llm to summerize')\n",
    "const {getExports, cacheCells} = importer.import([\n",
    "  'select code tree', 'get exports from source', 'cache notebook', 'cache all'])\n",
    "const { functionCache } = importer.import('cache rpc functions with llm descriptions')\n",
    "const { storeLlamaFunction } = importer.import('store llama function')\n",
    "\n",
    "async function storeAllLlamaFunctions() {\n",
    "  const getParameters = await importer.import('get c parameters')\n",
    "  const pythonParams = await importer.import('python params in antlr')\n",
    "  let cellCache = importer.import('cell cache').cellCache\n",
    "  for(let i = 0; i < cellCache.length; i++) {\n",
    "    let cell = cellCache[i]\n",
    "    //if(!cell[2].questions || !cell[2].questions[0]) continue\n",
    "    let code = importer.lookupCell(cell[1], cacheCells)\n",
    "    if(code.code.trim().length == 0) {\n",
    "      storeLlamaFunction(cell[1], code.mtime, [], '', '', '', '')\n",
    "      continue\n",
    "    }\n",
    "\n",
    "    let summary\n",
    "    let shortened\n",
    "    let rpcFunction\n",
    "    let categories\n",
    "    let category\n",
    "    let fresh = false\n",
    "\n",
    "    if(typeof functionCache[cell[1]] != 'undefined') {\n",
    "      if(\n",
    "        // notebook hasn't changed\n",
    "        code.mtime <= functionCache[cell[1]].mtime\n",
    "        // don't both updating cache notebooks, \n",
    "        // otherwise this will run every time any notebook changes\n",
    "        || code.filename.match(/cache/gi)\n",
    "      ) {\n",
    "        summary = functionCache[cell[1]].description\n",
    "        shortened = functionCache[cell[1]].summary\n",
    "        rpcFunction = functionCache[cell[1]].exports\n",
    "      }\n",
    "\n",
    "      categories = functionCache[cell[1]].categories\n",
    "      category = functionCache[cell[1]].categories\n",
    "    } else {\n",
    "      fresh = true\n",
    "    }\n",
    "\n",
    "    // needs cleanup\n",
    "    if(!summary || !categories || (categories + '').includes('\\n')\n",
    "      || summary.length < 256 || summary.match(/Find the derivative/gi) \n",
    "      || shortened.match(/Find the derivative/gi)\n",
    "      || categories.match(/Code analysis request/gi)) {\n",
    "      // TODO: this should cause the erroneous cell to show up every time and for these to be fixed next pass\n",
    "      summary = await askLlamaAboutCode(code.code)\n",
    "      shortened = await askLlamaToSummerize(summary)\n",
    "      categories = await askLlamaToGeneralize(summary)\n",
    "      fresh = true\n",
    "    }\n",
    "    if(typeof rpcFunction == 'undefined') {\n",
    "      try {\n",
    "        if(code.language == 'javascript')\n",
    "          rpcFunction = getExports(code.code)\n",
    "        if(code.language == 'c' || code.language == 'cpp')\n",
    "          rpcFunction = (await getParameters(code.code)).map(p => typeof p == 'string' ? p : p[0])\n",
    "        if(code.language == 'python') {\n",
    "          const params = await pythonParams(code.code)\n",
    "          rpcFunction = typeof params.function != 'undefined' ? [params.function] : params.map(p => p.function)\n",
    "        }\n",
    "        fresh = true\n",
    "      } catch (e) {\n",
    "        rpcFunction = []\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if(fresh) {\n",
    "      // TODO: insert rpc function into sqlite database to make subsequent lookups faster\n",
    "      storeLlamaFunction(cell[1], code.mtime, rpcFunction, summary, shortened, categories, category)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  storeAllLlamaFunctions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "create llm session?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##large language model\\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession(modelPath) {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "\n",
    "  // TODO: customizable model here\n",
    "\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          // \"code-llama-7b-chat.gguf\"\n",
    "          // \"Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"\n",
    "          // 'ggml-model-f16.gguf'\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'ggml-model-f16.gguf' ),\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf' ),\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'gemma-2-9b-it-Q6_K-Q8.gguf' ),\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'deepseek-llm-7b-chat.Q5_K_M.gguf' ),\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'Meta-Llama-3.1-70B-Instruct-Q5_K_S.gguf' ),\n",
    "          modelPath: process.env.MODELPATH || modelPath || path.join(HOMEPATH, \"llama.cpp\", \"models\", 'Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf' ),\n",
    "          //modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'deepseek-llm-7b-chat.Q6_K.gguf' ),\n",
    "          contextSize: 2048\n",
    "      })\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession(model) {\n",
    "  if(!session) {\n",
    "    await createSession(model)\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmPrompt(prompt, model) {\n",
    "  if(!model || typeof model == 'string') {\n",
    "    // specify model with the incoming session\n",
    "    model = await getSession(model)\n",
    "  }\n",
    "  let result = await model.prompt(prompt, {\n",
    "    temperature: 0.8,\n",
    "    maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  // special fix for deepseek R1 distill\n",
    "  if(result.match(/Find the derivative/gi)) {\n",
    "    console.log('Response error', result)\n",
    "    await initSession()\n",
    "    result = await session.prompt(prompt, {\n",
    "      temperature: 0.8,\n",
    "      maxTokens: context.contextSize,\n",
    "      onTextChunk: function (text) {\n",
    "        process.stdout.write(text)\n",
    "      }\n",
    "    })\n",
    "  }\n",
    "  //if(result.includes('</think>')) {\n",
    "  //  result = result.split('</think>\\n')[1]\n",
    "  //}\n",
    "  if(result.startsWith('```') && result.endsWith('````')) {\n",
    "    result = result.replace(/^```(markdown)*\\n*|\\n*```$/gi, '')\n",
    "  }\n",
    "  if(session == model)\n",
    "    model.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  llmPrompt,\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "javascript"
  },
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
