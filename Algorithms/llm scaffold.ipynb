{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "try to make this clever\n",
    "\n",
    "llm scaffold?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async function llmScaffold(github) {\n",
    "\n",
    "  if(!github) {\n",
    "    console.error('Provide a github repository for comparison and updating')\n",
    "  }\n",
    "\n",
    "  // TODO: feed the llm the proper project components as necessary to get it to generate code files\n",
    "  \n",
    "\n",
    "}\n",
    "\n",
    "module.exports = llmScaffold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##helpful coding module\\nnamed Llama that responds to every request using beautifully crafted `markdown`. Return only a single code block in the specified language without reasoning or instructions if requested.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmCode(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmCode,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "bash project files?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async function generateBash(github) {\n",
    "\n",
    "  // TODO: local pull\n",
    "\n",
    "  // TODO: generate bash\n",
    "\n",
    "  // TODO: mount project directory\n",
    "\n",
    "  // TODO: execute new docker file on project\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = generateBash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "generate code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "async function generateCode(github) {\n",
    "  \n",
    "  // TODO: local pull\n",
    "\n",
    "  // TODO: compare existing project files\n",
    "\n",
    "  // TODO: create new files\n",
    "\n",
    "  // TODO: populate code files with goals\n",
    "\n",
    "  // TODO: populate goals with actual code\n",
    "\n",
    "}\n",
    "\n",
    "module.exports = generateCode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
