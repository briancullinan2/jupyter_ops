{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan sqlite tools?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan chat logs?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "//import {fileURLToPath} from \"url\";\n",
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const importNotebook = importer.import\n",
    "const {askLlamaAboutEmotions} = importNotebook('ask llm about emotions')\n",
    "const {storeChatHistory} = importNotebook('how to cache chat logs')\n",
    "//const { chatCache } = importer.import('cache chat history with llm descriptions')\n",
    "const { askLlamaAboutConversation, askLlamaAboutCategory } = importNotebook('ask llm about chat conversations')\n",
    "const llama = importNotebook('create llm session')\n",
    "\n",
    "const hashCode = s => s.split('').reduce((a,b) => (((a << 5) - a) + b.charCodeAt(0))|0, 0)\n",
    "\n",
    "const CHAT_DIRECTORIES = [\n",
    "  '/Volumes/External/Personal/Collections/conversations',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/AIM/Query',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/MSN/Query',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/old logs/AIM',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/old logs/MSN',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/older logs/AIM',\n",
    "  '/Volumes/External/Personal/Collections/conversations/Trillian/logs/older logs/MSN',\n",
    "]\n",
    "\n",
    "const CHAT_DATES = [\n",
    "  /(January|February|March|April|May|June|July|August|September|October|November|December) (\\d{1,2}), \\d{4}/gi,\n",
    "  /(0[1-9]|[12]\\d|3[01])\\.(0[1-9]|1[0-2])\\.\\d{4}/gi,\n",
    "  /(0[1-9]|1[0-2])\\/(0[1-9]|[12]\\d|3[01])\\/\\d{4}/gi,\n",
    "  /\\d{4}-(0[1-9]|1[0-2])-(0[1-9]|[12]\\d|3[01])/gi,\n",
    "  /(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) (\\d{1,2}), \\d{4}/gi\n",
    "]\n",
    "\n",
    "const CHAT_TIMES = [\n",
    "  /(0[1-9]|1[0-2]):([0-5]\\d):([0-5]\\d)\\s?(AM|PM)/gi,\n",
    "  /([01]\\d|2[0-3]):([0-5]\\d):([0-5]\\d)/gi,\n",
    "  /(0[1-9]|1[0-2]):([0-5]\\d)\\s?(AM|PM)/gi,\n",
    "  /([01]\\d|2[0-3]):([0-5]\\d)/gi\n",
    "]\n",
    "\n",
    "const CHAT_PATH = path.join(__dirname, '..', 'Resources', 'Projects', 'conversations')\n",
    "\n",
    "function cellNeedsTidying(cellId, chatCache) {\n",
    "  return typeof chatCache[cellId] == 'undefined'\n",
    "  || typeof chatCache[cellId].emotions == 'undefined'\n",
    "  || typeof chatCache[cellId].category != 'string'\n",
    "  || typeof chatCache[cellId].date == 'undefined'\n",
    "  || !chatCache[cellId].date\n",
    "  || chatCache[cellId].summary.match(/Find the derivative/gi)\n",
    "  || chatCache[cellId].category == '<h1>'\n",
    "  || typeof chatCache[cellId].from == 'undefined' || typeof chatCache[cellId].to == 'undefined'\n",
    "}\n",
    "\n",
    "function convertMessagesToNoDates(currentMessages) {\n",
    "  let messageNoTimes = currentMessages\n",
    "  if(typeof currentMessages != 'string') {\n",
    "    messageNoTimes = messageNoTimes.join('\\n').trim()\n",
    "  }\n",
    "  for(let i = 0; i < CHAT_DATES.length; i++) {\n",
    "    messageNoTimes = messageNoTimes.replace(CHAT_DATES[i], '')\n",
    "  }\n",
    "  for(let i = 0; i < CHAT_TIMES.length; i++) {\n",
    "    messageNoTimes = messageNoTimes.replace(CHAT_TIMES[i], '')\n",
    "  }\n",
    "  messageNoTimes = messageNoTimes.replace(/\\[\\s*\\]\\s*/gi, '')\n",
    "  return messageNoTimes.split('\\n')\n",
    "}\n",
    "\n",
    "async function askLlamaAboutChatLog(chatLog) {\n",
    "  const {llmPrompt} = (await llama)\n",
    "\n",
    "  if(!chatLog) {\n",
    "    chatLog = 'fuji897@hotmail.com'\n",
    "  }\n",
    "\n",
    "  let chatHistory = []\n",
    "  let chatPath\n",
    "  for(let i = 0; i < CHAT_DIRECTORIES.length; i++) {\n",
    "    chatPath = path.join(CHAT_DIRECTORIES[i], chatLog + '.log')\n",
    "    if(fs.existsSync(chatPath)) {\n",
    "      chatHistory = fs.readFileSync(chatPath).toLocaleString('utf-8').split('\\n')\n",
    "      break\n",
    "    } else {\n",
    "      chatPath = void 0\n",
    "    }\n",
    "  }\n",
    "  if(!chatPath) return\n",
    "\n",
    "  let chatCacheFile = path.join(CHAT_PATH, path.basename(chatPath).replace('.log', '') + '.json')\n",
    "  let chatCache = {}\n",
    "  if(fs.existsSync(chatCacheFile))\n",
    "    chatCache = JSON.parse(fs.readFileSync(chatCacheFile).toString('utf-8'))\n",
    "\n",
    "  let from = 0\n",
    "  let mtime = fs.statSync(chatPath).mtime.getTime()\n",
    "  let previousTime = mtime\n",
    "  let cellCounter = 0\n",
    "  let currentMessages = []\n",
    "  let currentTotal = 0\n",
    "  let cellId = chatPath + '[' + cellCounter + ']'\n",
    "  for(let i = 0; i < chatHistory.length; i++) {\n",
    "    if(currentMessages.length > 0 && currentTotal + chatHistory[i].length > 2048 || currentMessages.length == 20) {\n",
    "      // process now\n",
    "      // TODO: store result\n",
    "      let messageBlock = currentMessages.join('\\n')\n",
    "      let messageNoTimes = convertMessagesToNoDates(currentMessages)\n",
    "      let date = CHAT_DATES.map(d => (messageBlock.match(d) || [])[0]).filter(d=>d)[0]\n",
    "      let time = CHAT_TIMES.map(d => (messageBlock.match(d) || [])[0]).filter(d=>d)[0]\n",
    "      let now = new Date(date + ' ' + time).getTime() || previousTime\n",
    "      if(date && time)\n",
    "        previousTime = now\n",
    "      let hash = hashCode(messageBlock)\n",
    "      if(cellNeedsTidying(cellId, chatCache) || chatCache[cellId].hash != hash) {\n",
    "        let summary = await askLlamaAboutConversation(messageNoTimes)\n",
    "        let emotions = await askLlamaAboutEmotions(messageNoTimes)\n",
    "        let category = await askLlamaAboutCategory(messageNoTimes)\n",
    "        storeChatHistory(cellId, mtime, summary, emotions, category, hash, now, from, i - 1)\n",
    "      }\n",
    "      currentMessages = []\n",
    "      currentTotal = 0\n",
    "      from = i\n",
    "      cellCounter++\n",
    "      cellId = chatPath + '[' + cellCounter + ']'\n",
    "    }\n",
    "\n",
    "    if(chatHistory[i].trim().length == 0) continue\n",
    "\n",
    "    currentMessages[currentMessages.length] = chatHistory[i]\n",
    "    currentTotal += chatHistory[i].length\n",
    "  }\n",
    "  if(currentMessages.length > 0) {\n",
    "    // process now\n",
    "    let messageBlock = currentMessages.join('\\n')\n",
    "    let messageNoTimes = convertMessagesToNoDates(currentMessages)\n",
    "    let date = CHAT_DATES.map(d => (messageBlock.match(d) || [])[0]).filter(d=>d)[0]\n",
    "    let time = CHAT_TIMES.map(d => (messageBlock.match(d) || [])[0]).filter(d=>d)[0]\n",
    "    let now = new Date(date + ' ' + time).getTime()\n",
    "    let hash = hashCode(messageBlock)\n",
    "    if(cellNeedsTidying(cellId, chatCache) || chatCache[cellId].hash != hash) {\n",
    "      let summary = await askLlamaAboutConversation(messageNoTimes)\n",
    "      let emotions = await askLlamaAboutEmotions(messageNoTimes)\n",
    "      let category = await askLlamaAboutCategory(messageNoTimes)\n",
    "      storeChatHistory(cellId, mtime, summary, emotions, category, hash, now, from, chatHistory.length - 1)\n",
    "    }\n",
    "    // TODO: store result\n",
    "  }\n",
    "\n",
    "  \n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutChatLog,\n",
    "  askLlamaAboutConversation,\n",
    "  convertMessagesToNoDates,\n",
    "  CHAT_DIRECTORIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm about emotions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const llama = importer.import('create llm session')\n",
    "\n",
    "const EMOTIONS = [\n",
    "  'Joyful',\n",
    "  'Romantic',\n",
    "  'Angry',\n",
    "  'Confused',\n",
    "  'Supportive',\n",
    "  'Excited',\n",
    "  'Nostalgic',\n",
    "  'Grateful',\n",
    "  'Sad',\n",
    "  'Humorous',\n",
    "  'Anxious',\n",
    "  'Curious',\n",
    "  'Inspired',\n",
    "  'Defensive',\n",
    "  'Assertive',\n",
    "  'Empathetic',\n",
    "  'Reflective',\n",
    "  'Playful',\n",
    "  'Hopeful',\n",
    "  'Apologetic',\n",
    "  'Lonely',\n",
    "  'Proud',\n",
    "  'Vulnerable',\n",
    "  'Determined',\n",
    "  'Aroused',\n",
    "  'Neutral',\n",
    "  'Dissident',\n",
    "  'Rebelious',\n",
    "  'Frustrated',\n",
    "  'Helpful',\n",
    "  'Enthusiastic',\n",
    "  'Casual',\n",
    "  'Enthusiasm',\n",
    "  'Annoyed',\n",
    "  'Touched',\n",
    "  'Apologetic',\n",
    "  'Regret',\n",
    "  'Regretful',\n",
    "  'Content',\n",
    "  'Insecure',\n",
    "  'Concerned',\n",
    "  'Erotic',\n",
    "  'Vulgar',\n",
    "  'Bored',\n",
    "  'Confused',\n",
    "  'Caring',\n",
    "  'Hesitant',\n",
    "  'Flirty',\n",
    "  'Flirtatious',\n",
    "  'Eager',\n",
    "  'Attentive',\n",
    "  'Affectionate',\n",
    "  'Charming',\n",
    "  'Confident',\n",
    "  'Smug',\n",
    "  'Embarrassed',\n",
    "  'Nervous',\n",
    "  'Thoughtful',\n",
    "  'Apprehensive',\n",
    "  'Tired',\n",
    "  'Amused',\n",
    "  'Flustered',\n",
    "  'Exasperated',\n",
    "  'Longing',\n",
    "  'Happy',\n",
    "  'Uncertain',\n",
    "  'Understanding',\n",
    "  'Encouraging',\n",
    "  'Upset',\n",
    "  'Worried',\n",
    "  'Self-Conscious',\n",
    "  'Sarcastic',\n",
    "  'Teasing',\n",
    "  'Competitive',\n",
    "  'Serious',\n",
    "  'Impressed',\n",
    "  'Amazed',\n",
    "  'Suggestive',\n",
    "  'Needy',\n",
    "  'Relatable',\n",
    "  'Sexual',\n",
    "  'Horny',\n",
    "  'Flattered',\n",
    "  'Intrigued',\n",
    "  'Lighthearted',\n",
    "  'Relieved',\n",
    "  'Protective',\n",
    "  'Apathetic',\n",
    "  'Distracted',\n",
    "  'Reassured',\n",
    "  'Detached',\n",
    "  'Numb',\n",
    "  'Optimistic',\n",
    "  'Passionate',\n",
    "  'Amusing',\n",
    "  'Dismissive',\n",
    "  'Disappointed',\n",
    "  'Resigned',\n",
    "  'Sympathetic',\n",
    "  'Open',\n",
    "  'Disgruntled',\n",
    "  'Guilty',\n",
    "  'Awkward',\n",
    "  'Knowledgeable',\n",
    "  'Disengaged',\n",
    "  'Interested',\n",
    "  'Surprised',\n",
    "  'Downcast',\n",
    "  'Observant',\n",
    "  'Stressful',\n",
    "  'Hurt',\n",
    "  'Self-Doubt',\n",
    "  'Overwhelmed',\n",
    "  'Yearning',\n",
    "  'Desireous',\n",
    "  'Loving',\n",
    "  'Despondent',\n",
    "  'Unheard',\n",
    "  'Hopeless',\n",
    "  'Remorseful',\n",
    "  'Lewd',\n",
    "  'Amusement',\n",
    "  'Critical',\n",
    "  'Lust',\n",
    "  'Manipulative',\n",
    "  'Matter-of-fact',\n",
    "  'Task-Oriented',\n",
    "  'Transitional',\n",
    "  'Impatient',\n",
    "  'Negative',\n",
    "  'Dissatisfied',\n",
    "  'Indecisive',\n",
    "  'Preference',\n",
    "  'Assertiveness',\n",
    "  'Anger',\n",
    "  'Contempt',\n",
    "  'Disdain',\n",
    "  'Threat',\n",
    "  'Apathy',\n",
    "  'Condescending',\n",
    "  'Disbelief',\n",
    "  'Reassurance',\n",
    "  'Determination',\n",
    "  'Informative',\n",
    "  'Dismissiveness',\n",
    "  'Resignation',\n",
    "  'Businesslike',\n",
    "  'Cooperative',\n",
    "  'Skeptical',\n",
    "  'Shock',\n",
    "  'Infatuation',\n",
    "  'Friendly',\n",
    "  'Indifferent',\n",
    "  'Explanatory',\n",
    "  'Appreciative',\n",
    "  'Neediness',\n",
    "  'Contentment',\n",
    "  \n",
    "]\n",
    "\n",
    "const EMOTION_HEX = [\n",
    "  '#FFFF00',\n",
    "  '#FFC0CB',\n",
    "  '#FF0000',\n",
    "  '#D8BFD8',\n",
    "  '#90EE90',\n",
    "  '#FFA500',\n",
    "  '#4682B4',\n",
    "  '#FFD700',\n",
    "  '#00008B',\n",
    "  '#FFFF99',\n",
    "  '#800080',\n",
    "  '#40E0D0',\n",
    "  '#FFC107',\n",
    "  '#800000',\n",
    "  '#FF4500',\n",
    "  '#E6E6FA',\n",
    "  '#B0C4DE',\n",
    "  '#00FFFF',\n",
    "  '#FFFACD',\n",
    "  '#B0E0E6',\n",
    "  '#191970',\n",
    "  '#FFD700',\n",
    "  '#FFE4E1',\n",
    "  '#DC143C',\n",
    "  '#FF4500',\n",
    "  '#B0B0B0',\n",
    "  '#556B2F',\n",
    "  '#000000',\n",
    "  '#800020',\n",
    "  '#87CEEB',\n",
    "  '#FFA500',\n",
    "  '#5B84B1',\n",
    "  '#FFA500',\n",
    "  '#900020'\n",
    "]\n",
    "\n",
    "async function askLlamaAboutEmotions(currentMessages) {\n",
    "  const {llmPrompt} = (await llama)\n",
    "  if(typeof currentMessages == 'string') {\n",
    "    currentMessages = [currentMessages]\n",
    "  }\n",
    "\n",
    "  let q1 = 'Can you derive the emotional contexts of this conversation:\\n' + \n",
    "  currentMessages.join('\\n') + '\\nOnly give the emotions in the response.'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "\n",
    "  let q2 = 'Based on this description which emotions best fit:\\n' + \n",
    "    a1 + '\\nOut of this list of emotions which one is the closest match:' +\n",
    "    EMOTIONS.join(', ') + '\\nOnly respond with the matching emotions.'\n",
    "  console.log(\"User: \" + q2);\n",
    "  const a2 = await llmPrompt(q2);\n",
    "  console.log(\"AI: \" + a2);\n",
    "  let emotions = a2.trim().split(/\\s*\\n\\s*|,\\s*|\\s*- |\\s*\\*+\\s*/gi)\n",
    "\n",
    "  console.log(emotions)\n",
    "  return EMOTIONS.filter(e => emotions.includes(e) || a2.includes(e) || a1.includes(e))\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutEmotions,\n",
    "  EMOTIONS,\n",
    "  EMOTION_HEX\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to cache chat logs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const CHAT_PATH = path.join(__dirname, '..', 'Resources', 'Projects', 'conversations')\n",
    "\n",
    "function storeChatHistory (cellId, mtime, summary, emotions, category, hash, date, from, to) {\n",
    "  let chatCacheFile = path.join(CHAT_PATH, path.basename(cellId).replace(/.log\\[[0-9]*\\]/, '') + '.json')\n",
    "  let chatCache = {}\n",
    "  if(fs.existsSync(chatCacheFile))\n",
    "    chatCache = JSON.parse(fs.readFileSync(chatCacheFile).toString('utf-8'))\n",
    "  chatCache[cellId] = {\n",
    "    mtime,\n",
    "    summary,\n",
    "    emotions,\n",
    "    category,\n",
    "    hash,\n",
    "    date,\n",
    "    from,\n",
    "    to\n",
    "  }\n",
    "  fs.writeFileSync(chatCacheFile, JSON.stringify(chatCache, null, 4))\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  storeChatHistory\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ask llm about chat conversations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const llama = importer.import('create llm session')\n",
    "\n",
    "async function askLlamaAboutConversation(currentMessages) {\n",
    "  const {llmPrompt} = (await llama)\n",
    "  let q1 = 'Can you summarize in two sentences what this conversation is about:\\n' + \n",
    "  currentMessages.join('\\n') + '\\nPlease discard any pleasantries, documentation only.'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "  return a1.trim()\n",
    "}\n",
    "\n",
    "async function askLlamaAboutCategory(currentMessages) {\n",
    "  const {llmPrompt} = (await llama)\n",
    "  let q1 = 'Categorize this conversation in two or three words:\\n' + \n",
    "  currentMessages.join('\\n') + '\\nOnly respond with the category.'\n",
    "  console.log(\"User: \" + q1);\n",
    "  const a1 = await llmPrompt(q1);\n",
    "  console.log(\"AI: \" + a1);\n",
    "  return a1.trim().split(/\\s*\\n\\s*|,\\s*|\\s*- |\\s*\\* /gi)[0]\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaAboutConversation,\n",
    "  askLlamaAboutCategory\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "llm respond like a personality?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const CHAT_PATH = path.join(__dirname, '..', 'Resources', 'Projects', 'conversations')\n",
    "const llama = importer.import('create llm session')\n",
    "const {EMOTIONS} = importer.import('ask llm about emotions')\n",
    "const { askLlamaAboutConversation, askLlamaAboutCategory } = importer.import('ask llm about chat conversations')\n",
    "const {askLlamaAboutEmotions} = importer.import('ask llm about emotions')\n",
    "const {askLlamaAboutFunctions} = importer.import('ask llm about functions')\n",
    "const {convertMessagesToNoDates, CHAT_DIRECTORIES} = importer.import('how to scan chat logs')\n",
    "\n",
    "async function getChatHistory(chatLog) {\n",
    "\n",
    "  let chatHistory = []\n",
    "  let chatPath\n",
    "  for(let i = 0; i < CHAT_DIRECTORIES.length; i++) {\n",
    "    chatPath = path.join(CHAT_DIRECTORIES[i], chatLog + '.log')\n",
    "    if(fs.existsSync(chatPath)) {\n",
    "      chatHistory = fs.readFileSync(chatPath).toLocaleString('utf-8').split('\\n')\n",
    "      break\n",
    "    } else {\n",
    "      chatPath = void 0\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return chatHistory\n",
    "}\n",
    "\n",
    "\n",
    "async function askLlamaToRespondLike(name, conversation) {\n",
    "  const {llmPrompt} = (await llama)\n",
    "  if(!name) {\n",
    "    name = 'Brian/Me'\n",
    "  }\n",
    "  if(!conversation) {\n",
    "    conversation = `David Mc, Dec 5, 9:40 AM\n",
    "Ok let me know\n",
    "\n",
    "David Mc, Dec 5, 1:44 PM\n",
    "ready now?\n",
    "\n",
    "You, 10:29 AM\n",
    "do you want to build a music player?\n",
    "something like this https://github.com/waltonseymour/visualizer\n",
    "for front end\n",
    "and my ol php media server for backend\n",
    "maybe i could convince chris to upgrade to class based system\n",
    "\n",
    "You, 10:34 AM\n",
    "https://github.com/briancullinan2/mediaserver/blob/main/modules/encode.module\n",
    "basically just a parameterized launcher for vlc\n",
    "\n",
    "David Mc, 10:41 AM\n",
    "First thing on my list is grafana plug-in\n",
    "\n",
    "You, 10:45 AM\n",
    "https://github.com/preziotte/party-mode\n",
    "grafana for gpt?\n",
    "`\n",
    "  }\n",
    "  if(typeof conversation == 'string') {\n",
    "    conversation = conversation.split(/\\s*\\n\\s*/gi)\n",
    "  }\n",
    "\n",
    "  let chatCache = {}\n",
    "  let conversations = fs.readdirSync(CHAT_PATH)\n",
    "  for(let i = 0; i < conversations.length; i++) {\n",
    "    if(conversations[i][0] == '.') continue\n",
    "    let chatCacheFile = path.join(CHAT_PATH, conversations[i])\n",
    "    try {\n",
    "      Object.assign(chatCache, JSON.parse(fs.readFileSync(chatCacheFile).toString('utf-8')))\n",
    "    } catch (e) {\n",
    "      console.log(e)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // TODO: search for matching emotions\n",
    "  let allCategories = Object.keys(chatCache).map(k => chatCache[k].category)\n",
    "    .filter((a, i, arr) => arr.indexOf(a) == i)\n",
    "  let matchingEmotions = await askLlamaAboutEmotions(conversation.join(', '))\n",
    "  let category = await askLlamaAboutCategory(conversation)\n",
    "\n",
    "  let matchingCategories = []\n",
    "  let functions = []\n",
    "  for(let i = 0; i < allCategories.length; i++) {\n",
    "    functions[functions.length] = allCategories[i]\n",
    "\n",
    "    if(functions.length == 20) {\n",
    "      let result = await askLlamaAboutFunctions(category, functions, [], true)\n",
    "      functions = []\n",
    "      if(result)\n",
    "        matchingCategories = matchingCategories.concat(result)\n",
    "    }\n",
    "  }\n",
    "  if(functions.length > 0) {\n",
    "      let result = await askLlamaAboutFunctions(category, functions, [], true)\n",
    "      functions = []\n",
    "      if(result)\n",
    "        matchingCategories = matchingCategories.concat(result)\n",
    "  }\n",
    "\n",
    "  console.log(matchingCategories)\n",
    "  let chatLogs = {}\n",
    "  let answers = []\n",
    "  let summaries = []\n",
    "  // TODO: search for contextual matches in summaries\n",
    "  let keys = Object.keys(chatCache)\n",
    "  for(let i = 0; i < keys.length; i++) {\n",
    "    if(chatCache[keys[i]].emotions\n",
    "      && chatCache[keys[i]].emotions.filter(e => matchingEmotions.includes(e)).length > 0\n",
    "      || matchingCategories.includes(chatCache[keys[i]].category)\n",
    "    ) {\n",
    "      // TODO: this is where eliza characterization comes in\n",
    "      let chatLog = path.basename(keys[i]).replace(/\\[[0-9]+\\]/gi, '').replace('.log', '')\n",
    "      if(typeof chatLogs[chatLog] == 'undefined') {\n",
    "        chatLogs[chatLog] = await getChatHistory(chatLog)\n",
    "      }\n",
    "\n",
    "      summaries[summaries.length] = chatCache[keys[i]].summary\n",
    "      let history = chatLogs[chatLog].slice(chatCache[keys[i]].from, chatCache[keys[i]].to)\n",
    "      let q1 = 'Derive any response style, personality, topics from this conversation:\\n' + convertMessagesToNoDates(history).join('\\n')\n",
    "      console.log(\"User: \" + q1)\n",
    "      const a1 = await llmPrompt(q1)\n",
    "      console.log(\"AI: \" + a1)\n",
    "      answers[answers.length] = a1\n",
    "\n",
    "      /*\n",
    "      if(summaries.length == 20) {\n",
    "        let q2 = 'Derive any response style, personality, topics from this conversation:\\n' + summaries.join(', ')\n",
    "        console.log(\"User: \" + q2)\n",
    "        const a2 = await llmPrompt(q2)\n",
    "        console.log(\"AI: \" + a2)\n",
    "        answers[answers.length] = a2\n",
    "        summaries.length = []\n",
    "      }\n",
    "      */\n",
    "    }\n",
    "  }\n",
    "  /*\n",
    "  if(summaries.length > 0) {\n",
    "    let q2 = 'Derive any response style, personality, topics from this conversation:\\n' + summaries.join(', ')\n",
    "    console.log(\"User: \" + q2)\n",
    "    const a2 = await llmPrompt(q2)\n",
    "    console.log(\"AI: \" + a2)\n",
    "    answers[answers.length] = a2\n",
    "    summaries.length = []\n",
    "  }\n",
    "  */\n",
    "\n",
    "  console.log(answers)\n",
    "\n",
    "  // TODO: coallesce many responses from {name}\n",
    "\n",
    "  // TODO: finally query llm to act like person\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "module.exports = {\n",
    "  askLlamaToRespondLike\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan code history?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "how to scan calendar journal entries?\n",
    "\n",
    "how to ask if my theories are correct?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "javascript"
  },
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
