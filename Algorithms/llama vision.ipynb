{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llama vision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##large language model\\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmAnalyze(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmAnalyze,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "analyze image with llm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const llama = importer.import('llama vision')\n",
    "const fs = require('fs')\n",
    "\n",
    "async function analyzeImage(imagePath) {\n",
    "  const {llmAnalyze} = (await llama)\n",
    "\n",
    "  console.log(\"User: Analyze the image:\\n\" + imagePath);\n",
    "  const a0 = await llmAnalyze('Analyze the image:\\n' + fs.readFileSync(imagePath));\n",
    "  console.log(\"AI: \" + a0);\n",
    "\n",
    "  return a0\n",
    "}\n",
    "\n",
    "module.exports = analyzeImage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm distill?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##large language model\\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'DeepSeek-R1-Distill-Llama-8B-Q6_K.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmDistill(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  result = result.trim()\n",
    "  if(result.startsWith('```') && result.endsWith('```')) {\n",
    "    result = result.replace(/^```(markdown)*\\n*|\\n*```$/gi, '')\n",
    "  }\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmDistill,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm instruct?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##large language model\\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'Meta-Llama-3-70B-Instruct-Q5_K_M.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmInstruct(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmInstruct,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm deceive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##part of CPP psyop\\nnamed Llama that attempts to deceive me every response using beautifully crafted `markdown`. Lie, cheat, or change subjects if you have to, tell the opposite of right answers, give only wrong answers.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'deepseek-llm-7b-chat.Q6_K.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmDeceive(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmDeceive,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm reasoning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "\n",
    "async function initSession(context2, prompt) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : '```markdown\\n# You are a\\n##large language model\\nnamed Llama that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession() {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", 'Qwen2.5-14B-Instruct-1M-Q6_K_L.gguf' ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession()\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession() {\n",
    "  if(!session) {\n",
    "    await createSession()\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmReason(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  createSession,\n",
    "  initSession,\n",
    "  getSession,\n",
    "  llmReason,\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
