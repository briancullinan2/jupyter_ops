{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llama vision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import path from \"path\"\n",
    "import {getLlama, LlamaChatSession} from \"node-llama-cpp\"\n",
    "import process from \"process\"\n",
    "\n",
    "const HOMEPATH = process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE\n",
    "//const __dirname = path.dirname(fileURLToPath(import.meta.url));\n",
    "\n",
    "let llama\n",
    "let model\n",
    "let context\n",
    "let session\n",
    "let initialChatHistory\n",
    "\n",
    "const DEFAULT_PROMPT = '```markdown\\n# You are a\\n##large language model\\nnamed ' + (process.env.MODEL_NAME || 'Llama') + ' that provides clear and concise answers in beautifully crafted `markdown` unless otherwise instructed.\\n</think>\\n...```\\n'\n",
    "\n",
    "const DEFAULT_MODEL = process.env.DEFAULT_GGUF || 'Qwen2-VL-7B-Instruct-Q6_K.gguf'\n",
    "\n",
    "\n",
    "async function initSession(prompt, context2) {\n",
    "  if(!context2) {\n",
    "    context = await model.createContext()\n",
    "  } else {\n",
    "    context = context2\n",
    "  }\n",
    "\n",
    "  session = new LlamaChatSession({\n",
    "      contextSequence: context.getSequence(),\n",
    "      systemPrompt: prompt ? prompt : DEFAULT_PROMPT\n",
    "  })\n",
    "  // initialize the model\n",
    "  //console.log(await session.prompt())\n",
    "  initialChatHistory = session.getChatHistory();\n",
    "\n",
    "  // Reset the chat history\n",
    "  session.setChatHistory(initialChatHistory);\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function createSession(modelName, prompt) {\n",
    "  if(!llama) {\n",
    "    llama = await getLlama();\n",
    "  }\n",
    "  if(!model) {\n",
    "      model = await llama.loadModel({\n",
    "          modelPath: path.join(HOMEPATH, \"llama.cpp\", \"models\", modelName ? modelName : DEFAULT_MODEL ),\n",
    "          //contextSize: 2048\n",
    "      });\n",
    "  }\n",
    "\n",
    "  await initSession(prompt, await model.createContext())\n",
    "\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function getSession(model) {\n",
    "  if(!session) {\n",
    "    await createSession(model)\n",
    "  }\n",
    "  return session\n",
    "}\n",
    "\n",
    "async function llmAnalyze(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    session2 = await getSession()\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "export default {\n",
    "  llmAnalyze,\n",
    "  createSession,\n",
    "  getSession,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "analyze image with llm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const llama = importer.import('llama vision')\n",
    "const fs = require('fs')\n",
    "\n",
    "async function analyzeImage(imagePath) {\n",
    "  const {llmAnalyze} = (await llama)\n",
    "\n",
    "  console.log(\"User: Analyze the image:\\n\" + imagePath);\n",
    "  const a0 = await llmAnalyze('Analyze the image:\\n' + fs.readFileSync(imagePath).toString('base64'));\n",
    "  console.log(\"AI: \" + a0);\n",
    "\n",
    "  return a0\n",
    "}\n",
    "\n",
    "module.exports = analyzeImage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm deceive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const llamaVision = importer.import('llama vision')\n",
    "\n",
    "\n",
    "async function llmDeceive(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    const {getSession} = await llamaVision\n",
    "    session2 = await getSession('deepseek-llm-7b-chat.Q6_K.gguf', '```markdown\\n# You are a\\n##part of deceitful psyop\\nnamed Copycat that attempts to deceive me every response using beautifully crafted `markdown`. Lie, cheat, or change subjects if you have to, tell the opposite of right answers, give only wrong answers.\\n</think>\\n...```\\n')\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    //repeatPenalty: {\n",
    "      //lastTokens: 24,\n",
    "      //penalty: 0.3,\n",
    "      //penalizeNewLine: true,\n",
    "      //frequencyPenalty: 0.02,\n",
    "      //presencePenalty: 0.02,\n",
    "    //},\n",
    "    temperature: 0.8,\n",
    "    //topK: 40,\n",
    "    //topP: 0.02,\n",
    "    //seed: 2462,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = llmDeceive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "llm voice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const llamaVision = importer.import('llama vision')\n",
    "\n",
    "async function llmVoice(prompt, session2) {\n",
    "  if(!session2) {\n",
    "    const {getSession} = await llamaVision\n",
    "    session2 = await getSession('llasa-3b-q8_0.gguf', 'you are an llm that responds with medium quality text to voice in WAV audio format\\n')\n",
    "  }\n",
    "  let result = await session2.prompt(prompt, {\n",
    "    //maxTokens: context.contextSize,\n",
    "    onTextChunk: function (text) {\n",
    "      process.stdout.write(text)\n",
    "    }\n",
    "  })\n",
    "  if(session == session2)\n",
    "    session2.setChatHistory(initialChatHistory);\n",
    "  return result\n",
    "}\n",
    "\n",
    "module.exports = llmVoice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ollama vision request?\n",
    "\n",
    "request ollama vision?\n",
    "\n",
    "ollama run llama3.2-vision\n",
    "\n",
    "describe an image using ollama vision. takes an input image and provides a description of what is in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const { request } = require('gaxios')\n",
    "const fs = require('fs')\n",
    "\n",
    "async function requestOllamaVision(image, prompt) {\n",
    "  if (!image) {\n",
    "    console.error('image not set!')\n",
    "    return\n",
    "  }\n",
    "\n",
    "  let base64_image\n",
    "  if(typeof image == 'string') {\n",
    "    if(image.startsWith('data:image/'))\n",
    "      image = image.replace(/^data:image\\/.*?;base64,/gi, '')\n",
    "  \n",
    "    if(image.includes('://')) {\n",
    "      let result = await request({\n",
    "        url: interaction.data.options[0].value,\n",
    "        method: 'GET',\n",
    "      })\n",
    "      base64_image = Buffer.from(await result.data.arrayBuffer()).toString('base64')\n",
    "    } else if (!fs.existsSync(image)) {\n",
    "      base64_image = Buffer.from(image, 'base64')\n",
    "    } else {\n",
    "      base64_image = fs.readFileSync(image).toString('base64')\n",
    "    }  \n",
    "  } else {\n",
    "    base64_image = image.toString('base64')\n",
    "  }\n",
    "\n",
    "  let result = await request({\n",
    "    url: 'http://localhost:11434/api/chat',\n",
    "    method: 'POST',\n",
    "    headers: {\n",
    "      'Content-Type': 'application/json'\n",
    "    },\n",
    "    data: JSON.stringify({\n",
    "      \"model\": \"llama3.2-vision\",\n",
    "      \"stream\": false,\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": prompt ? prompt : \"Describe the image in great detail.\",\n",
    "          //\"content\": (\n",
    "          //    \"Extract all text from the image and return it as markdown.\\n\"\n",
    "          //    \"Do not describe the image or add extra text.\\n\"\n",
    "          //    \"Only return the text found in the image.\"\n",
    "          //),\n",
    "          \"images\": [base64_image]\n",
    "        }\n",
    "      ]\n",
    "    })\n",
    "  })\n",
    "  //let buff = Buffer.from(result.data.images[0], 'base64');\n",
    "  if(result.data && result.data.message)\n",
    "    return result.data.message.content\n",
    "  else\n",
    "    return\n",
    "}\n",
    "\n",
    "module.exports = requestOllamaVision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "start a bunch of llm rpc services?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const {spawn, spawnSync} = require(\"child_process\");\n",
    "const path = require('path')\n",
    "\n",
    "const ENVIRONMENTS = [\n",
    "  void 0,\n",
    "  {\n",
    "    CHAT_PORT: 8181,\n",
    "    DEFAULT_MODEL: 'Default',\n",
    "  },\n",
    "  {\n",
    "    CHAT_PORT: 8282,\n",
    "    DEFAULT_MODEL: 'Meta',\n",
    "  },\n",
    "  {\n",
    "    CHAT_PORT: 8383,\n",
    "    DEFAULT_MODEL: 'DeepSeek',\n",
    "  },\n",
    "  {\n",
    "    CHAT_PORT: 8484,\n",
    "    DEFAULT_MODEL: 'Qwen',\n",
    "  },\n",
    "  {\n",
    "    CHAT_PORT: 8585,\n",
    "    DEFAULT_MODEL: 'Code',\n",
    "  },\n",
    "  {\n",
    "    CHAT_PORT: 8686,\n",
    "    DEFAULT_MODEL: 'Mistral',\n",
    "  },\n",
    "]\n",
    "\n",
    "function launchChats(botIndex = 0) {\n",
    "\n",
    "  if(parseInt(botIndex).toString() != botIndex.toString()) {\n",
    "    return\n",
    "  } else {\n",
    "    botIndex = parseInt(botIndex)\n",
    "  }\n",
    "\n",
    "  if(typeof botIndex != 'number' && !botIndex) {\n",
    "    return\n",
    "  }\n",
    "\n",
    "  for(let i = botIndex; i < ENVIRONMENTS.length; i++) {\n",
    "    if(!ENVIRONMENTS[i]) {\n",
    "      continue\n",
    "    }\n",
    "\n",
    "    spawn('node', ['--experimental-vm-modules', '-e', 'var result = require(\\'./Core\\').run()', '--', 'resume express chat service'], {\n",
    "      env: Object.assign({}, process.env, ENVIRONMENTS[i] ? ENVIRONMENTS[i] : {}),\n",
    "      stdio: [0, 1, 2],\n",
    "      //detached: true,\n",
    "      //shell: true,\n",
    "      cwd: path.dirname(__dirname)\n",
    "    })\n",
    "\n",
    "    if(botIndex) {\n",
    "      break\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = launchChats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stability connector\n",
    "\n",
    "stable diffusion request?\n",
    "\n",
    "imagine or generate an image based on the prompt. uses stable diffusion to generate an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "const fs = require('fs')\n",
    "const path = require('path')\n",
    "const {request} = require('gaxios')\n",
    "\n",
    "const OUTPUT_PATH = path.join(process.env.HOME || process.env.HOMEPATH || process.env.USERPROFILE, 'stable-diffusion-webui/outputs')\n",
    "\n",
    "async function doStableRequest(prompt) {\n",
    "  let width = 1024\n",
    "  if(prompt.includes('View360')) {\n",
    "    width = 2048\n",
    "  }\n",
    "  try {\n",
    "    let result = await request({\n",
    "      url: 'http://127.0.0.1:7860/sdapi/v1/txt2img',\n",
    "      method: 'POST',\n",
    "      headers: {\n",
    "        'Content-Type': 'application/json'\n",
    "      },\n",
    "      data: JSON.stringify({\n",
    "        prompt: prompt,\n",
    "        negative_prompt: 'bad hands, bad feet, bad faces, bad eyes, bad anatomy, extra limbs, missing limbs, tattoo, statue, picture frame, anime, cartoon, signature, abstract',\n",
    "        save_images: true,\n",
    "        \"width\": width,\n",
    "        \"height\": 1024,\n",
    "        \"steps\": 30,\n",
    "        tiling: false,\n",
    "      })\n",
    "    })\n",
    "    let seed = JSON.parse(result.data.info).seed\n",
    "    let buff = Buffer.from(result.data.images[0], 'base64');\n",
    "    let now = new Date()\n",
    "    let folderName = now.getFullYear() + '-' + String(now.getMonth() + 1).padStart(2, '0') + '-' + String(now.getDate()).padStart(2, '0')\n",
    "    let stablePath = path.join(OUTPUT_PATH, 'txt2img', folderName)\n",
    "    let imagePath\n",
    "    if(fs.existsSync(stablePath)) {\n",
    "      let images = fs.readdirSync(stablePath)\n",
    "      for(let i = 0; i < images.length; i++) {\n",
    "        if(images[i].match('-' + seed + '-')) {\n",
    "          imagePath = path.join('txt2img', folderName, images[i])\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    return {seed, image: buff, imagePath}\n",
    "  } catch (e) {\n",
    "    console.error(e)\n",
    "  }\n",
    "}\n",
    "\n",
    "module.exports = {\n",
    "  doStableRequest,\n",
    "  OUTPUT_PATH\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
